{
  
    
        "post0": {
            "title": "NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶",
            "content": ". Seja muito bem-vindo √† 10¬™ edi√ß√£o da NLP Newsletter. N√≥s esperamos que todos estejam bem e se mantendo seguros. Essa edi√ß√£o cobre t√≥picos como melhores pr√°ticas envolvendo Modelos de Linguagem, reprodutibilidade em ML e privacidade e seguran√ßa em NLP. . Atualiza√ß√µes da dar.ai üî¨üéì‚öôÔ∏è . Com o intuito de ajudar na an√°lise explorat√≥ria do COVID-19 Open Research Dataset e na obten√ß√£o de insights a partir dessa literatura, n√≥s publicamos um notebook com os passos para a implementa√ß√£o de uma aplica√ß√£o simples de busca por similaridade textual utilizando ferramentas de c√≥digo-aberto e modelos de linguagem pr√©-treinados publicamente dispon√≠veis. | . N√≥s realizamos um treinamento virtual na Open Data Science Conference na semana passada, com o tema Deep Learning for Modern NLP. Voc√™ pode acessar os materiais aqui. | . Tamb√©m na semana passada, n√≥s publicamos dois artigos bem interessantes, numa colabora√ß√£o com membros da nossa comunidade. Um dos trabalhos aborda unsupervised progressive learning, um problema que envolve um agente que analisa uma sequ√™ncia de vetores de dados n√£o anotados (fluxo de dados) e aprende representa√ß√µes a partir da mesma. O segundo trabalho resume uma abordagem para Citation Intent Classification (que consiste em identificar porqu√™ um autor citou outro trabalho) utilizando o modelo ELMo. | . N√≥s publicamos recentemente um notebook que fornece ideias para o ajuste fino de modelos de linguagem pr√©-treinados para a tarefa de classifica√ß√£o de emo√ß√µes. | . Pesquisas e Publica√ß√µes üìô . XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization . No in√≠cio dessa semana, pesquisadores da Google AI e da DeepMind publicaram um interessante benchmark multi-tarefa denominado XTREME, que busca encorajar a avalia√ß√£o das capacidades de generaliza√ß√£o em diferentes idiomas de modelos de linguagem que aprendem representa√ß√µes multil√≠ngues. O benchmark conta com 40 idiomas e 9 tarefas, que requerem entendimento sobre diferentes n√≠veis de significado, tanto do ponto de vista sint√°tico quanto sem√¢ntico. O trabalho fornece bases para compara√ß√µes utilizando modelos estado-da-arte para representa√ß√µes multil√≠ngues, como o mBERT, XML e o MMTE. . . Fonte: Google AI Blog . Evaluating Machines by their Real-World Language Use . Foi demonstrado que modelos de linguagem apresentam um desempenho relativamente bom em diversas tarefas, como question answering e sequence labeling. Entretanto, um novo artigo prop√µe um framework e benchmark para melhor avaliar se modelos de linguagem (LMs) conseguem desempenhar bem seu papel com o uso de linguagem do mundo real em situa√ß√µes mais complexas (por exemplo, gerar conselhos proveitosos para o cen√°rio atual do mundo). Resultados emp√≠ricos mostraram que modelos do estado-da-arte atual, como o T5, geram conselhos √∫teis como os escritos por humanos em apenas 9% dos casos. Essas observa√ß√µes apontam as defici√™ncias dos LMs no que diz respeito a entender e modelar conhecimentos de mundo e do senso comum. . Give your Text Representation Models some Love: the Case for Basque . √â poss√≠vel que modelos monol√≠ngues (como os word embeddings do FastText e o BERT) treinados em grandes bases de dados de idiomas espec√≠ficos produzam melhores resultados que alternativas multil√≠ngues? Num artigo recente, pesquisadores estudaram o desempenho de diversos modelos desse tipo utilizando uma grande base de dados para a l√≠ngua basca. Os resultados indicaram que modelos monol√≠ngues podem de fato produzir melhores resultados em tarefas como classifica√ß√£o de t√≥picos, de sentimentos e PoS tagging para esse idioma. Seria muito interessante verificar se o comportamento se repete para outros idiomas e quais resultados interessantes e novos desafio podem surgir. . . Figura extra√≠da de Agerri et al. (2020) . Advancing Self-Supervised and Semi-Supervised Learning with SimCLR . Numa edi√ß√£o anterior da Newsletter, n√≥s apresentamos o SimCLR, m√©todo desenvolvido pela Google AI que prop√µe um framework para contrastive self-supervised learning de representa√ß√µes visuais, com o objetivo de melhorar os resultados da tarefa de classifica√ß√£o de imagens em diferentes cen√°rios, como transfer-learning ou aprendizado semi-supervisionado, utilizando bases n√£o-anotadas. Os resultados obtidos demonstraram que a abordagem alcan√ßa resultados estado-da-arte no ImageNet utilizando apenas 1% de dados anotados, o que tamb√©m √© um indicativo das poss√≠veis vantagens do m√©todo em cen√°rios com escassez de dados. . . Fonte: Google AI Blog . Vale mencionar que o aprendizado auto-supervisionado (self-supervised learning) √© um dos t√≥picos mais quentes na √°rea atualmente. Se voc√™ tem interesse em saber mais, confira: . Computers Already Learn From Us. But Can They Teach Themselves? | The Illustrated Self-Supervised Learning | Self-supervised learning and computer vision | . Byte Pair Encoding is Suboptimal for Language Model Pretraining . Kaj Bostrom e Greg Durrett publicaram um trabalho onde foi investigado se o Byte Pair Encoding (BPE), um algoritmo para tokeniza√ß√£o habitualmente utilizado, √© a estrat√©gia √≥tima para o treinamento de modelos de linguagem. Os autores propuseram uma avalia√ß√£o direta do impacto da tokeniza√ß√£o no desempenho desses modelos, o que, segundo eles, √© raramente examinado, como observado na literatura. Para verificar isso, LMs foram treinados do zero em experimentos controlados, empregando diferentes t√©cnicas de tokeniza√ß√£o, a saber, Unigram e BPE. Ap√≥s isso, os modelos pr√©-treinados foram testados em diversas tarefas. Os resultados mostraram que o desempenho utilizando a estrat√©gia Unigram se equiparou e at√© mesmo foi superior ao BPE. . Longformer: The Long-Document Transformer . Pesquisadores do Allen AI publicaram um novo modelo baseado no Transformer, denominado Longformer, desenvolvido visando um desempenho mais eficiente em textos longos. Como j√° √© conhecido, uma das limita√ß√µes de modelos baseados no Transformer √© que eles s√£o computacionalmente custosos, devido √† maneira como a opera√ß√£o de self-attention escala (quadraticamente com o tamanho da sequ√™ncia), limitando assim a utiliza√ß√£o de contextos mais longos. Recentemente, v√°rias alternativas como o Reformer e o Sparse Transformers foram propostas, visando possibilitar a aplica√ß√£o dessa classe de modelos para documentos maiores. O Longformer combina modelagem a n√≠vel de caractere e self-attention (uma mistura do mecanismo de aten√ß√£o local e global) para requerer menos mem√≥ria e demonstra sua efici√™ncia na modelagens de textos longos. Os autores tamb√©m mostraram que o seu modelo pr√©-treinado supera outros m√©todos quando aplicados √† tarefas a n√≠vel de documento, como question answering e classifica√ß√£o de texto. . . Figura extra√≠da de Beltagy et al. (2020) . Criatividade, √âtica e Sociedade üåé . Reprodutibilidade em ML . . A quest√£o da reprodutibilidade vem sendo discutida ativamente pelas comunidades de Aprendizado de M√°quina. Com o intuito de encorajar uma ci√™ncia mais aberta, transparente e acess√≠vel, diversos esfor√ßos v√™m sendo realizados a favor dela. Se voc√™ quiser entender como est√° essa quest√£o no campo de ML, confira essa publica√ß√£o feita por Joelle Pineau, dentre outros. | . Recentemente, e inspirado por esses esfor√ßos, o time do Papers With Code (que agora fazem parte do grupo de IA do Facebook) realizaram uma postagem explicando uma checklist de reprodutibilidade bem √∫til, com o objetivo de ‚Äúfacilitar pesquisas reprodut√≠veis apresentadas nas principais confer√™ncias de ML‚Äù (em tradu√ß√£o livre). A checklist avalia os c√≥digos disponibilizados nos seguintes aspectos: | . Depend√™ncias: O reposit√≥rio apresenta informa√ß√µes sobre as depend√™ncias ou instru√ß√µes sobre como preparar o ambiente de desenvolvimento? . | C√≥digos de treinamento: O reposit√≥rio fornece uma maneira de treinar o(s) modelo(s) descritos no artigo? . | C√≥digos de Avalia√ß√£o: O reposit√≥rio fornece um c√≥digo para calcular o desempenho do(s) modelo(s) treinado(s) ou rodar os experimentos neles? . | Modelos pr√©-treinados: O reposit√≥rio fornece acesso gratuito aos par√¢metros do modelo pr√©-treinado? . | Resultados: O reposit√≥rio fornece uma tabela/gr√°fico com os principais resultados e o c√≥digo para reproduzir esses resultados? . | . Fonte: Papers with Code . Ainda nessa quest√£o de ci√™ncia aberta e reprodutibilidade, aqui est√° uma postagem interessante, feita por um pesquisador de NLP, oferecendo uma recompensa pela reprodu√ß√£o de resultados de um artigo que outro pesquisador n√£o conseguiu reproduzir. | . Privacidade e Seguran√ßa em NLP . Ser√° que um modelo de linguagem pr√©-treinado pode ser roubado, ou sua exposi√ß√£o para uso via API pode trazer implica√ß√µes de seguran√ßa? Em um novo artigo, pesquisadores testaram APIs de modelos baseados no BERT para implica√ß√µes de seguran√ßa, no que diz respeito √† utiliza√ß√£o de consultas para roubo do modelo. Resumidamente, eles observaram que um advers√°rio pode roubar um modelo refinado apenas utilizando sequ√™ncias de palavras sem sentido e refinando o seu pr√≥prio modelo com as predi√ß√µes do modelo-alvo. Leia mais sobre ataques de extra√ß√£o de modelos aqui. . . Sistema de extra√ß√£o de modelos aplicado a um modelo-alvo treinado no SQuAD (Fonte). . Outro artigo interessante, aceito na ACL 2020, investigou se modelos de linguagem pr√©-treinados s√£o suscet√≠veis a ataques. Os autores desenvolveram um m√©todo de ‚Äúenvenenamento‚Äù que √© capaz de injetar vulnerabilidades nos par√¢metros pr√©-treinados, tornando os modelos vulner√°veis √† amea√ßas. Devido a essas vulnerabilidades, √© poss√≠vel mostrar que esses modelos exp√µem backdoors que podem ser aproveitadas por invasores com o intuito de manipular as predi√ß√µes do modelo, simplesmente injetando qualquer palavra-chave arbitr√°ria. Para testar esse comportamento, modelos pr√©-treinados foram utilizados em tarefas que envolviam bases de dados ‚Äúcorrompidas‚Äù com palavras-chave espec√≠ficas, feitas para for√ßar o modelos a classificar exemplos de maneira incorreta. . . Figura extra√≠da de Kurita et al. (2020) . Uma s√©rie de pesquisas e aplica√ß√µes baseadas em IA para COVID-19 . A COVID-19 provou-se um dos maiores desafios dos tempos modernos. Pesquisadores de todas as partes do mundo tentam encontrar maneiras de contribuir e ajudar a entender a doen√ßa, fornecendo desde ferramentas de busca at√© bases de dados. Sebastian Ruder publicou uma edi√ß√£o dedicada da sua Newsletter destacando alguns projetos interessantes que pesquisadores de IA v√™m desenvolvendo. . | Ainda nesse assunto, pesquisadores do Allen AI ir√£o discutir a agora popular base de dados COVID-19 Open Research Dataset (CORD-19) num meetup virtual que acontecer√° no final desse m√™s (27/04/2020). . | A CORD-19 vem sendo utilizada por muitos pesquisadores para a constru√ß√£o de aplica√ß√µes impulsionadas por NLP, como ferramentas de busca. Confira esse artigo recente para um exemplo de implementa√ß√£o dessas ferramentas que auxiliam pesquisadores a obter insights r√°pidos relacionados √† CORD-19 a partir de resultados reportados em artigos de especialistas. De acordo com os autores, tais ferramentas ajudam em tomadas de decis√£o baseadas em evid√™ncias. . | ArCOV-19 √© uma base de dados de tweets em √°rabe sobre COVID-19, que cobre um per√≠odo de 27 de janeiro at√© 31 de mar√ßo de 2020 (e a coleta continua!). √â a primeira base dados publicamente dispon√≠vel do Twitter √Årabe cobrindo a pandemia do COVID-19, onde est√£o inclusos cerca de 748K tweets populares (de acordo com o crit√©rio de busca do pr√≥prio Twitter) junto com as redes de propaga√ß√£o do sub-conjunto mais popular de postagens. As redes incluem tanto retweets quando threads de respostas. ArCOV-19 √© projetado para permitir a pesquisa em diversas √°reas, como NLP, Ci√™ncia de Dados, Computadores e Sociedade, entre outras. . | . Ferramentas e Bases de Dados ‚öôÔ∏è . Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence . Mesmo n√£o sendo uma ferramenta ou base de dados por si s√≥, esse excelente artigo, com autoria de Sebastian Raschka, Joshua Patterson, e Corey Nolet, fornece uma vis√£o geral compreensiva de alguns dos principais desenvolvimentos em termos de tend√™ncias tecnol√≥gicas em ML, com foco na linguagem de programa√ß√£o Python. . . Figura extra√≠da de Raschka et al. (2020) . Interpretabilidade e Explicabilidade em ML . A Hugging Face disponibilizou uma ferramenta de visualiza√ß√£o denominada exBERT, que nos permite visualizar as representa√ß√µes aprendidas por modelos de linguagem como o BERT e RoBERTa. Essa funcionalidade foi integrada √† p√°gina de modelos, com o objetivo de prover um melhor entendimento sobre como os modelos de linguagem est√£o aprendendo, assim como quais propriedades s√£o potencialmente codificadas por eles nessas representa√ß√µes. . A OpenAI disponibilizou recentemente uma aplica√ß√£o web chamada Microscope que cont√©m uma cole√ß√£o de visualiza√ß√µes obtidas de camadas e neur√¥nios de diversos modelos de vis√£o computacional que s√£o comumente estudados no contexto de interpretabilidade. O objetivo principal √© facilitar a an√°lise e compartilhamento de insights interessantes, obtidos a partir das caracter√≠sticas aprendidas pelas redes neurais, assim como viabilizar um melhor entendimento dos mesmos. . . CloudCV: ViLBERT Multi-Task Demo . No edi√ß√£o anterior da NLP Research Highlights, n√≥s apresentamos o ViLBERT multi-tarefa, que √© um m√©todo para o aprimoramento de vision-and-language models que pode ser utilizado em recupera√ß√£o de imagens baseada em descri√ß√µes e visual question answering (VQA). Agora, os autores disponibilizaram uma aplica√ß√£o web para teste dos modelos em 8 tarefas diferentes de linguagem e vis√£o computacional, como VQA e pointing question answering. . A Twitter Dataset of 150+ million tweets related to COVID-19 for open research . Devido √† relev√¢ncia da pandemia global de COVID-19, pesquisadores est√£o liberando uma base de dados com tweets relacionados a doen√ßa. Desde a primeira vers√£o disponibilizada, dados adicionais de novos colaborados foram adicionados, permitindo o crescimento da base at√© seu volume atual. A aquisi√ß√£o dedicada de dados come√ßou em 11 de mar√ßo, com mais de 4 milh√µes de tweets por dia. . A tiny autograd engine . Andrej Karpathy disponibilizou recentemente uma biblioteca conhecida como micrograd, que permite a constru√ß√£o e treinamento de redes neurais utilizando uma interface simples e intuitiva. Na verdade, ele escreveu a biblioteca completa com aproximadamente 150 linhas de c√≥digo, o que, segundo ele, √© a mais compacta ferramenta de diferencia√ß√£o autom√°tica que existe. Idealmente, bibliotecas como essa podem ser utilizadas para fins educacionais. . Artigos e Postagens ‚úçÔ∏è . The Transformer Family and Recent Developments . Numa nova e oportuna postagem, Lilian Weng resumiu alguns dos recentes avan√ßos no modelo Transformer. O artigo utiliza uma nota√ß√£o amig√°vel, apresenta uma revis√£o da literatura bem como as √∫ltimas melhorias propostas, como aten√ß√£o com contextos mais longos (Transformer XL) e redu√ß√£o nos requisitos computacionais e de mem√≥ria. . . A compress√£o de modelos √© uma importante √°rea de pesquisa em NLP, devido √† natureza e ao tamanho de modelos de linguagem pr√©-treinados. Idealmente, conforme os modelos produzem novos resultados estado-da-arte para as mais diferentes tarefas de NLP, torna-se importante reduzir seus requisitos computacionais, tornando sua utiliza√ß√£o vi√°vel em produ√ß√£o. Madison May publicou recentemente outro excelente artigo trazendo um vis√£o geral de alguns m√©todos utilizados para compress√£o de modelos para NLP. Alguns dos t√≥picos principais incluem poda, otimiza√ß√£o dos grafos de computa√ß√£o, destila√ß√£o de conhecimento, substitui√ß√£o progressiva de m√≥dulos, entre outros. . Educa√ß√£o üéì . Guest Lecture on Language Models by Alec Radford . Se voc√™ tem interesse em conhecer os aspectos te√≥ricos dos m√©todos utilizados para o aprendizado de modelos de linguagem como o CBOW, Word2Vec, ELMo, GPT, BERT, ELECTRA, T5 e GPT, ent√£o voc√™ deveria conferir essa excelente aula do Alec Radford (pesquisador na OpenAI). Ela faz parte de um curso em andamento, lecionado pelo Pieter Abbeel, sobre t√©cnicas de aprendizado n√£o-supervisionado com redes neurais profundas. . . Python Numpy Tutorial (with Jupyter and Colab) . O popular curso online de Stanford, Convolutional Neural Network for Visual Recognition, agora inclui um link para um notebook do Colab com o seu guia introdut√≥rio ao NumPy, que apresenta um passo a passo extenso mas muito interessante para iniciantes. . New mobile neural network architectures . Interessado em construir modelos de redes neurais para dispositivos m√≥veis ou de borda? Ent√£o essa postagem bem acess√≠vel pode te ajudar! O artigo cobre diversas configura√ß√µes de redes e inclui testes de velocidade. . Data-Driven Sentence Simplification: Survey and Benchmark . A tarefa de Sentence simplification (simplifica√ß√£o de frases, numa tradu√ß√£o livre), possui a finalidade de modificar uma frase de modo a torn√°-la mais f√°cil de ler e entender. Essa colet√¢nea foca em abordagens que tentam aprender a simplificar utilizando uma base de pares de senten√ßas em ingl√™s, contendo as vers√µes originais e simplificadas, que √© um paradigma dominante nos dias atuais. Tamb√©m est√° incluso um benchmark dos diferentes m√©todos em diversas bases de dados, que os compara e destaca os pontos fortes e fracos de cada um deles. . T√≥picos Avan√ßados em Aprendizado de M√°quina . Yisong Yue publicou todas as v√≠deo-aulas do curso Data-Driven Algorithm Design. S√£o cobertos t√≥picos de ML como otimiza√ß√£o Bayesiana, computa√ß√£o diferenci√°vel e imitation learning. . . Men√ß√µes Honrosas ‚≠êÔ∏è . Acesse as edi√ß√µes anteriores da NLP Newsletter aqui (√∫ltima edi√ß√£o em PT-BR). . Harvard est√° oferecendo uma √≥tima sele√ß√£o de cursos self-paced de maneira gratuita! . ARBML fornece implementa√ß√µes de diversos projetos de NLP e ML para a l√≠ngua √°rabe, incluindo experi√™ncias em tempo real utilizando diversas interfaces, como a web, linha de comando e notebooks. . NLP Dashboard √© uma aplica√ß√£o web de NLP interessante, que oferece Reconhecimento de Entidades Nomeadas e an√°lises estat√≠sticas de textos e not√≠cias. Constru√≠da utilizando spaCy, Flask e Python. . Caso voc√™ ainda n√£o conhe√ßa, Connor Shorten mant√©m um canal no YouTube bastante informativo, onde ele resume artigos de ML bem recentes e interessantes. S√£o apresentados os detalhes importantes de cada trabalho atrav√©s de um excelente e conciso resumo. Connor tamb√©m come√ßou um podcast com outros grandes pesquisadores e educadores da √°rea. . Aqui est√° um reposit√≥rio impressionante e bem completo que apresenta as melhoras pr√°ticas e recomenda√ß√µes (via notebooks e explica√ß√µes) para diversos cen√°rios de NLP, como classifica√ß√£o de texto, entailment (estabelecer rela√ß√µes l√≥gicas, como implica√ß√£o e contradi√ß√£o, entre textos), sumariza√ß√£o de texto, question answering, etc. . . Se voc√™ conhece bases de dados, projetos, postagens, tutoriais ou artigos que gostaria de ver na pr√≥xima edi√ß√£o da Newsletter, por favor nos envie utilizando esse formul√°rio. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html",
            "relUrl": "/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html",
            "date": " ‚Ä¢ Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "NLP Newsletter [PT-BR] #8: NeRF, CORD-19, Stanza, Text Generation 101, Notebooks, SECNLP, Dreamer,‚Ä¶",
            "content": ". Atualiza√ß√µes da dair.ai . N√≥s melhoramos a categoriza√ß√£o de todos os TL;DR‚Äôs e resumos j√° inclu√≠dos no reposit√≥rio do NLP Paper Summaries. | Todos os issues e tradu√ß√µes da Newsletter passaram a ser mantidos aqui. | Tamb√©m foram introduzidos nessas semanas os Notebooks, focando no compartilhamento de notebooks de Ci√™ncia de Dados com a comunidade. Se voc√™ tem algum que gostaria de compartilhar, entre em contato conosco! | N√≥s disponibilizamos um tutorial que demonstra como realizar uma classifica√ß√£o de emo√ß√µes utilizando a TextVectorization ‚Äî uma funcionalidade experimental do TensorFlow 2.1 que auxilia no tratamento de texto em redes neurais. | . Pesquisas e Publica√ß√µes üìô . Surveys on Contextual Embeddings . Esse artigo fornece um compilado de metodologias para o aprendizado de embeddings contextualizados. Tamb√©m est√£o inclusos uma revis√£o dos casos de uso da t√©cnica para transfer learning, m√©todos de compress√£o de modelos e an√°lises. . Outro trabalho traz uma cole√ß√£o de m√©todos utilizados para a melhoria de modelos de linguagem baseados no Transformer. . E aqui est√° outra colet√¢nea de modelos de linguagem pr√©-treinados, que prop√µe uma taxonomia para modelos dessa natureza em NLP. . . Qiu et al., 2020 . Visualizing Neural Networks with the Grand Tour . O Grand Tour √© um m√©todo linear (em contraste com outras t√©cnicas n√£o-lineares, como o t-SNE) que realiza a proje√ß√£o de bases de dados de dimens√£o alta para duas dimens√µes. Neste novo artigo do Distill, Li et al. (2020) prop√µem a utiliza√ß√£o das habilidades do Grand Tour para visualizar o comportamento de uma rede neural durante o processo de treinamento. Comportamentos de interesse nas an√°lises incluem as mudan√ßas de pesos e como essas afetam o processo de treinamento, comunica√ß√£o entre camadas do modelo e o efeito de exemplos adversariais ao serem apresentados para a rede neural. . . Fonte: Distill . Meta-Learning Initializations for Low-Resource Drug Discovery . Diversos trabalhos demonstram como o meta-learning pode viabilizar a ado√ß√£o de t√©cnicas de Deep Learning para melhorar benchmarks de few-shot learning. Essa ideia √© particularmente √∫til quando nos deparamos com situa√ß√µes onde a quantidade de dados dispon√≠veis √© limitada, como no caso do desenvolvimento de novos medicamentos. Um artigo recente aplicou uma t√©cnica de meta-learning denominada Model-Agnostic-Meta-Learning (MAML), e suas variantes, para predizer propriedades qu√≠micas em cen√°rios de escassez de dados. Os resultados obtidos demonstraram que a abordagem utilizada tem um desempenho similar a outros m√©todos multi-tarefa pr√©-treinados. . NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis . Um trabalho bastante interessante envolvendo pesquisadores da UC Berkeley, Google Research e da UC San Diego desenvolveu um m√©todo (NeRF) para a cria√ß√£o de novas perspectivas em cen√°rios complexos. Tomando um conjunto de imagens RGB como base de dados, o modelo utiliza coordenadas 5D (localiza√ß√£o espacial e dire√ß√£o) para o treinamento de uma rede neural profunda totalmente conectada, otimizando uma continuous volumetric scene function, e retornando a densidade de volume e radi√¢ncia para aquela localiza√ß√£o. Os diversos valores de sa√≠da s√£o combinados ao longo de um camera ray e renderizados como pixels. Essas sa√≠das renderizadas s√£o utilizadas para otimizar representa√ß√µes de cenas atrav√©s da minimiza√ß√£o do erro de renderiza√ß√£o para todos os camera rays das imagens RGB. Comparada com outras abordagens para a tarefa, a NeRF √© quantitativa e qualitativamente melhor, al√©m de conseguir resolver algumas inconsist√™ncias das outras abordagens, como a aus√™ncia de pequenos detalhes e flickering indesejado. . . Introducing Dreamer: Scalable Reinforcement Learning Using World Models . O Dreamer √© um agente de Aprendizado por Refor√ßo que busca resolver algumas limita√ß√µes (como imediatismo e inefici√™ncia computacional) observados em agentes baseados em modelos para resolver tarefas com alto n√≠vel de dificuldade. Esse agente, proposto por pesquisadores da DeepMind e da Google AI, √© treinado para modelar o mundo no qual est√° inserido e desenvolver a habilidade de aprender comportamentos focados no longo prazo utilizando o backpropagation. Resultados estado-da-arte foram obtidos em 20 tarefas de controle, baseadas nas imagens de entrada fornecidas. Al√©m disso, o modelo √© eficiente e pode operar de forma paralela, tornando-o mais interessante do ponto de vista computacional. As tr√™s tarefas envolvidas no treinamento do agente, com objetivos distintos, s√£o sintetizadas na Figura abaixo. . . Fonte: Google AI Blog . Criatividade, √âtica e Sociedade üåé . COVID-19 Open Research Dataset (CORD-19) . Num esfor√ßo para encorajar a utiliza√ß√£o da IA na luta contra a COVID-19, o Allen Institute of AI publicou o COVID-19 Open Research Dataset (CORD-19), um recurso publicamente dispon√≠vel que busca promover colabora√ß√£o global. A base de dados cont√©m milhares de artigos que permitem a obten√ß√£o de insights, atrav√©s do emprego de t√©cnicas de NLP, que podem ajudar na luta contra o coronav√≠rus. . SECNLP: A survey of embeddings in clinical natural language processing . O SECNLP √© um trabalho que inclui uma revis√£o detalhada de uma ampla gama de t√©cnicas de NLP aplicadas no contexto de sa√∫de. O trabalho foca principalmente em m√©todos de embedding, problemas/desafios que essas representa√ß√µes buscam resolver, e uma discuss√£o sobre poss√≠veis dire√ß√µes de pesquisas. . AI for 3D Generative Design . Essa postagem apresenta uma abordagem utilizada para a gera√ß√£o de objetos 3D a partir de descri√ß√µes em linguagem natural. A ideia √© criar um solu√ß√£o que permita ao designer repetir o processo de cria√ß√£o de maneira mais √°gil e explorar mais possibilidades. Ap√≥s a cria√ß√£o de uma base de conhecimento do ‚Äúespa√ßo de design‚Äù composto de modelos 3D e descri√ß√µes textuais, dois autoencoders (como exemplificado na Figura abaixo) s√£o utilizados para codificar o conhecimento de maneira intuitiva. Com isso, o modelo √© capaz de gerar um design 3D a partir de uma legenda, como voc√™ pode conferir nessa demonstra√ß√£o. . . Fonte . Ferramentas e Bases de Dados ‚öôÔ∏è . Stanza‚Ää‚Äî‚ÄäA Python NLP Library for Many Human Languages . O grupo de NLP de Stanford disponibilizou a Stanza (denominada anteriormente como StanfordNLP), uma biblioteca em Python que oferece ferramentas de an√°lise para mais de 70 idiomas. As funcionalidades incluem Tokeniza√ß√£o, Multi-Word Token Expansion, Lematiza√ß√£o, POS tagging, Reconhecimento de Entidades Nomeadas e muito mais. A biblioteca √© baseada no PyTorch, com suporte a utiliza√ß√£o em GPUs e modelos de redes neurais pr√©-treinadas. A Explosion j√° criou um wrapper para a Stanza, possibilitando sua utiliza√ß√£o como um componente do Pipeline do spaCy. . GridWorld Playground . Pablo Castro criou esse site interessante que implementa um playground para a cria√ß√£o de ambientes em grade, com o objetivo de observar e verificar como agentes de aprendizado por refor√ßo tentam chegar ao objetivo, utilizando a t√©cnica do Q-Learning. Dentre as funcionalidades, est√£o inclusas a habilidade de mudar os par√¢metros do ambiente e de aprendizado em tempo real, mudar a posi√ß√£o dos agentes, e transferir value functions entre os dois. . . X-Stance: A Multilingual Multi-Target Dataset for Stance Detection . A tarefa de Stance detection consiste na identifica√ß√£o do posicionamento de um sujeito frente a uma declara√ß√£o de um ator, podendo ser utilizada na avalia√ß√£o de not√≠cias falsas. Jannis Vamvas e Rico Sennrich disponibilizaram recentemente uma base de dados de larga-escala composta por textos escritos por candidatos das elei√ß√µes na Su√≠√ßa. M√∫ltiplos idiomas est√£o dispon√≠veis na base, o que possibilita a avalia√ß√£o da tarefa de detec√ß√£o de posicionamento em contextos multil√≠ngues. Os autores tamb√©m propuseram a utiliza√ß√£o de um modelo BERT multil√≠ngue, que apresentou um desempenho satisfat√≥rio nos cen√°rios zero-shot cross-lingual and cross-target transfer. . Create interactive textual heatmaps for Jupyter notebooks . Andreas Madsen criou uma biblioteca Python chamada TextualHeatMap, que pode ser utilizada para gerar visualiza√ß√µes que auxiliam no entendimento de quais partes de uma frase est√£o sendo utilizadas pelo modelo na hora de predizer a pr√≥xima palavra, como ocorre em modelos de linguagem. . . Fonte: textualheatmap . Artigos e Postagens ‚úçÔ∏è . How to generate text: using different decoding methods for language generation with Transformers . A Hugging Face publicou um artigo revisando diferentes t√©cnicas utilizadas para a gera√ß√£o de texto, focando em abordagens baseadas no Transformer. S√£o discutidos m√©todos como beam search e varia√ß√µes de processos de amostragem (‚Äúsimples‚Äù, ‚ÄúTop-K‚Äù e ‚ÄúTop-p‚Äù). J√° foram publicadas diversas outras postagens sobre esse mesmo assunto, por√©m nessa os autores dedicaram um bom tempo explicando os aspectos pr√°ticos das t√©cnicas e como elas podem ser utilizadas na biblioteca Transformers. . Training RoBERTa from Scratch‚Ää‚Äî‚ÄäThe Missing Guide . Motivado pela falta de um guia acess√≠vel para o treinamento do zero (from scratch) de modelos de linguagem baseados no BERT utilizando a biblioteca Transformers da Hugging Face, Marcin Zablocki disponibilizou esse tutorial detalhado. O guia mostra como treinar um modelo de linguagem para o Polon√™s e traz v√°rias dicas sobre como evitar erros comuns, prepara√ß√£o dos dados, configura√ß√µes de pr√©-processamento, tokeniza√ß√£o, treinamento, monitoramento do processo de treino e compartilhamento do modelo. . . Educa√ß√£o üéì . Getting started with JAX (MLPs, CNNs &amp; RNNs) . Robert Lange publicou recentemente um tutorial ilustrando como treinar uma Gated Recurrent Unit (GRU) com a nova biblioteca da Google, a JAX. Na edi√ß√£o passada da Newsletter, n√≥s abordamos alguns t√≥picos relacionados √† biblioteca. . NLP for Developers: Word Embeddings . Rachael Tatman publicou o primeiro epis√≥dio da sua nova s√©rie, a NLP for Developers, que cobrir√° as melhores pr√°ticas relacionadas √† aplica√ß√£o de diversos m√©todos de NLP. O primeiro v√≠deo apresenta uma introdu√ß√£o √† word embeddings e como s√£o utilizados, al√©m de dicas para evitar erros comuns quando trabalhamos com esse tipo de representa√ß√£o. . Thomas Wolf: An Introduction to Transfer Learning and HuggingFace . Thomas Wolf apresentou essa palestra sobre Transfer Learning no meetup NLP Zurich, fornecendo uma excelente introdu√ß√£o ao assunto para o contexto de NLP. A palestra apresenta uma vis√£o geral dos momentos mais importantes para a √°rea, al√©m de uma introdu√ß√£o √†s bibliotecas Transformers e Tokenizers, dois dos m√≥dulos mais populares da Hugging Face. . . Men√ß√µes Honrosas ‚≠êÔ∏è . Voc√™ sabia que o Google Sheets fornece uma ferramenta de tradu√ß√£o gratuita? Amit Chaudhary compartilhou uma postagem que mostra como utilizar essa funcionalidade para ‚Äútradu√ß√£o reversa‚Äù, fornecendo uma maneira de aumentar a sua base de dados em tarefas de NLP. . A New York NLP estar√° organizando um meetup online para uma palestra intitulada ‚ÄúUsing Wikipedia and Wikidata for NLP‚Äù onde ser√° discutido como se beneficiar dos dados dessas plataformas para diferentes projetos e casos de uso de NLP. . Lavanya Shukla escreveu esse tutorial sobre como utilizar a PyTorch Lightning para otimizar hiper-par√¢metros de uma rede neural enquanto utilizamos funcionalidades de estrutura e estilo de c√≥digo fornecidas pela biblioteca. O modelo resultante e seu desempenho utilizando diferentes combina√ß√µes de hiper-par√¢metros podem ser visualizados utilizando o logger WandB, que pode ser passado como par√¢metro para o objeto respons√°vel pelo treinamento do modelo. . Um grupo de pesquisadores publicou um estudo investigando de maneira mais aprofundada porqu√™ a t√©cnica de batch normalization (BN) tende a prejudicar a performance de modelos baseados no Transformer para diferentes tarefas de NLP. Com base nos comportamentos observados, os autores propuseram uma nova abordagem denominada power normalization. A t√©cnica proposta apresenta um desempenho superior tanto ao BN quanto ao layer normalization, outra t√©cnica amplamente utilizada atualmente. . Esse blog post apresenta uma extensa lista de livros para ajudar voc√™ a iniciar seus estudos e experi√™ncias com Aprendizado de M√°quina. . . Se voc√™ conhece bases de dados, projetos, postagens, tutoriais ou artigos que gostaria de ver na pr√≥xima edi√ß√£o da Newsletter, sinta-se a vontade para nos contactar atrav√©s do e-mail ellfae@gmail.com ou de uma mensagem direta no twitter. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/22/NLP_Newsletter-PT-BR-_8.html",
            "relUrl": "/nlp_newsletter/2020/03/22/NLP_Newsletter-PT-BR-_8.html",
            "date": " ‚Ä¢ Mar 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "NLP Newsletter [PT-BR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶",
            "content": ". Seja muito bem-vindo a s√©tima edi√ß√£o da NLP Newsletter. Esperamos que voc√™ tenha um dia incr√≠vel e que voc√™ as pessoas que voc√™ ama estejam em seguran√ßa nessas semanas dif√≠ceis. N√≥s decidimos publicar essa edi√ß√£o na esperan√ßa de trazer mais alegria aos nossos leitores. Sendo assim, por favor leia a Newsletter durante o seu tempo livre. Nesse momento, √© importante mantermos o foco no que √© a verdadeira prioridade - nossa fam√≠lia e amigos. ‚ù§Ô∏è üíõ üíö . Algumas atualiza√ß√µes sobre a NLP Newsletter e a dar.ai . Todas as tradu√ß√µes em franc√™s e em chin√™s das edi√ß√µes anteriores est√£o agora dispon√≠veis. Descubra como voc√™ pode contribuir com a tradu√ß√£o das edi√ß√µes anteriores (assim como as futuras!) da Newsletter nesse link. . Nota do tradutor: As tradu√ß√µes de todas as edi√ß√µes da Newsletter, exceto a 3¬™, para portugu√™s tamb√©m est√£o dispon√≠veis! | . N√≥s criamos recentemente dois reposit√≥rios no Github que cont√™m resumos de artigos de NLP e notebooks utilizando PyTorch para que voc√™ possa come√ßar a ter experi√™ncia com redes neurais. . Pesquisas e Publica√ß√µes üìô . Measuring Compositional Generalization . No contexto de Aprendizado de M√°quina, compositional generalization se refere a habilidade de representar o conhecimento aprendido com a base de dados e aplic√°-lo a novos e diferentes contextos. At√© o presente momento, n√£o estava claro como medir essa composicionalidade nas redes neurais. Recentemente, o time de IA da Google apresentou um dos maiores benchmarks para compositional generalization, utilizando tarefas como question answering e semantic parsing. A imagem abaixo apresenta um exemplo do modelo proposto utilizando os chamados √°tomos (unidades utilizadas para se gerar os exemplos) para que sejam produzidos compostos (novas combina√ß√µes dos √°tomos). A ideia deste trabalho √© construir bases de treino e teste que combinam exemplos que possuem a mesma distribui√ß√£o pelos diferentes √°tomos mas com distribui√ß√µes diferentes sobre os compostos. Os autores argumentam que essa √© uma maneira mais confi√°vel de se testar a compositional generalization. . . Cr√©dito: Google AI Blog . Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping . Pesquisadores testaram uma s√©rie de procedimentos de refinamento (fine-tuning) com o objetivo de compreender melhor o efeito das diferentes estrat√©gias de inicializa√ß√£o de pesos e pol√≠ticas de early stopping no desempenho de modelos de linguagem. Atrav√©s de exaustivos experimentos de refinamento do BERT, foi constatado que seeds aleat√≥rias distintas produzem resultados bastante discrepantes. Em particular, o estudo reporta que certas inicializa√ß√µes de pesos de fato conferem ao modelo um bom desempenho em diversas tarefas. Todas as bases e testes realizados foram disponibilizadas, para uso de outros pesquisadores interessados em entender as din√¢micas que ocorrem durante o fine-tuning de maneira mais aprofundada. . Zoom In: An Introduction to Circuits . Pesquisadores da OpenAI publicaram uma postagem discutindo o estado atual da tarefa de interpretabilidade de redes neurais, assim como uma nova abordagem para a interpreta√ß√£o das mesmas. Inspirada pela biologia celular, os autores buscaram entender modelos de vis√£o computacional e o que eles aprendem de maneira bastante aprofundada, atrav√©s da inspe√ß√£o dos pesos do modelo. Basicamente, o estudo apresentou algumas conclus√µes, obtidas a partir dos experimentos realizados, as quais eles acreditam que possam ser utilizadas como base para uma melhor interpreta√ß√£o das redes neurais. . . NLP Research Highlights‚Ää‚Äî‚ÄäIssue #1 . Numa nova iniciativa da dar.ai, a NLP Research Highlights, s√£o fornecidas descri√ß√µes detalhadas de t√≥picos atuais e bem importantes da pesquisa em NLP. A ideia √© que essa iniciativa seja utilizada para acompanhar os avan√ßos da √°rea atrav√©s de resumos acess√≠veis desses trabalhos. Na primeira edi√ß√£o trimestral, os t√≥picos abordados tratam sobre melhorias em modelos de linguagem e em agentes conversacionais para sistemas de reconhecimento de voz. Os resumos s√£o mantidos aqui. . Learning to Simulate Complex Physics with Graph Networks . Nos √∫ltimos meses, as Graph Neural Networks (GNNs) (redes neurais que operam sobre redes) foram um assunto recorrente nas edi√ß√µes da Newsletter, devido a sua efetividade em tarefas n√£o s√≥ da √°rea de NLP como tamb√©m em gen√¥mica e materiais. Um artigo publicado recentemente, prop√µe um framework geral baseado em GNNs que √© capaz de realizar simula√ß√µes f√≠sicas em diferentes cen√°rios, como fluidos e materiais male√°veis. Os autores argumentam que eles obtiveram um desempenho estado-da-arte nesses diferentes contextos e que a abordagem proposta √© possivelmente o melhor simulador treinado da atualmente. Os experimentos realizados incluem a simula√ß√£o de materiais como fluidos viscosos sobre a √°gua e outras intera√ß√µes com objetos r√≠gidos. Tamb√©m foi testado um modelo pr√©-treinado em tarefas out-of-distribution e os resultados obtidos foram bastante promissores, evidenciando o potencial de generaliza√ß√£o para outros cen√°rios. . . (Sanchez-Gonzalez et al., 2020) . Modelos BERT para idiomas espec√≠ficos . O BERT √Årabe (AraBERT) est√° agora dispon√≠vel na biblioteca de Transformers da Hugging Face. Voc√™ pode acessar o modelo aqui e o artigo aqui. . Recentemente, uma vers√£o em japon√™s do BERT tamb√©m foi disponibilizada. Uma vers√£o em polon√™s tamb√©m est√° dispon√≠vel, batizada como Polbert. . Criatividade, √âtica e Sociedade üåé . Computational predictions of protein structures associated with COVID-19 . A DeepMind publicou suas predi√ß√µes de estruturas das prote√≠nas que se ligam ao v√≠rus causador da COVID-19. As predi√ß√µes foram obtidas diretamente do sistema AlphaFold, embora n√£o tenham sido verificadas experimentalmente. A ideia √© que essa publica√ß√µes encorajem outras contribui√ß√µes que busquem entender melhor e v√≠rus e suas fun√ß√µes. . Court cases that sound like the weirdest fights . Janelle Shane compartilhou os resultados de um divertido experimento onde um modelo do GPT-2 foi refinado para gerar processos judiciais contra objetos inanimados. Foi disponibilizado ao modelo uma lista de processos do governo sobre apreens√µes de objetivos contrabandeados e artefatos perigosos, e foram geradas acusa√ß√µes como as apresentadas na imagem abaixo. . . fonte . Toward Human-Centered Design for ML Frameworks . A Google AI publicou os resultados de uma grande pesquisa com 645 pessoas que utilizaram a vers√£o do TensorFlow para JavaScript. O objetivo era entender quais eram as funcionalidades mais importantes da biblioteca para desenvolvedores fora da √°rea de ML, assim como a sua experi√™ncia com as atuais bibliotecas de Aprendizado de M√°quina. Uma das conclus√µes obtidas mostra que a falta de entendimento conceitual de ML dificulta a utiliza√ß√£o de bibliotecas espec√≠ficas para esse grupo de usu√°rios. Os participantes do estudo tamb√©m reportaram a necessidade de instru√ß√µes mais acess√≠veis sobre como aplicar modelos de ML em diferentes problemas e um suporte mais expl√≠cito para modifica√ß√µes do usu√°rio. . Face and hand tracking in the browser with MediaPipe and TensorFlow.js . Este excelente artigo do TensorFlow apresenta um passo-a-passo para habilitar um sistema de tracking do rosto e das m√£os diretamente no navegador utilizando o TensorFlow.js e o MediaPipe. . . Cr√©ditos: Blog do TensorFlow . Ferramentas e Bases de Dados ‚öôÔ∏è . NLP Paper Summaries . N√≥s criamos recentemente um [reposit√≥rio]https://github.com/dair-ai/nlp_paper_summaries) contendo uma lista de resumos de artigos de NLP cuidadosamente formulados, para alguns dos mais interessantes e importantes papers da √°rea nos √∫ltimos anos. O foco principal da iniciativa √© expandir a acessibilidade do p√∫blico-geral √† t√≥picos e pesquisas de NLP. . . Uma biblioteca de vis√£o computacional diferenci√°vel em PyTorch . A Kornia √© uma biblioteca constru√≠da sobre o PyTorch que permite a utiliza√ß√£o de uma s√©rie de operadores para vis√£o computacional diferenci√°vel utilizando o PyTorch. Algumas das funcionalidades incluem transforma√ß√µes em images, depth estimation, processamento de imagens em baixo-n√≠vel, dentre v√°rias outras. O m√≥dulo √© fortemente inspirado no OpenCV, com a diferen√ßa de ser focado em pesquisa, ao inv√©s de aplica√ß√µes prontas para produ√ß√£o. . . Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train . DIET (Dual Intent and Entity Transformer) √© uma arquitetura multi-tarefa de natural language understanding (NLU) proposta pela Rasa. A framework foca no treinamento multi-tarefa, com o objetivo de melhorar o desempenho nos problemas de classifica√ß√£o de inten√ß√µes e reconhecimento de entidades nomeadas. Outros benef√≠cios do DIET incluem a flexibilidade de utiliza√ß√£o de qualquer embedding pr√©-treinado, como o BERT e o GloVe. O foco principal, entretanto, √© disponibilizar um modelo que ultrapassa o estado-da-arte atual nessas tarefas e que seja mais r√°pido de treinar (o speedup reportado foi de 6x!). O modelo est√° dispon√≠vel na biblioteca rasa. . . framework DIET . Perdido no meio dos modelos BERT? . O BERT Lang Street √© uma plataforma que possui a capacidade de buscar por mais de 30 modelos baseados no BERT, em 18 idiomas e 28 tarefas, totalizando 177 entradas em sua base de dados. Dessa forma, se voc√™ quiser descobrir o estado-da-arte para a tarefa de classifica√ß√£o de sentimentos utilizando modelos BERT, basta procurar por ‚Äúsentiment‚Äù na barra de busca (como exemplificado abaixo). . . Med7 . O Andrey Kormilitzin disponibilizou o Med7, que √© um modelo para NLP (em particular Reconhecimento de Entidades Nomeadas (NER)) em relat√≥rios m√©dicos eletr√¥nicos. O modelo √© capaz de identificar at√© 7 categorias de entidades e est√° dispon√≠vel para uso com a biblioteca spaCy. . . . Uma biblioteca em c√≥digo-aberto para Quantum Machine Learning . TensorFlow Quantum √© uma biblioteca que fornece uma s√©rie de funcionalidades para a prototipagem r√°pida de modelos qu√¢nticos de ML, possibilitando a aplica√ß√£o destes em problemas em √°reas como a medicina e materiais. . Fast and Easy Infinitely Wide Networks with Neural Tangents . A Neural Tangents √© uma biblioteca que permite aos pesquisadores construir e treinar modelos de dimens√£o infinita e redes neurais utilizando a JAX. Leia a postagem de lan√ßamento aqui e acesse a biblioteca aqui. . . Artigos e Postagens ‚úçÔ∏è . From PyTorch to JAX: towards neural net frameworks that purify stateful code . Sabrina J. Mielke publicou um artigo com um passo-a-passo que ilustra a constru√ß√£o e treinamento de redes neurais utilizado o JAX. A postagem busca comparar o funcionamento interno das redes com o PyTorch e o JAX, o que auxilia num melhor entendimento dos benef√≠cios e diferen√ßas entra as duas bibliotecas. . . fonte . Why do we still use 18-year old BLEU? . Nesse blog post, Ehud Reiter discorre sobre porqu√™ n√≥s ainda utilizamos t√©cnicas de avalia√ß√£o antigas como BLUE para mensurar o desempenho de modelos de NLP em tarefas como tradu√ß√£o autom√°tica (machine translation). Como um pesquisador da √°rea, ele conta sobre as implica√ß√µes para t√©cnicas que realizam a avalia√ß√£o em tarefas de NLP mais recentes. . Introducing BART . O BART √© um novo modelo proposto pelo Facebook que consiste num denoising autoencoder para o pr√©-treinamento de modelos sequence-to-sequence, que pode melhorar o desempenho dos mesmos em tarefas como sumariza√ß√£o abstrata. Sam Shleifer disponibilizou um resumo interessante do BART e como ele realizou a integra√ß√£o do modelo na biblioteca Transformers da Hugging Face. . A Survey of Long-Term Context in Transformers . Madison May escreveu recentemente um compilado bastante interessante descrevendo estrat√©gias para melhorar abordagens baseadas em Transformers, que incluem Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer, e routing transformer. Alguns dos modelos j√° haviam aparecido em publica√ß√µes da dar.ai e na lista de resumos de artigos. . ‚ÄúMind your language, GPT-2‚Äù: how to control style and content in automatic text writing . Apesar da flu√™ncia impressionante na escrita autom√°tica de texto evidenciada no ano passado, continua sendo um desafio controlar atributos como estrutura ou conte√∫do em textos gerados por modelos neurais. Numa postagem recente, Manuel Tonneau discute o progresso atual e as perspectivas na √°rea de gera√ß√£o de texto parametriz√°vel, como o modelo GPT-2 da Hugging Face refinado no arXiv e o T5 da Google, al√©m do CTRL da Salesforce e do PPLM do time de IA da Uber. . Educa√ß√£o üéì . Talk: The Future of NLP in Python . Em uma de nossas edi√ß√µes anteriores, foi apresentado o THiNC, uma biblioteca funcional de Deep Learning focada na compatibilidade com outras j√° existentes. Essa apresenta√ß√£o, utilizada pela Ines Montani na PyCon Colombia, introduz a biblioteca mais profundamente. . Transformers Notebooks . A Hugging Face publicou uma cole√ß√£o de notebooks no Colab que auxilia no in√≠cio da utiliza√ß√£o de sua biblioteca Transformers. Alguns notebooks incluem o uso de tokeniza√ß√£o, configura√ß√£o de pipelines de NLP, e o treinamento de modelos de linguagem em bases de dados pr√≥prias. . . TensorFlow 2.0 in 7 hours . Confira esse curso gr√°tis de ~7 horas sobre o TensorFlow 2.0, onde s√£o cobertos t√≥picos como o b√°sico de redes neurais, NLP com redes neurais recorrentes (RNNs) e uma introdu√ß√£o ao Aprendizado por Refor√ßo. . DeepMind: The Podcast . A DeepMind liberou todos os epis√≥dios (numa playlist no YouTube) do seu podcast com cientistas, pesquisadores e engenheiros, onde s√£o discutidos t√≥picos como *Artificial General Intelligence, neuroci√™ncia e rob√≥tica. . Cursos de Machine Learning and Deep Learning . A Berkeley est√° disponibilizando publicamente o plano de estudos do seu curso em ‚ÄúDeep Unsupervised Learning‚Äù, focado principalmente nos aspectos te√≥ricos do self-supervised learning e em modelos generativos. Outros t√≥picos incluem modelos de vari√°veis latentes, modelos autorregressivos e flow models. As aulas e os slides tamb√©m est√£o dispon√≠veis. . N√≥s tamb√©m encontramos essa lista impressionante de cursos avan√ßados de ML, NLP e Deep Learning dispon√≠vel de maneira online. . E aqui est√° um outro curso intitulado ‚ÄúIntroduction to Machine Learning‚Äù que aborda assuntos como regress√£o supervisionada, avalia√ß√£o de desempenho, random forests, ajuste de par√¢metros, dicas pr√°ticas e muito mais. . Men√ß√µes Honrosas ‚≠êÔ∏è . A edi√ß√£o anterior da Newsletter (6¬™ edi√ß√£o) est√° dispon√≠vel aqui. . Connon Shorten publicou um v√≠deo explicando o modelo ELECTRA, que prop√µe a utiliza√ß√£o de uma t√©cnica chamada replaced token detection como forma de pr√©-treinar Transformers de maneira mais eficiente. Se voc√™ tiver interesse em saber mais, n√≥s tamb√©m escrevemos um breve resumo do modelo aqui. . Rachael Tatman est√° trabalhando numa nova s√©rie denominada NLP for Developers onde o objetivo √© discutir diferentes m√©todos de NLP de maneira mais aprofundada, quando utiliz√°-los e como lidar com dificuldades comuns apresentadas por essas t√©cnicas. . A DeepMind liberou o AlphaGo‚Ää‚Äî‚ÄäThe Movie no YouTube para celebrar o 4¬∫ anivers√°rio da vit√≥ria do modelo sobre o Lee Sedol no jogo de Go. . A OpenMined est√° com vagas abertas para os cargos de Research Engineer e Research Scientist, que parecem ser boas oportunidades para se envolver com privacy-preserving AI. . . Se voc√™ conhecer bases de dados, projetos, postagens, tutoriais ou artigos que voc√™ gostaria de ver na pr√≥xima edi√ß√£o da Newsletter, sinta-se a vontade para nos contactar atrav√©s do e-mail ellfae@gmail.com ou de uma mensagem direta no twitter. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html",
            "relUrl": "/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html",
            "date": " ‚Ä¢ Mar 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML",
            "content": ". . Seja muito bem-vindo √† sexta edi√ß√£o da NLP Newsletter. Agradecemos por todo o suporte e dedica√ß√£o √† leitura dos temas mais recentes em ML e NLP. Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais. . Algumas atualiza√ß√£oes sobre a NLP Newsletter e o dair.ai . . N√≥s estamos traduzindo a Newsletter para outros idiomas, como o Portugu√™s Brasileiro, Chin√™s, √Årabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as tradu√ß√µes ü§ó. Voc√™ tamb√©m pode contribuir aqui! . . No m√™s passado, n√≥s realizamos o lan√ßamento oficial do nosso novo website. Voc√™ pode dar uma olhada em nossa organiza√ß√£o no GitHub para mais informa√ß√µes sobre os projetos em andamento. Se voc√™ est√° interessado em saber mais sobre as contribui√ß√µes j√° realizadas para a dar.ai, ou mesmo contribuir para a democratiza√ß√£o das tecnologias, ensino e pesquisa sobre Intelig√™ncia Artificial, veja nossa se√ß√£o de issues. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! . Publica√ß√µes üìô . A Primer in BERTology: What we know about how BERT works . . Modelos baseados no Transformer mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como sequence labeling e question answering. Um desses modelos, o BERT (Devlin et al. 2019), vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo artigo entitulado ‚ÄúA Primer in BERTology: What we know about how BERT works‚Äù busca come√ßar a responder quest√µes sobre as raz√µes que possibilitam o BERT funcionar t√£o bem em tantas tarefas de NLP. Alguns dos t√≥picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo √© representado, al√©m de m√©todos que outros pesquisadores est√£o utilizando para melhorar o processo de aprendizado. . Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer . . A Google AI publicou recentemente um m√©todo que incorpora todas as li√ß√µes aprendidas e melhorias do Transfer Learning para NLP num franework unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho prop√µe que a maioria das tarefas de NLP podem ser formuladas no formato text-to-text, onde tanto a entrada quanto a sa√≠da do problema apresentam-se na forma de texto. Os autores alegam que ‚Äúesse framework fornece uma fun√ß√£o objetivo para treinamento que √© consistente tanto na fase de pr√©-treinamento quanto no fine-tuning‚Äù. O T5 √© essencialmente um encoder-decoder baseado no Transformer, com v√°rias melhorias, em especial nos componentes de aten√ß√£o da arquitetura. O modelo foi pr√©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como Colossal Clean Crawled Corpus, onde foi estabelecido um novo estado-da-arte para tarefas como sumariza√ß√£o, question answering e classifica√ß√£o de texto. . . (Raffel et al. 2020) . 12-in-1: Multi-Task Vision and Language Representation Learning . . Os esfor√ßos de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avan√ßos na √°rea de lingu√≠stica e vis√£o computacional, mesmo quando os conhecimentos necess√°rios para abordar essas tarefas possuem interse√ß√£o. Um novo artigo (que ser√° apresentado na CVPR) prop√µe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de lingu√≠stica e vis√£o computacional, gerando uma modelo mais gen√©rico para as mesmas. O m√©todo reduz a quantidade de par√¢metros e apresenta um bom desempenho em problemas como recupera√ß√£o de imagens baseadas em legendas, e question answering visual. . . (Lu et al. 2020) . BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations . . Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT √© capaz de gerar representa√ß√µes que generalizam para outras √°reas, al√©m de texto e vis√£o computacional. Os autores apresentam um modelo denominado BERT-gen, que tira proveito de representa√ß√µes mono e multi-modais para obter desempenhos superiores em bases de dados de gera√ß√µes de perguntas baseadas em imagens. . . (Scialom et al. 2020) . Criatividade e Sociedade üé® . The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence . . Gary Marcus publicou recentemente um trabalho onde ele explica a s√©rie de passos que, na opini√£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo √© priorizar a constru√ß√£o de sistemas h√≠bridos e orientados √† conhecimento, guiados por modelos cognitivos, ao inv√©s da proposi√ß√£o de modelos com mais par√¢metros que exigem mais dados e poder computacional. . 10 Breakthrough Technologies 2020 . . A revista MIT Technology Review publicou a lista dos 10 avan√ßos tecnol√≥gicos que segundo eles far√£o a diferen√ßa na resolu√ß√£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista ‚Äî sem ordem espec√≠fica ‚Äî inclui a internet n√£o-hack√°vel, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, mol√©culas descobertas por sistemas de IA, mega-constela√ß√µes de sat√©lites artificias, supremacia qu√¢ntica, IA em aparelhos celulares, privacidade diferencial e climate attribution. . Time to rethink the publication process in machine learning . . Yoshua Bengio escreveu recentemente sobre suas preocupa√ß√µes em rela√ß√£o aos atuais ciclos acelerados de publica√ß√µes em Aprendizado de M√°quina. O ponto principal √© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e s√£o apenas incrementais, deixando o investimento de tempo na revis√£o e verifica√ß√£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes s√£o aqueles que precisam lidar com as consequ√™ncias negativas da press√£o e estresse gerados por essa situa√ß√£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas a√ß√µes para ajudar no processo de desacelera√ß√£o das publica√ß√µes para o bem da ci√™ncia. . Ferramentas e Bases de Dados ‚öôÔ∏è . Implementa√ß√£o da PointerGenerator network com a AllenNLP . . Redes Pointer-Generator buscam aprimorar o mecanismo de aten√ß√£o de modelos sequence-to-sequence e s√£o utilizadas para melhorar o desempenho em tarefas como sumariza√ß√£o abstrata. Se voc√™ gostaria de utilizando essa t√©cnica com a framework AllenNLP, saiba que o Kundan Krishna desenvolveu um m√≥dulo que permite a execu√ß√£o de um modelo pr√©-treinado dessa categoria, al√©m do treinamento de um novo modelo do zero. . . Question answering para diferentes idiomas . . Com a dissemina√ß√£o de modelos baseados no Transformer e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esfor√ßo significativo na constru√ß√£o e libera√ß√£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder compartilhou uma lista de datasets que podem ser utilizados no desenvolvimento de m√©todos para question answering em diversas l√≠nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), KorQuAD, SberQuAD, FQuAD, Arabic-SQuAD, SQuAD-it e Spanish SQuAD. . PyTorch Lightning . . A PyTorch Lightning √© uma ferramenta que possibilita a abstra√ß√£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), al√©m do uso de precis√£o de 16 bits. Fazer essas configura√ß√µes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em v√°rias GPUs/TPUs sem a necessidade de altera√ß√£o do c√≥digo. . Graph Neural Networks no TF2 . . O time de pesquisa da Microsoft liberou uma biblioteca com a implementa√ß√£o de diversas arquiteturas de Graph Neural Networks (GNNs). A biblioteca, baseada na vers√£o 2.0 do TensorFlow, fornece funcionalidades para manipula√ß√£o de dados que podem ser utilizadas diretamente nas itera√ß√µes de treino/avalia√ß√£o. . Pre-training SmallBERTa‚Ää‚Äî‚ÄäA tiny model to train on a tiny dataset . . Voc√™ j√° pensou em treinar o seu pr√≥prio modelo de linguagem do zero, mas nunca teve o poder computacional necess√°rio para isso? Se j√°, ent√£o o Aditya Malte pode lhe ajudar com esse excelente notebook no Colab que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida. . √âtica em IA üö® . Why faces don‚Äôt always tell the truth about feelings . H√° algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emo√ß√µes em contextos visuais ou textuais. Um novo artigo reabre o debate que t√©cnicas de IA que tentam reconhecer emo√ß√µes diretamente de imagens faciais n√£o est√£o fazendo seu trabalho direito. O argumento principal, formulado por psic√≥logos proeminentes na √°rea, √© que n√£o existe evid√™ncia da exist√™ncia de express√µes universais que possam ser utilizadas na detec√ß√£o de emo√ß√µes de maneira independente. Seria necess√°ria uma melhor compreens√£o de tra√ßos de personalidade e movimentos corporais por parte do modelo, dentre outras caracter√≠sticas, para que seja poss√≠vel detectar as emo√ß√µes humanas de maneira mais precisa. . Differential Privacy and Federated Learning Explicadas . . Uma das considera√ß√µes √©ticas que devem ser levadas em considera√ß√£o durante a constru√ß√£o de sistemas de IA √© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atrav√©s da differential privacy ou do federated learning. Se voc√™ quiser saber mais sobre esses dois t√≥picos, Jordan Harrod produziu uma excelente introdu√ß√£o nesse v√≠deo, que inclui uma sess√£o hands-on utilizando notebooks do Colab. . Artigos e Postagens ‚úçÔ∏è . A Deep Dive into the Reformer . . Madison May realizou uma postagem em seu blog que fornece uma an√°lise mais profunda do Reformer, um novo modelo baseado no Transformer, proposto recentemente pela Google AI. O Reformer j√° havia aparecido numa edi√ß√£o anterior da Newsletter. . Uma plataforma de blogs gratuita . A fastpages permite a cria√ß√£o e configura√ß√£o autom√°tica de um blog utilizando a GitHub pages de maneira gratuita. Essa solu√ß√£o simplifica o processo de publica√ß√£o e tamb√©m oferece suporte √† utiliza√ß√£o de documentos exportados e Jupyter notebooks. . Dicas para entrevistas na Google . . Pablo Castro, do time da Google Brain, publicou uma excelente postagem destacando as principais dicas para aqueles interessados em aplicar para uma posi√ß√£o na Google. Os t√≥picos abordados incluem dicas sobre o processo de entrevistas, como prepara√ß√£o, o que esperar durante e o que acontece depois delas. . Transformers are Graph Neural Networks . . Graph Neural Networks (GNNs) e Transformers mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente artigo em seu blog, evidenciando a conex√£o entre GNNs e Transformers, e as diversas maneiras pelas quais esses m√©todos podem ser combinados e utilizados em conjunto. . . Representa√ß√£o de uma frase como um grafo completo de palavras‚Ää‚Äî‚Ääfonte . CNNs e Equivari√¢ncia . . Fabian Fuchs e Ed Wagstaff discutiram a import√¢ncia da equivari√¢ncia e como as Convolutional Neural Networks (CNNs) garantem essa propriedade. O conceito √© apresentado e discutido posteriormente no contexto de CNNs em rela√ß√£o √† transla√ß√£o. . Self-supervised learning com imagens . A t√©cnica de self-supervised learning foi amplamente discutida nas edi√ß√µes anteriores da Newsletter devido ao seu papel em modelos recentes para language modeling. Esse blog post, feito pelo Jonathan Whitaker, fornece uma explica√ß√£o intuitiva da t√©cnica de aprendizado no contexto de imagens. Se voc√™ deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tamb√©m publicou um artigo interessante descrevendo o conceito de maneira visual. . Educa√ß√£o üéì . Stanford CS330: Deep Multi-Task and Meta-Learning . A universidade de Stanford liberou recentemente suas v√≠deo-aulas, numa playlist no YouTube, para o novo curso em deep multi-task e meta-learning. Os assuntos apresentados incluem bayesian meta-learning, lifelong learning, uma vis√£o geral sobre aprendizado por refor√ßo, model-based reinforcement learning, entre outros. . PyTorch Notebooks . A dar.ai liberou recentemente um compilado de notebooks apresentando uma introdu√ß√£o √† redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos t√≥picos j√° dispon√≠veis incluem como implementar um modelo de regress√£o log√≠stica do zero, assim como a programa√ß√£o de redes neurais feed-forward e recorrentes. Notebooks no Colab est√£o dispon√≠veis no GitHub. . The fastai book (draft) . Jeremy Howard e Sylvain Gugger liberaram uma lista com alguns notebooks para um futuro curso que introduz conceitos de Deep Learning e como implementar diferentes m√©todos utilizando o PyTorch e a biblioteca da fastai. . Cursos gratuitos de Ci√™ncia de Dados . . O Kaggle disponibilizou uma s√©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontap√© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdu√ß√£o ao Aprendizado de M√°quina e ao Python, Visualiza√ß√£o de Dados, Feature Engineering, Deep Learning, entre outros. . . Um outro excelente curso online de Ci√™ncia de Dados disponibiliza notas de aulas, slides e notebooks sobre t√≥picos que v√£o desde an√°lise explorat√≥ria at√© interpreta√ß√£o de modelos para Processamento de Linguagem Natural. . . 8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch . . A nepture.ai publicou um excelente artigo que cont√©m discuss√µes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na cria√ß√£o do PyTorch e nas ferramentas constru√≠das com base na biblioteca. . Visualizando Adaptive Sparse Attention Models . . Sashs Rush compartilhou um notebook impressionante que explica e mostra os detalhes t√©cnicos sobre como produzir sa√≠das esparsas com a softmax e induzir esparsidade nos componentes de aten√ß√£o do modelo Transformer, auxiliando na atribui√ß√£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade. . . Visualizando a distribui√ß√£o de probabilidade da sa√≠da da softmax . Men√ß√µes Honrosas ‚≠êÔ∏è . Voc√™ pode conferir a edi√ß√£o da passada da üóû Newsletter aqui. . . Conor Bell escreveu esse script em Python que permite a visualiza√ß√£o e prepara√ß√£o de uma base de dados que pode ser utilizada no modelo StyleGAN. . . Manu Romero compartilhou um modelo de POS tagging para o espanhol. O modelo est√° dispon√≠vel para uso utilizando a biblioteca Transformers da Hugging Face. Ser√° interessante acompanhar a divulga√ß√£o de modelos para outros idiomas. . Esse reposit√≥rio cont√©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem rela√ß√£o com o BERT e abordam diversos problemas como compress√£o de modelos, tarefas de dom√≠nios espec√≠ficos, entre outros. . . Connor Shorten publicou um v√≠deo de 15 minutos explicando um novo framework que busca reduzir o efeito das ‚Äúshortcut‚Äù features no self-supervised representation learning. Essa √© uma tarefa importante porqu√™, caso n√£o seja realizada corretamente, o modelo pode falhar em aprender representa√ß√µes sem√¢nticas √∫teis e potencialmente se tornar ineficiente durante o transfer learning. . Sebastian Ruder publicou uma nova edi√ß√£o da newsletter NLP News, que apresenta t√≥picos e recursos como an√°lises de artigos de ML e NLP em 2019, e apresenta√ß√µes sobre os fundamentos do Deep Learning e Transfer Learning. Confira aqui. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "relUrl": "/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "date": " ‚Ä¢ Mar 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶",
            "content": ". Publica√ß√µes üìô . Turing-NLG: A 17-billion-parameter language model by Microsoft . O Turing Natural Language Generation (T-NLG) √© um modelo de linguagem com 17 bilh√µes de par√¢metros proposto pelo grupo de pesquisa em Intelig√™ncia Artificial da Microsoft. O T-NLG √© composto por 78 camadas, baseado na arquitetura dos Transformers, que superou o estado da arte anterior (atribuido ao Megatron-LM da Nvidia) considerando a Perplexidade na base de dados WikiText-103. O modelo foi testado em diversas tarefas, como question answering e sumariza√ß√£o abstrata, onde foram observados comportamentos interessantes e desej√°veis para modelos dessa categoria, como ‚Äúzero shot‚Äù question answering (onde o modelo responde a um pergunta sem estar ciente do contexto explicitamente), e baixa necessidade de bases previamente anotadas, para as tarefas citadas anteriormente. O modelo pode ser treinado gra√ßas √† biblioteca DeepSpeed, utilizando o esquema de otimiza√ß√£o ZeRO (que tamb√©m aparece nessa edi√ß√£o da Newsletter). . . Modelos de Linguagem e suas quantidades de par√¢metros‚Ää‚Äî‚Ääfonte . Neural based Dependency Parsing . A pesquisadora Miryam de Lhoneux liberou recentemente sua tese de Doutorado, entitulada ‚ÄúLinguistically Informed Neural Dependency Parsing for Typologically Diverse Languages‚Äù. O trabalho desenvolvido prop√µe a utiliza√ß√£o de abordagens baseadas em redes neurais para a tarefa de an√°lise de depend√™ncias (dependency parsing) em idiomas com diversas tipologias (l√≠nguas que apresentam padr√µes funcionais e estruturais diferentes). O trabalho apresentado pela autora indica que a incorpora√ß√£o de RNNs e camadas recursivas nos analisadores pode ser ben√©fica para a tarefa, uma vez que essas arquiteturas podem indicar o conhecimento lingu√≠stico necess√°rio √† an√°lise. Outras extens√µes incluem a utiliza√ß√£o de analisadores poliglotas e estrat√©gias de compartilhamento de par√¢metros para a an√°lise de linguagens correlacionadas ou descorrelacionadas. . End-to-end Cloud-based Information Extraction with BERT . Um time de pesquisadores publicou um artigo descrevendo como modelos de Transformers, como o BERT, podem auxiliar sistemas de extra√ß√£o de informa√ß√£o de ponta a ponta em documentos de dom√≠nios espec√≠ficos, como documenta√ß√£o regulat√≥ria e de concess√£o de propriedades. Esse tipo de trabalho, al√©m de otimizar opera√ß√µes de neg√≥cios, demonstra a efici√™ncia e aplicabilidade de modelos baseados no BERT em cen√°rios onde bases de dados anotadas s√£o extremamente limitadas. Uma plataforma na nuvem √© apresentada e discutida, assim como os detalhes de sua implementa√ß√£o (ver figura abaixo). . . fonte . Question Answering Benchmark . Em Wolfson et al. (2020), apresenta-se um benchmark para a tarefa de question understanding e um m√©todo para decomposi√ß√£o de quest√µes, uma etapa necess√°ria para a determina√ß√£o de uma resposta apropriada. Os autores recorreram √† um servi√ßo de crowdsourcing para a cria√ß√£o da base anotada de decomposi√ß√£o de quest√µes. Com objetivo de demonstar a viabilidade e aplicabilidade do m√©todo proposto, os autores mostraram que √© poss√≠vel melhorar o desempenho de modelos utilizando essa t√©cnica sobre a base de dados HotPotQA. . . ‚ÄúQuest√µes de diferentes fontes de informa√ß√µes exibem uma estrutura composicional semelhante. Quest√µes em linguagem natural (parte de cima) s√£o decompostas seguindo a metodologia QDMR (meio) e deterministicamente mapeadas para uma linguagem pseudo-formal (parte de baixo).‚Äù‚Ää‚Äî‚Ääfonte . Radioactive data: tracing through training . Membros da equipe de pesquisa em IA do Facebook publicaram recentemente um trabalho interessante que prop√µe a marca√ß√£o de imagens (referenciadas como radioactivate data) de tal maneira que seja poss√≠vel verificar se uma determinada base de dados foi utilizada no treinamento de um modelo de Aprendizado de M√°quina. Os autores conclu√≠ram que √© poss√≠vel utilizar uma marca√ß√£o mais robusta, que move as features para uma determinada dire√ß√£o, e que pode ser empregada para auxiliar a detec√ß√£o de dados ‚Äúradioativos‚Äù, mesmo quando apenas 1% destes est√£o presentes na base de treinamento. . Essa √© uma tarefa bem desafiadora, uma vez que qualquer modifica√ß√£o nos dados pode potencialmente prejudicar o desempenho do modelo. De acordo com os autores, o trabalho proposto pode ‚Äúajudar pesquisadores e engenheiros a monitorar quais bases de dados foram utilizadas no treinamento de um modelo, com o objetivo de compreender melhor como bases de dados de diferentes naturezas influenciam o desempenho de diversas redes neurais‚Äù. Parece uma tarefa crucial para aplica√ß√µes mission-critical. Confira o artigo completo aqui. . REALM: Retrieval-Augmented Language Model Pre-Training . O REALM √© um m√©todo de recupera√ß√£o em larga escala baseado em redes neurais, que faz uso de bases de conhecimento textual para pr√©-treinar um modelo de linguagem de maneira n√£o-supervisionada. Essencialmente, o objetivo da abordagem √© capturar o conhecimento, de uma maneira mais interpret√°vel, expondo o modelo √† conhecimentos gerais utilizados durante o processo de treinamento e infer√™ncia atrav√©s do backpropagation. As bases onde o m√©todo foi testado e avaliado incluem benchmarks de open-domain question answering. Al√©m do aumento observado na acur√°cia do modelo, outros benef√≠cios incluem modularidade e interpretabilidade dos componentes. . . fonte . Criatividade e Sociedade üé® . Apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas . Durante a semana passada, uma peti√ß√£o circulou na internet, reivindicando a permiss√£o para apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas, como as relacionadas √† Aprendizado de M√°quina. Para saber mais, acesse change.org. Parece que Yoshua Bengio, um dos pioneiros do Deep Learning, est√° convocando as pessoas √† assinar a peti√ß√£o. Ele deixou isso bem claro em seu novo blog. . Abstra√ß√£o e Desafios de Racioc√≠nio . Fran√ßois Chollet postou recentemente uma competi√ß√£o no Kaggle onde ele disponibilizou o Abstraction and Reasoning Corpus (ARC), uma base de dados que tem como objetivo encorajar os usu√°rios a desenvolver sistemas de IA para resolver tarefas √†s quais nunca foram expostos. A esperan√ßa √© que essa competi√ß√£o seja o pontap√© inicial para a constru√ß√£o de modelos mais robustos de IA, capazes de resolver novos problemas por conta pr√≥pria de maneira mais eficiente e r√°pida, ajudando na resolu√ß√£o de aplica√ß√µes mais desafiadoras do mundo real como a melhoria de carros aut√¥nomos que operam em ambientes diversos e extremos. . Publica√ß√µes de Aprendizado de M√°quina e Processamento de Linguagem Natural em 2019 . Marek Rei liberou sua an√°lise anual com estat√≠sticas das publica√ß√µes sobre ML e NLP em 2019. As confer√™ncias consideradas nas an√°lises foram ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, e AAAI. . Growing Neural Cellular Automata . Morfog√™nese √© um processo de auto-organiza√ß√£o pelo qual alguns animais, como as salamandras, podem regenerar partes de seus corpos que sofreram danos. O processo √© robusto a perturba√ß√µes e adaptativo na natureza. Inspirado nesse esse fen√¥meno biol√≥gico e com a necessidade de uma melhor compreens√£o desse mecanismo, pesquisadores publicaram um trabalho entitulado ‚ÄúGrowing Neural Cellular Automata‚Äù, que adota um modelo diferenci√°vel para o processo de morfog√™nese buscando replicar os comportamentos e propriedades de sistemas de auto-repara√ß√£o. . Espera-se que o processo seja capaz de criar m√°quinas ‚Äúauto-repar√°veis‚Äù que possuam a mesma robustez e maleabilidade dos organismos biol√≥gicos. Al√©m disso, o m√©todo pode possibilitar um melhor entendimento do processo de regenera√ß√£o em si. √Åreas que podem se beneficiar com essa pesquisa incluem a medicina regenerativa e a modelagem de sistemas sociais e biol√≥gicos. . . fonte . Visualiza√ß√£o do mecanismo de Aten√ß√£o do Transformer . Hendrik Strobelt compartilhou esse reposit√≥rio bem interessante que mostra como construir rapidamente uma visualiza√ß√£o simples e interativa da Aten√ß√£o do Transformer atrav√©s de uma aplica√ß√£o web utilizando as bibliotecas Hugging Face e d3.js. . . fonte . SketchTransfer: A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks . O SketchTransfer prop√µe uma nova tarefa que tem por objetivo testar a habilidade de redes neurais profundas em manter a capacidade invari√¢ncia frente a presen√ßa/aus√™ncia de detalhes. Um debate de longa data existe acerca da inabilidade de redes profundas em generalizar varia√ß√µes que n√£o foram vistas durante o treinamento, algo que os humanos conseguem fazer com relativa facilidade em situa√ß√µes como, por exemplo, a falta de alguns detalhes visuais quando assistimos desenhos. O trabalho discute e disponibiliza uma base de dados esbo√ßos n√£o-anotados e imagens reais anotadas, permitindo que os pesquisadores possam estudar o problema de ‚Äúdetail-invariance‚Äù de maneira bastante cuidadosa. . . fonte . Bibliotecas e Bases de Dados ‚öôÔ∏è . DeepSpeed + ZeRO . A Microsoft liberou um pacote para otimiza√ß√£o chamado DeepSpeed, compat√≠vel com o PyTorch, que possibilita o treinamento de modelos com 100 bilh√µes de par√¢metros. A biblioteca d√° destaque a 4 importantes aspectos do processo de treinamento: opera√ß√£o em escala, velocidade, custo, e usabilidade. A DeepSeed foi liberada junto com o ZeRO, uma tecnologia que otimiza a utiliza√ß√£o da mem√≥ria, e que possibilita o emprego do Deep Learning em larga escala de maneira distribu√≠da com as atuais tecnologias de GPU, al√©m de melhorar o throughput em 3-5 vezes em rela√ß√£o √† melhor solu√ß√£o atual. A tecnologia possibilita o treinamento de modelos de tamanho arbitr√°rio que podem ocupar a mem√≥ria total dispon√≠vel, distribu√≠da pelos diversos dispositivos na infra-estrutura. . . fonte . Deep Learning em Superf√≠cies 3D de Maneira R√°pida e Eficiente . A PyTorch3D √© uma biblioteca em c√≥digo aberto para a pesquisa de Deep Learning aplicado √† superf√≠cies 3D. Esse pacote, baseado no PyTorch, busca auxiliar no suporte e entendimento de dados em 3D aplicados √† redes neurais. A biblioteca consiste de implementa√ß√µes eficientes e otimizadas de operadores e fun√ß√µes de custo 3D comumente utilizadas. Um renderizador diferenci√°vel modular tamb√©m est√° dispon√≠vel, que pode ser √∫til durante a pesquisa e explora√ß√£o de entradas 3D com padr√µes complexos e na gera√ß√£o de predi√ß√µes de alta qualidade. . . fonte . Gerenciamento de Configura√ß√µes para projetos de ML . Hydra √© uma ferramenta de configura√ß√£o escrita em Python, que auxilia no gerenciamento de projetos complexos de ML de maneira mais eficiente. O prop√≥sito √© dar suporte a pesquisadores que utilizam o PyTorch, oferencedo a possibilidade de reutiliza√ß√£o de configura√ß√µes de projetos de maneira funcional. O benef√≠cio principal oferecido √© a possibilidade do programador compor configura√ß√µes como comp√µe-se c√≥digo, o que permite a r√°pida altera√ß√£o de arquivos de configura√ß√£o. A Hydra pode ainda gerenciar automaticamente o diret√≥rio de trabalho que armazena as sa√≠das do seu projeto de ML, o que √© bem √∫til quando precisamos salvar e acessar diversos resultados provenientes de m√∫ltiplos jobs. Para saber, mais visite o site. . Uma biblioteca para Infer√™ncia Causal com Redes Bayesianas . A CausalNex √© uma ferramenta que busca combinar o aprendizado de m√°quina e o racioc√≠nio causal, possibilitando a descoberta de relacionamentos estruturais na base de dados. Os autores prepararam um tutorial introdut√≥rio que mostra porqu√™ e como inferir causalidade com as Redes Bayesianas utilizando a biblioteca proposta. . . fonte . Google Colab Pro agora est√° dispon√≠vel . O Google Colab comecou a oferecer uma vers√£o Pro, que disponibiliza vantagens como o acesso exclusivo √† GPUs e TPUs, tempos de execu√ß√£o mais longos e mais mem√≥ria. . TyDi QA: A Multilingual Question Answering Benchmark . O grupo de IA da Google introduziu recentemente a TyDi QA, uma base de dados multi-idiomas que busca encorajar pesquisadores a abordar a tarefa de question answering em l√≠nguas mais tipologicamente diversas, ou seja, que apresentam padr√µes estrturais n√£o-convencionais. A libera√ß√£o da base visa motivar a constru√ß√£o de modelos mais robustos a idiomas tipologicamente distantes, como o √Årabe, Bengali, Coreano, Russo, Telugo e Tail√¢ndes, podendo generalizar para outros dialetos. . . fonte . Question Answering para Node.js . A empresa Hugging Face liberou uma biblioteca para question answering baseada no DistilBERT, dando continuidade a sua miss√£o de tornar a √°rea de Processamento de Linguagem Natural mais acess√≠vel. O modelo apresentado pode rodar num ambiente de produ√ß√£o utilizando Node.js com apenas 3 linhas de c√≥digo, se benficiando da implementa√ß√£o eficiente oferecida pela Tokenizers, tamb√©m desenvolvida pelo Hugging Face, e a vers√£o em Javascript do TensorFlow (TensorFlow.js). . √âtica em IA üö® . Identificando vi√©s subjetivo em texto . Num podcast que contou com a participa√ß√£o de Diyi Yang, um pesquisador em ci√™ncia social computacional, foi discutido como sistemas de IA podem auxiliar na identifica√ß√£o de vi√©s subjetivo em informa√ß√µes textuais. Essa √© uma √°rea de pesquisa importante envolvendo Intelig√™ncia Artificial e NLP, especialmente quando discutimos sobre o consumo de textos, como t√≠tulos e chamadas de not√≠cias, e a facilidade que esses meios possuem para influenciar leitores com opini√µes subjetivas. . Do ponto de vista da aplica√ß√£o, se torna de vital import√¢ncia a tarefa de identifica√ß√£o desse vi√©s de maneira autom√°tica, assim como a conscientiza√ß√£o dos leitores, para que esses se tornem mais atentos e criteriosos em rela√ß√£o ao conte√∫do que est√£o consumindo. . Artificial Intelligence, Values and Alignment . A dissemina√ß√£o de sistemas de IA e a forma com que esses sistemas se alinham com os valores humanos √© uma √°rea de pesquisa envolvendo √©tica na Intelig√™ncia Artificial em crescente atividade. A DeepMind publicou recentemente um artigo que busca investigar de maneira mais profunda as quest√µes filos√≥ficas envolvidas no alinhamento da IA com os valores humanos. O trabalho discute duas frentes: a t√©cnica (como codificar valores que permitem que agentes de IA produzam resultados confi√°veis), e a normativa (quais princ√≠pios deveriam ser codificados pelos modelos), e como eles se relacionam e podem ser garantidos. O artigo encoraja uma abordagem baseada em princ√≠pios para o alinhamento de valores pelos sistemas de Intelig√™ncia, de modo a preservar um tratamento igualit√°rio e justo frente √†s diferen√ßas de convic√ß√µes e opini√µes. . Auditoria em Sistemas de IA . A VentureBeat divulgou que pesquisadores da Google, numa colabora√ß√£o com outros grupos, criaram um framework chamado SMACTR, que permite a auditoria de sistemas de IA. A motiva√ß√£o para esse trabalho envolve a aus√™ncia de ‚Äúpresta√ß√£o de contas‚Äù dos sistemas atuais, que s√£o colocados √† disposi√ß√£o do p√∫blico geral. A reportagem completa pode ser acessada aqui, assim como o trabalho completo. . Artigos e Postagens ‚úçÔ∏è . Destila√ß√£o de modelos em sistemas de NLP . Num novo epis√≥dio do podcast NLP Highlights, Thomas Wolf and Victor Sanh discutiram sobre a destila√ß√£o de modelos e como a t√©cnica pode ser utilizada como uma alternativa fact√≠vel para a compress√£o de grandes arquiteturas, como o BERT, para aplica√ß√µes escal√°veis de NLP em cen√°rios reais. A metodologia √© discutida no trabalho publicado pelos convidados, entitulado DistilBERT, onde s√£o constru√≠dos modelos menores (baseados na mesma arquitetura do modelo original) que tentam simluar o comportamento do modelo com maior n√∫mero de par√¢metros, de acordo com suas sa√≠das. Essencialmente, o menor modelo (student) tenta modelar a distribui√ß√£o de probabilidade do modelo maior (teacher) baseado na distribui√ß√£o emp√≠rica gerado por suas sa√≠das. . BERT, ELMo, &amp; GPT-2: How contextual are contextualized word representations? . O sucesso de m√©todos contextualizados como o BERT para resolu√ß√£o de uma ampla gama de tarefas complexas de NLP √© um assunto que est√° em voga no momento. Nesse post, Kawin Ethayarajh tenta responder a quest√£o que diz respeito √† qu√£o contextuais os modelos como BERT, o ELMo e o GPT-2 e seus respectivos word embedddings contextualizados s√£o. As caracter√≠sticas exploradas incluem m√©tricas de contextualidade, especificidade de contexto, al√©m de compara√ß√µes entre representa√ß√µes vetoriais de palavras ‚Äúest√°ticas‚Äù e suas vers√µes contextualizadas. . . fonte . Esparsidade em Redes Neurais . Fran√ßois Lagunas, pesquisador na √°rea de ML, escreveu esse excelente post compartilhando seu otimismo em rela√ß√£o √† utiliza√ß√£o de tensores esparsos em modelos de redes neurais. A expectativa √© empregar alguma forma de esparsidade visando a redu√ß√£o do tamanho dos modelos atuais, que de certa forma est√£o se tornando impratic√°veis, dadas suas colossais quantidades de par√¢metros. Os Transformers, por exemplo, com seus bilh√µes de par√¢metros, poderiam se beneficar com o emprego dessa t√©cnica. . Entretanto, os detalhes de implementa√ß√£o para viabilizar a utiliza√ß√£o eficiente da esparsidade em GPU ainda n√£o est√£o claros‚Ä¶ Felizmente, a comunidade de Aprendizado de M√°quina j√° est√° trabalhando nisso! . Treinando Seu Pr√≥prio Modelo de Linguagem . Se voc√™ est√° interessado em aprender como treinar um modelo de linguagem do zero, confira esse excelente tutorial da Hugging Face que utiliza as suas incr√≠veis bibliotecas Tokenizers e Transformers no treinamento do modelo. . Tokenizers: How machines read . Cathal Horan publicou um blog post impresssionante e bem detalhado sobre como e quais tipos de tokenizers v√™m sendo utilizados nos mais recentes modelos de NLP, auxiliando modelos de Intelig√™ncia a aprender por meio de informa√ß√µes textuais. O post tamb√©m discute e motiva porqu√™ a tarefa de tokeniza√ß√£o √© uma importante e desafiadora √°rea de pesquisa ativa. O artigo apresenta ainda como treinar o seu pr√≥prio tokenizer utilizando m√©todos como o SentencePiece e o WordPiece. . . fonte . Educa√ß√£o üéì . Machine Learning na VU Amsterdam . Agora voc√™ pode acompanahar o curso 2020 MLVU machine learning pela internet, onde est√£o inclusas a lista completa de slides, videos e o plano de estudos. O curso oferece uma introdu√ß√£o √† ML, al√©m de cobrir t√≥picos mais avan√ßados de Deep Learning, como Variational AutoEncoders (VAEs) e Redes Neurais Adversariais (GANs). . . fonte . Materiais de Matem√°tica para ML . Suzana Iliƒá e a organiza√ß√£o Machine Learning Tokyo (MLT) v√™m realizando um excelente trabalho em prol da democratiza√ß√£o do conhecimento em ML. Confira esse reposit√≥rio que apresenta uma cole√ß√£o de fontes e materiais sobre os fundamentos matem√°ticos utilizados em Aprendizado de M√°quina. . Introduction to Deep Learning . Acompanhe o curso ‚ÄúIntroduction to Deep Learning‚Äù do MIT nesse site. Novas aulas ser√£o postadas toda semanas e todos os materiais, como slides, v√≠deos e c√≥digos utilizados, ser√£o publicados. . Deep Learning com PyTorch . Alfredo Canziani publicou os slides e notebooks utilizados no minicurso de Deep Learning com Pytorch. O reposit√≥rio cont√©m ainda um site que incluem notas sobre os conceitos apresentados no curso. . Missing Semester of Your CS . O ‚ÄúMissing Semester of Your CS‚Äù √© um excelente curso online composto por recursos que podem ser potencialmente √∫teis para cientistas de dados que n√£o possuem background na √°rea de desenvolvimenteo. Est√£o inclusos materiais sobre shell scripting e versionamento. O curso foi disponibilizado por alunos do MIT. . fonte . Advanced Deep Learning . A CMU disponibilizou recentemente os slides e plano de estudos para o curso ‚ÄúAdvanced Deep Learning‚Äù, que cobre t√≥picos como modelos autoregressivos, modelos generativos, aprendizado autosupervisionado, entre outros. O p√∫blico-alvo s√£o alunos de mestrado e doutorado com s√≥lidos conhecimentos de ML. . Men√ß√µes honrosas ‚≠êÔ∏è . Voc√™ pode encontrar a √∫ltima Newsletter aqui. Essa edi√ß√£o cobriu t√≥picos como melhorias em agentes conversacionais, divulga√ß√£o de modelos BERT para idiomas espec√≠ficos (entre eles o Portugu√™s!!!), bases de dados publicamente dispon√≠vies, introdu√ß√£o de novas bibliotecas para Deep Learning, e muito mais. . Em Xu et al. (2020), foi proposto um m√©todo para progressivamente substituir e comprimir modelos BERT atrav√©s da separa√ß√£o de seus componentes originais. Atrav√©s dessa substitui√ß√£o progressiva, aliado ao processo de treinamento, √© poss√≠vel combinar os componentes originais e suas vers√µes compactadas. A metodologia apresentada supera outras abordagens de knowledge distillation no benchmark GLUE. . Um outro curso interessante √© o ‚ÄúIntroduction to Machine Learning‚Äù, que cobre o b√°sico de ML, regress√£o supervisionada, random forests, ajuste de par√¢metros, dentre outros conceitos fundamentais. . A vers√£o para üá¨üá∑ grego do BERT (GreekBERT) est√° dispon√≠vel para uso atrav√©s da biblioteca Transformers, da Hugging Face. . Jeremy Howard publicou um artigo descrevendo a biblioteca de Deep Learning fast.ai, que √© amplamente utilizada para pesquisa e ensino em seus cursos de Deep Learning. Uma leitura bastante recomendada para desenvolvedores de software que trabalham construindo e melhorando pacotes de Aprendizado de M√°quina e Deep Learning. . A Deeplearning.ai completou o lan√ßamento dos seus 4 cursos da s√©rie TensorFlow: Data and Deployment Specialization. A especializa√ß√£o visa ensinar desenvolverdores a como realizar o deploy de modelos de maneira efetiva e eficiente nos mais diferentes cen√°rios, al√©m de utilizar dados de maneiras eficazes durante o treinamento de modelos. . Sebastian Raschka publicou recentemente um artigo entitulado ‚ÄúMachine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence‚Äù. O trabalho apresenta uma revis√£o bastante acess√≠vel √†s mais variadas ferramentas de ML. √â um artigo excelente para compreender as vantagens de algumas bibliotecas e conceitos utilizados em Aprendizado de M√°quina. Al√©m disso, levanta-se a discuss√£o sobre o futuro de bibliotecas de ML baseadas em Python. .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html",
            "relUrl": "/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html",
            "date": " ‚Ä¢ Feb 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I‚Äôm Victor Garritano! . Currently I‚Äôm a M.Sc. Candidate in AI at PESC - COPPE - UFRJ. I also work as Data Science Intern at TWIST Systems. . In this blog you will find my contributions for NLP Newsletter, and much more! .",
          "url": "https://victorgarritano.github.io/personal_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}