{
  
    
        "post0": {
            "title": "NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶",
            "content": ". Seja muito bem-vindo a s√©tima edi√ß√£o da NLP Newsletter. Esperamos que voc√™ tenha um dia incr√≠vel e que voc√™ as pessoas que voc√™ ama estejam em seguran√ßa nessas semanas dif√≠ceis. N√≥s decidimos publicar essa edi√ß√£o na esperan√ßa de trazer mais alegria aos nossos leitores. Sendo assim, por favor leia a Newsletter durante o seu tempo livre. Nesse momento, √© importante mantermos o foco no que √© a verdadeira prioridade - nossa fam√≠lia e amigos. ‚ù§Ô∏è üíõ üíö . Algumas atualiza√ß√µes sobre a NLP Newsletter e a dar.ai . Todas as tradu√ß√µes em franc√™s e em chin√™s das edi√ß√µes anteriores est√£o agora dispon√≠veis. Descubra como voc√™ pode contribuir com a tradu√ß√£o das edi√ß√µes anteriores (assim como as futuras!) da Newsletter nesse link. . Nota do tradutor: As tradu√ß√µes de todas as edi√ß√µes da Newsletter, exceto a 3¬™, para portugu√™s tamb√©m est√£o dispon√≠veis! | . N√≥s criamos recentemente dois reposit√≥rios no Github que cont√™m resumos de artigos de NLP e notebooks utilizando PyTorch para que voc√™ possa come√ßar a ter experi√™ncia com redes neurais. . Pesquisas e Publica√ß√µes üìô . Measuring Compositional Generalization . No contexto de Aprendizado de M√°quina, compositional generalization se refere a habilidade de representar o conhecimento aprendido com a base de dados e aplic√°-lo a novos e diferentes contextos. At√© o presente momento, n√£o estava claro como medir essa composicionalidade nas redes neurais. Recentemente, o time de IA da Google apresentou um dos maiores benchmarks para compositional generalization, utilizando tarefas como question answering e semantic parsing. A imagem abaixo apresenta um exemplo do modelo proposto utilizando os chamados √°tomos (unidades utilizadas para se gerar os exemplos) para que sejam produzidos compostos (novas combina√ß√µes dos √°tomos). A ideia deste trabalho √© construir bases de treino e teste que combinam exemplos que possuem a mesma distribui√ß√£o pelos diferentes √°tomos mas com distribui√ß√µes diferentes sobre os compostos. Os autores argumentam que essa √© uma maneira mais confi√°vel de se testar a compositional generalization. . . Cr√©dito: Google AI Blog . Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping . Pesquisadores testaram uma s√©rie de procedimentos de refinamento (fine-tuning) com o objetivo de compreender melhor o efeito das diferentes estrat√©gias de inicializa√ß√£o de pesos e pol√≠ticas de early stopping no desempenho de modelos de linguagem. Atrav√©s de exaustivos experimentos de refinamento do BERT, foi constatado que seeds aleat√≥rias distintas produzem resultados bastante discrepantes. Em particular, o estudo reporta que certas inicializa√ß√µes de pesos de fato conferem ao modelo um bom desempenho em diversas tarefas. Todas as bases e testes realizados foram disponibilizadas, para uso de outros pesquisadores interessados em entender as din√¢micas que ocorrem durante o fine-tuning de maneira mais aprofundada. . Zoom In: An Introduction to Circuits . Pesquisadores da OpenAI publicaram uma postagem discutindo o estado atual da tarefa de interpretabilidade de redes neurais, assim como uma nova abordagem para a interpreta√ß√£o das mesmas. Inspirada pela biologia celular, os autores buscaram entender modelos de vis√£o computacional e o que eles aprendem de maneira bastante aprofundada, atrav√©s da inspe√ß√£o dos pesos do modelo. Basicamente, o estudo apresentou algumas conclus√µes, obtidas a partir dos experimentos realizados, as quais eles acreditam que possam ser utilizadas como base para uma melhor interpreta√ß√£o das redes neurais. . . NLP Research Highlights‚Ää‚Äî‚ÄäIssue #1 . Numa nova iniciativa da dar.ai, a NLP Research Highlights, s√£o fornecidas descri√ß√µes detalhadas de t√≥picos atuais e bem importantes da pesquisa em NLP. A ideia √© que essa iniciativa seja utilizada para acompanhar os avan√ßos da √°rea atrav√©s de resumos acess√≠veis desses trabalhos. Na primeira edi√ß√£o trimestral, os t√≥picos abordados tratam sobre melhorias em modelos de linguagem e em agentes conversacionais para sistemas de reconhecimento de voz. Os resumos s√£o mantidos aqui. . Learning to Simulate Complex Physics with Graph Networks . Nos √∫ltimos meses, as Graph Neural Networks (GNNs) (redes neurais que operam sobre redes) foram um assunto recorrente nas edi√ß√µes da Newsletter, devido a sua efetividade em tarefas n√£o s√≥ da √°rea de NLP como tamb√©m em gen√¥mica e materiais. Um artigo publicado recentemente, prop√µe um framework geral baseado em GNNs que √© capaz de realizar simula√ß√µes f√≠sicas em diferentes cen√°rios, como fluidos e materiais male√°veis. Os autores argumentam que eles obtiveram um desempenho estado-da-arte nesses diferentes contextos e que a abordagem proposta √© possivelmente o melhor simulador treinado da atualmente. Os experimentos realizados incluem a simula√ß√£o de materiais como fluidos viscosos sobre a √°gua e outras intera√ß√µes com objetos r√≠gidos. Tamb√©m foi testado um modelo pr√©-treinado em tarefas out-of-distribution e os resultados obtidos foram bastante promissores, evidenciando o potencial de generaliza√ß√£o para outros cen√°rios. . . (Sanchez-Gonzalez et al., 2020) . Modelos BERT para idiomas espec√≠ficos . O BERT √Årabe (AraBERT) est√° agora dispon√≠vel na biblioteca de Transformers da Hugging Face. Voc√™ pode acessar o modelo aqui e o artigo aqui. . Recentemente, uma vers√£o em japon√™s do BERT tamb√©m foi disponibilizada. Uma vers√£o em polon√™s tamb√©m est√° dispon√≠vel, batizada como Polbert. . Criatividade, √âtica e Sociedade üåé . Computational predictions of protein structures associated with COVID-19 . A DeepMind publicou suas predi√ß√µes de estruturas das prote√≠nas que se ligam ao v√≠rus causador da COVID-19. As predi√ß√µes foram obtidas diretamente do sistema AlphaFold, embora n√£o tenham sido verificadas experimentalmente. A ideia √© que essa publica√ß√µes encorajem outras contribui√ß√µes que busquem entender melhor e v√≠rus e suas fun√ß√µes. . Court cases that sound like the weirdest fights . Janelle Shane compartilhou os resultados de um divertido experimento onde um modelo do GPT-2 foi refinado para gerar processos judiciais contra objetos inanimados. Foi disponibilizado ao modelo uma lista de processos do governo sobre apreens√µes de objetivos contrabandeados e artefatos perigosos, e foram geradas acusa√ß√µes como as apresentadas na imagem abaixo. . . fonte . Toward Human-Centered Design for ML Frameworks . A Google AI publicou os resultados de uma grande pesquisa com 645 pessoas que utilizaram a vers√£o do TensorFlow para JavaScript. O objetivo era entender quais eram as funcionalidades mais importantes da biblioteca para desenvolvedores fora da √°rea de ML, assim como a sua experi√™ncia com as atuais bibliotecas de Aprendizado de M√°quina. Uma das conclus√µes obtidas mostra que a falta de entendimento conceitual de ML dificulta a utiliza√ß√£o de bibliotecas espec√≠ficas para esse grupo de usu√°rios. Os participantes do estudo tamb√©m reportaram a necessidade de instru√ß√µes mais acess√≠veis sobre como aplicar modelos de ML em diferentes problemas e um suporte mais expl√≠cito para modifica√ß√µes do usu√°rio. . Face and hand tracking in the browser with MediaPipe and TensorFlow.js . Este excelente artigo do TensorFlow apresenta um passo-a-passo para habilitar um sistema de tracking do rosto e das m√£os diretamente no navegador utilizando o TensorFlow.js e o MediaPipe. . . Cr√©ditos: Blog do TensorFlow . Ferramentas e Bases de Dados ‚öôÔ∏è . NLP Paper Summaries . N√≥s criamos recentemente um [reposit√≥rio]https://github.com/dair-ai/nlp_paper_summaries) contendo uma lista de resumos de artigos de NLP cuidadosamente formulados, para alguns dos mais interessantes e importantes papers da √°rea nos √∫ltimos anos. O foco principal da iniciativa √© expandir a acessibilidade do p√∫blico-geral √† t√≥picos e pesquisas de NLP. . . Uma biblioteca de vis√£o computacional diferenci√°vel em PyTorch . A Kornia √© uma biblioteca constru√≠da sobre o PyTorch que permite a utiliza√ß√£o de uma s√©rie de operadores para vis√£o computacional diferenci√°vel utilizando o PyTorch. Algumas das funcionalidades incluem transforma√ß√µes em images, depth estimation, processamento de imagens em baixo-n√≠vel, dentre v√°rias outras. O m√≥dulo √© fortemente inspirado no OpenCV, com a diferen√ßa de ser focado em pesquisa, ao inv√©s de aplica√ß√µes prontas para produ√ß√£o. . . Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train . DIET (Dual Intent and Entity Transformer) √© uma arquitetura multi-tarefa de natural language understanding (NLU) proposta pela Rasa. A framework foca no treinamento multi-tarefa, com o objetivo de melhorar o desempenho nos problemas de classifica√ß√£o de inten√ß√µes e reconhecimento de entidades nomeadas. Outros benef√≠cios do DIET incluem a flexibilidade de utiliza√ß√£o de qualquer embedding pr√©-treinado, como o BERT e o GloVe. O foco principal, entretanto, √© disponibilizar um modelo que ultrapassa o estado-da-arte atual nessas tarefas e que seja mais r√°pido de treinar (o speedup reportado foi de 6x!). O modelo est√° dispon√≠vel na biblioteca rasa. . . framework DIET . Perdido no meio dos modelos BERT? . O BERT Lang Street √© uma plataforma que possui a capacidade de buscar por mais de 30 modelos baseados no BERT, em 18 idiomas e 28 tarefas, totalizando 177 entradas em sua base de dados. Dessa forma, se voc√™ quiser descobrir o estado-da-arte para a tarefa de classifica√ß√£o de sentimentos utilizando modelos BERT, basta procurar por ‚Äúsentiment‚Äù na barra de busca (como exemplificado abaixo). . . Med7 . O Andrey Kormilitzin disponibilizou o Med7, que √© um modelo para NLP (em particular Reconhecimento de Entidades Nomeadas (NER)) em relat√≥rios m√©dicos eletr√¥nicos. O modelo √© capaz de identificar at√© 7 categorias de entidades e est√° dispon√≠vel para uso com a biblioteca spaCy. . . . Uma biblioteca em c√≥digo-aberto para Quantum Machine Learning . TensorFlow Quantum √© uma biblioteca que fornece uma s√©rie de funcionalidades para a prototipagem r√°pida de modelos qu√¢nticos de ML, possibilitando a aplica√ß√£o destes em problemas em √°reas como a medicina e materiais. . Fast and Easy Infinitely Wide Networks with Neural Tangents . A Neural Tangents √© uma biblioteca que permite aos pesquisadores construir e treinar modelos de dimens√£o infinita e redes neurais utilizando a JAX. Leia a postagem de lan√ßamento aqui e acesse a biblioteca aqui. . . Artigos e Postagens ‚úçÔ∏è . From PyTorch to JAX: towards neural net frameworks that purify stateful code . Sabrina J. Mielke publicou um artigo com um passo-a-passo que ilustra a constru√ß√£o e treinamento de redes neurais utilizado o JAX. A postagem busca comparar o funcionamento interno das redes com o PyTorch e o JAX, o que auxilia num melhor entendimento dos benef√≠cios e diferen√ßas entra as duas bibliotecas. . . fonte . Why do we still use 18-year old BLEU? . Nesse blog post, Ehud Reiter discorre sobre porqu√™ n√≥s ainda utilizamos t√©cnicas de avalia√ß√£o antigas como BLUE para mensurar o desempenho de modelos de NLP em tarefas como tradu√ß√£o autom√°tica (machine translation). Como um pesquisador da √°rea, ele conta sobre as implica√ß√µes para t√©cnicas que realizam a avalia√ß√£o em tarefas de NLP mais recentes. . Introducing BART . O BART √© um novo modelo proposto pelo Facebook que consiste num denoising autoencoder para o pr√©-treinamento de modelos sequence-to-sequence, que pode melhorar o desempenho dos mesmos em tarefas como sumariza√ß√£o abstrata. Sam Shleifer disponibilizou um resumo interessante do BART e como ele realizou a integra√ß√£o do modelo na biblioteca Transformers da Hugging Face. . A Survey of Long-Term Context in Transformers . Madison May escreveu recentemente um compilado bastante interessante descrevendo estrat√©gias para melhorar abordagens baseadas em Transformers, que incluem Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer, e routing transformer. Alguns dos modelos j√° haviam aparecido em publica√ß√µes da dar.ai e na lista de resumos de artigos. . ‚ÄúMind your language, GPT-2‚Äù: how to control style and content in automatic text writing . Apesar da flu√™ncia impressionante na escrita autom√°tica de texto evidenciada no ano passado, continua sendo um desafio controlar atributos como estrutura ou conte√∫do em textos gerados por modelos neurais. Numa postagem recente, Manuel Tonneau discute o progresso atual e as perspectivas na √°rea de gera√ß√£o de texto parametriz√°vel, como o modelo GPT-2 da Hugging Face refinado no arXiv e o T5 da Google, al√©m do CTRL da Salesforce e do PPLM do time de IA da Uber. . Educa√ß√£o üéì . Talk: The Future of NLP in Python . Em uma de nossas edi√ß√µes anteriores, foi apresentado o THiNC, uma biblioteca funcional de Deep Learning focada na compatibilidade com outras j√° existentes. Essa apresenta√ß√£o, utilizada pela Ines Montani na PyCon Colombia, introduz a biblioteca mais profundamente. . Transformers Notebooks . A Hugging Face publicou uma cole√ß√£o de notebooks no Colab que auxilia no in√≠cio da utiliza√ß√£o de sua biblioteca Transformers. Alguns notebooks incluem o uso de tokeniza√ß√£o, configura√ß√£o de pipelines de NLP, e o treinamento de modelos de linguagem em bases de dados pr√≥prias. . . TensorFlow 2.0 in 7 hours . Confira esse curso gr√°tis de ~7 horas sobre o TensorFlow 2.0, onde s√£o cobertos t√≥picos como o b√°sico de redes neurais, NLP com redes neurais recorrentes (RNNs) e uma introdu√ß√£o ao Aprendizado por Refor√ßo. . DeepMind: The Podcast . A DeepMind liberou todos os epis√≥dios (numa playlist no YouTube) do seu podcast com cientistas, pesquisadores e engenheiros, onde s√£o discutidos t√≥picos como *Artificial General Intelligence, neuroci√™ncia e rob√≥tica. . Cursos de Machine Learning and Deep Learning . A Berkeley est√° disponibilizando publicamente o plano de estudos do seu curso em ‚ÄúDeep Unsupervised Learning‚Äù, focado principalmente nos aspectos te√≥ricos do self-supervised learning e em modelos generativos. Outros t√≥picos incluem modelos de vari√°veis latentes, modelos autorregressivos e flow models. As aulas e os slides tamb√©m est√£o dispon√≠veis. . N√≥s tamb√©m encontramos essa lista impressionante de cursos avan√ßados de ML, NLP e Deep Learning dispon√≠vel de maneira online. . E aqui est√° um outro curso intitulado ‚ÄúIntroduction to Machine Learning‚Äù que aborda assuntos como regress√£o supervisionada, avalia√ß√£o de desempenho, random forests, ajuste de par√¢metros, dicas pr√°ticas e muito mais. . Men√ß√µes Honrosas ‚≠êÔ∏è . A edi√ß√£o anterior da Newsletter (6¬™ edi√ß√£o) est√° dispon√≠vel aqui. . Connon Shorten publicou um v√≠deo explicando o modelo ELECTRA, que prop√µe a utiliza√ß√£o de uma t√©cnica chamada replaced token detection como forma de pr√©-treinar Transformers de maneira mais eficiente. Se voc√™ tiver interesse em saber mais, n√≥s tamb√©m escrevemos um breve resumo do modelo aqui. . Rachael Tatman est√° trabalhando numa nova s√©rie denominada NLP for Developers onde o objetivo √© discutir diferentes m√©todos de NLP de maneira mais aprofundada, quando utiliz√°-los e como lidar com dificuldades comuns apresentadas por essas t√©cnicas. . A DeepMind liberou o AlphaGo‚Ää‚Äî‚ÄäThe Movie no YouTube para celebrar o 4¬∫ anivers√°rio da vit√≥ria do modelo sobre o Lee Sedol no jogo de Go. . A OpenMined est√° com vagas abertas para os cargos de Research Engineer e Research Scientist, que parecem ser boas oportunidades para se envolver com privacy-preserving AI. . . Se voc√™ conhecer bases de dados, projetos, postagens, tutoriais ou artigos que voc√™ gostaria de ver na pr√≥xima edi√ß√£o da Newsletter, sinta-se a vontade para nos contactar atrav√©s do e-mail ellfae@gmail.com ou de uma mensagem direta no twitter. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html",
            "relUrl": "/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html",
            "date": " ‚Ä¢ Mar 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML",
            "content": ". . Seja muito bem-vindo √† sexta edi√ß√£o da NLP Newsletter. Agradecemos por todo o suporte e dedica√ß√£o √† leitura dos temas mais recentes em ML e NLP. Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais. . Algumas atualiza√ß√£oes sobre a NLP Newsletter e o dair.ai . . N√≥s estamos traduzindo a Newsletter para outros idiomas, como o Portugu√™s Brasileiro, Chin√™s, √Årabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as tradu√ß√µes ü§ó. Voc√™ tamb√©m pode contribuir aqui! . . No m√™s passado, n√≥s realizamos o lan√ßamento oficial do nosso novo website. Voc√™ pode dar uma olhada em nossa organiza√ß√£o no GitHub para mais informa√ß√µes sobre os projetos em andamento. Se voc√™ est√° interessado em saber mais sobre as contribui√ß√µes j√° realizadas para a dar.ai, ou mesmo contribuir para a democratiza√ß√£o das tecnologias, ensino e pesquisa sobre Intelig√™ncia Artificial, veja nossa se√ß√£o de issues. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! . Publica√ß√µes üìô . A Primer in BERTology: What we know about how BERT works . . Modelos baseados no Transformer mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como sequence labeling e question answering. Um desses modelos, o BERT (Devlin et al. 2019), vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo artigo entitulado ‚ÄúA Primer in BERTology: What we know about how BERT works‚Äù busca come√ßar a responder quest√µes sobre as raz√µes que possibilitam o BERT funcionar t√£o bem em tantas tarefas de NLP. Alguns dos t√≥picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo √© representado, al√©m de m√©todos que outros pesquisadores est√£o utilizando para melhorar o processo de aprendizado. . Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer . . A Google AI publicou recentemente um m√©todo que incorpora todas as li√ß√µes aprendidas e melhorias do Transfer Learning para NLP num franework unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho prop√µe que a maioria das tarefas de NLP podem ser formuladas no formato text-to-text, onde tanto a entrada quanto a sa√≠da do problema apresentam-se na forma de texto. Os autores alegam que ‚Äúesse framework fornece uma fun√ß√£o objetivo para treinamento que √© consistente tanto na fase de pr√©-treinamento quanto no fine-tuning‚Äù. O T5 √© essencialmente um encoder-decoder baseado no Transformer, com v√°rias melhorias, em especial nos componentes de aten√ß√£o da arquitetura. O modelo foi pr√©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como Colossal Clean Crawled Corpus, onde foi estabelecido um novo estado-da-arte para tarefas como sumariza√ß√£o, question answering e classifica√ß√£o de texto. . . (Raffel et al. 2020) . 12-in-1: Multi-Task Vision and Language Representation Learning . . Os esfor√ßos de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avan√ßos na √°rea de lingu√≠stica e vis√£o computacional, mesmo quando os conhecimentos necess√°rios para abordar essas tarefas possuem interse√ß√£o. Um novo artigo (que ser√° apresentado na CVPR) prop√µe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de lingu√≠stica e vis√£o computacional, gerando uma modelo mais gen√©rico para as mesmas. O m√©todo reduz a quantidade de par√¢metros e apresenta um bom desempenho em problemas como recupera√ß√£o de imagens baseadas em legendas, e question answering visual. . . (Lu et al. 2020) . BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations . . Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT √© capaz de gerar representa√ß√µes que generalizam para outras √°reas, al√©m de texto e vis√£o computacional. Os autores apresentam um modelo denominado BERT-gen, que tira proveito de representa√ß√µes mono e multi-modais para obter desempenhos superiores em bases de dados de gera√ß√µes de perguntas baseadas em imagens. . . (Scialom et al. 2020) . Criatividade e Sociedade üé® . The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence . . Gary Marcus publicou recentemente um trabalho onde ele explica a s√©rie de passos que, na opini√£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo √© priorizar a constru√ß√£o de sistemas h√≠bridos e orientados √† conhecimento, guiados por modelos cognitivos, ao inv√©s da proposi√ß√£o de modelos com mais par√¢metros que exigem mais dados e poder computacional. . 10 Breakthrough Technologies 2020 . . A revista MIT Technology Review publicou a lista dos 10 avan√ßos tecnol√≥gicos que segundo eles far√£o a diferen√ßa na resolu√ß√£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista ‚Äî sem ordem espec√≠fica ‚Äî inclui a internet n√£o-hack√°vel, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, mol√©culas descobertas por sistemas de IA, mega-constela√ß√µes de sat√©lites artificias, supremacia qu√¢ntica, IA em aparelhos celulares, privacidade diferencial e climate attribution. . Time to rethink the publication process in machine learning . . Yoshua Bengio escreveu recentemente sobre suas preocupa√ß√µes em rela√ß√£o aos atuais ciclos acelerados de publica√ß√µes em Aprendizado de M√°quina. O ponto principal √© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e s√£o apenas incrementais, deixando o investimento de tempo na revis√£o e verifica√ß√£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes s√£o aqueles que precisam lidar com as consequ√™ncias negativas da press√£o e estresse gerados por essa situa√ß√£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas a√ß√µes para ajudar no processo de desacelera√ß√£o das publica√ß√µes para o bem da ci√™ncia. . Ferramentas e Bases de Dados ‚öôÔ∏è . Implementa√ß√£o da PointerGenerator network com a AllenNLP . . Redes Pointer-Generator buscam aprimorar o mecanismo de aten√ß√£o de modelos sequence-to-sequence e s√£o utilizadas para melhorar o desempenho em tarefas como sumariza√ß√£o abstrata. Se voc√™ gostaria de utilizando essa t√©cnica com a framework AllenNLP, saiba que o Kundan Krishna desenvolveu um m√≥dulo que permite a execu√ß√£o de um modelo pr√©-treinado dessa categoria, al√©m do treinamento de um novo modelo do zero. . . Question answering para diferentes idiomas . . Com a dissemina√ß√£o de modelos baseados no Transformer e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esfor√ßo significativo na constru√ß√£o e libera√ß√£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder compartilhou uma lista de datasets que podem ser utilizados no desenvolvimento de m√©todos para question answering em diversas l√≠nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), KorQuAD, SberQuAD, FQuAD, Arabic-SQuAD, SQuAD-it e Spanish SQuAD. . PyTorch Lightning . . A PyTorch Lightning √© uma ferramenta que possibilita a abstra√ß√£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), al√©m do uso de precis√£o de 16 bits. Fazer essas configura√ß√µes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em v√°rias GPUs/TPUs sem a necessidade de altera√ß√£o do c√≥digo. . Graph Neural Networks no TF2 . . O time de pesquisa da Microsoft liberou uma biblioteca com a implementa√ß√£o de diversas arquiteturas de Graph Neural Networks (GNNs). A biblioteca, baseada na vers√£o 2.0 do TensorFlow, fornece funcionalidades para manipula√ß√£o de dados que podem ser utilizadas diretamente nas itera√ß√µes de treino/avalia√ß√£o. . Pre-training SmallBERTa‚Ää‚Äî‚ÄäA tiny model to train on a tiny dataset . . Voc√™ j√° pensou em treinar o seu pr√≥prio modelo de linguagem do zero, mas nunca teve o poder computacional necess√°rio para isso? Se j√°, ent√£o o Aditya Malte pode lhe ajudar com esse excelente notebook no Colab que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida. . √âtica em IA üö® . Why faces don‚Äôt always tell the truth about feelings . H√° algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emo√ß√µes em contextos visuais ou textuais. Um novo artigo reabre o debate que t√©cnicas de IA que tentam reconhecer emo√ß√µes diretamente de imagens faciais n√£o est√£o fazendo seu trabalho direito. O argumento principal, formulado por psic√≥logos proeminentes na √°rea, √© que n√£o existe evid√™ncia da exist√™ncia de express√µes universais que possam ser utilizadas na detec√ß√£o de emo√ß√µes de maneira independente. Seria necess√°ria uma melhor compreens√£o de tra√ßos de personalidade e movimentos corporais por parte do modelo, dentre outras caracter√≠sticas, para que seja poss√≠vel detectar as emo√ß√µes humanas de maneira mais precisa. . Differential Privacy and Federated Learning Explicadas . . Uma das considera√ß√µes √©ticas que devem ser levadas em considera√ß√£o durante a constru√ß√£o de sistemas de IA √© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atrav√©s da differential privacy ou do federated learning. Se voc√™ quiser saber mais sobre esses dois t√≥picos, Jordan Harrod produziu uma excelente introdu√ß√£o nesse v√≠deo, que inclui uma sess√£o hands-on utilizando notebooks do Colab. . Artigos e Postagens ‚úçÔ∏è . A Deep Dive into the Reformer . . Madison May realizou uma postagem em seu blog que fornece uma an√°lise mais profunda do Reformer, um novo modelo baseado no Transformer, proposto recentemente pela Google AI. O Reformer j√° havia aparecido numa edi√ß√£o anterior da Newsletter. . Uma plataforma de blogs gratuita . A fastpages permite a cria√ß√£o e configura√ß√£o autom√°tica de um blog utilizando a GitHub pages de maneira gratuita. Essa solu√ß√£o simplifica o processo de publica√ß√£o e tamb√©m oferece suporte √† utiliza√ß√£o de documentos exportados e Jupyter notebooks. . Dicas para entrevistas na Google . . Pablo Castro, do time da Google Brain, publicou uma excelente postagem destacando as principais dicas para aqueles interessados em aplicar para uma posi√ß√£o na Google. Os t√≥picos abordados incluem dicas sobre o processo de entrevistas, como prepara√ß√£o, o que esperar durante e o que acontece depois delas. . Transformers are Graph Neural Networks . . Graph Neural Networks (GNNs) e Transformers mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente artigo em seu blog, evidenciando a conex√£o entre GNNs e Transformers, e as diversas maneiras pelas quais esses m√©todos podem ser combinados e utilizados em conjunto. . . Representa√ß√£o de uma frase como um grafo completo de palavras‚Ää‚Äî‚Ääfonte . CNNs e Equivari√¢ncia . . Fabian Fuchs e Ed Wagstaff discutiram a import√¢ncia da equivari√¢ncia e como as Convolutional Neural Networks (CNNs) garantem essa propriedade. O conceito √© apresentado e discutido posteriormente no contexto de CNNs em rela√ß√£o √† transla√ß√£o. . Self-supervised learning com imagens . A t√©cnica de self-supervised learning foi amplamente discutida nas edi√ß√µes anteriores da Newsletter devido ao seu papel em modelos recentes para language modeling. Esse blog post, feito pelo Jonathan Whitaker, fornece uma explica√ß√£o intuitiva da t√©cnica de aprendizado no contexto de imagens. Se voc√™ deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tamb√©m publicou um artigo interessante descrevendo o conceito de maneira visual. . Educa√ß√£o üéì . Stanford CS330: Deep Multi-Task and Meta-Learning . A universidade de Stanford liberou recentemente suas v√≠deo-aulas, numa playlist no YouTube, para o novo curso em deep multi-task e meta-learning. Os assuntos apresentados incluem bayesian meta-learning, lifelong learning, uma vis√£o geral sobre aprendizado por refor√ßo, model-based reinforcement learning, entre outros. . PyTorch Notebooks . A dar.ai liberou recentemente um compilado de notebooks apresentando uma introdu√ß√£o √† redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos t√≥picos j√° dispon√≠veis incluem como implementar um modelo de regress√£o log√≠stica do zero, assim como a programa√ß√£o de redes neurais feed-forward e recorrentes. Notebooks no Colab est√£o dispon√≠veis no GitHub. . The fastai book (draft) . Jeremy Howard e Sylvain Gugger liberaram uma lista com alguns notebooks para um futuro curso que introduz conceitos de Deep Learning e como implementar diferentes m√©todos utilizando o PyTorch e a biblioteca da fastai. . Cursos gratuitos de Ci√™ncia de Dados . . O Kaggle disponibilizou uma s√©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontap√© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdu√ß√£o ao Aprendizado de M√°quina e ao Python, Visualiza√ß√£o de Dados, Feature Engineering, Deep Learning, entre outros. . . Um outro excelente curso online de Ci√™ncia de Dados disponibiliza notas de aulas, slides e notebooks sobre t√≥picos que v√£o desde an√°lise explorat√≥ria at√© interpreta√ß√£o de modelos para Processamento de Linguagem Natural. . . 8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch . . A nepture.ai publicou um excelente artigo que cont√©m discuss√µes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na cria√ß√£o do PyTorch e nas ferramentas constru√≠das com base na biblioteca. . Visualizando Adaptive Sparse Attention Models . . Sashs Rush compartilhou um notebook impressionante que explica e mostra os detalhes t√©cnicos sobre como produzir sa√≠das esparsas com a softmax e induzir esparsidade nos componentes de aten√ß√£o do modelo Transformer, auxiliando na atribui√ß√£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade. . . Visualizando a distribui√ß√£o de probabilidade da sa√≠da da softmax . Men√ß√µes Honrosas ‚≠êÔ∏è . Voc√™ pode conferir a edi√ß√£o da passada da üóû Newsletter aqui. . . Conor Bell escreveu esse script em Python que permite a visualiza√ß√£o e prepara√ß√£o de uma base de dados que pode ser utilizada no modelo StyleGAN. . . Manu Romero compartilhou um modelo de POS tagging para o espanhol. O modelo est√° dispon√≠vel para uso utilizando a biblioteca Transformers da Hugging Face. Ser√° interessante acompanhar a divulga√ß√£o de modelos para outros idiomas. . Esse reposit√≥rio cont√©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem rela√ß√£o com o BERT e abordam diversos problemas como compress√£o de modelos, tarefas de dom√≠nios espec√≠ficos, entre outros. . . Connor Shorten publicou um v√≠deo de 15 minutos explicando um novo framework que busca reduzir o efeito das ‚Äúshortcut‚Äù features no self-supervised representation learning. Essa √© uma tarefa importante porqu√™, caso n√£o seja realizada corretamente, o modelo pode falhar em aprender representa√ß√µes sem√¢nticas √∫teis e potencialmente se tornar ineficiente durante o transfer learning. . Sebastian Ruder publicou uma nova edi√ß√£o da newsletter NLP News, que apresenta t√≥picos e recursos como an√°lises de artigos de ML e NLP em 2019, e apresenta√ß√µes sobre os fundamentos do Deep Learning e Transfer Learning. Confira aqui. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "relUrl": "/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "date": " ‚Ä¢ Mar 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://victorgarritano.github.io/personal_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I‚Äôm Victor Garritano! . Currently I‚Äôm a M.Sc. Candidate in AI at PESC - COPPE - UFRJ. I also work as Data Science Intern at TWIST Systems. . In this blog you will find my contributions for NLP Newsletter, and much more! .",
          "url": "https://victorgarritano.github.io/personal_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}