{
  
    
        "post0": {
            "title": "NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML",
            "content": ". . Seja muito bem-vindo √† sexta edi√ß√£o da NLP Newsletter. Agradecemos por todo o suporte e dedica√ß√£o √† leitura dos temas mais recentes em ML e NLP. Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais. . Algumas atualiza√ß√£oes sobre a NLP Newsletter e o dair.ai . . N√≥s estamos traduzindo a Newsletter para outros idiomas, como o Portugu√™s Brasileiro, Chin√™s, √Årabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as tradu√ß√µes ü§ó. Voc√™ tamb√©m pode contribuir aqui! . . No m√™s passado, n√≥s realizamos o lan√ßamento oficial do nosso novo website. Voc√™ pode dar uma olhada em nossa organiza√ß√£o no GitHub para mais informa√ß√µes sobre os projetos em andamento. Se voc√™ est√° interessado em saber mais sobre as contribui√ß√µes j√° realizadas para a dar.ai, ou mesmo contribuir para a democratiza√ß√£o das tecnologias, ensino e pesquisa sobre Intelig√™ncia Artificial, veja nossa se√ß√£o de issues. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! . Publica√ß√µes üìô . A Primer in BERTology: What we know about how BERT works . . Modelos baseados no Transformer mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como sequence labeling e question answering. Um desses modelos, o BERT (Devlin et al. 2019), vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo artigo entitulado ‚ÄúA Primer in BERTology: What we know about how BERT works‚Äù busca come√ßar a responder quest√µes sobre as raz√µes que possibilitam o BERT funcionar t√£o bem em tantas tarefas de NLP. Alguns dos t√≥picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo √© representado, al√©m de m√©todos que outros pesquisadores est√£o utilizando para melhorar o processo de aprendizado. . Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer . . A Google AI publicou recentemente um m√©todo que incorpora todas as li√ß√µes aprendidas e melhorias do Transfer Learning para NLP num franework unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho prop√µe que a maioria das tarefas de NLP podem ser formuladas no formato text-to-text, onde tanto a entrada quanto a sa√≠da do problema apresentam-se na forma de texto. Os autores alegam que ‚Äúesse framework fornece uma fun√ß√£o objetivo para treinamento que √© consistente tanto na fase de pr√©-treinamento quanto no fine-tuning‚Äù. O T5 √© essencialmente um encoder-decoder baseado no Transformer, com v√°rias melhorias, em especial nos componentes de aten√ß√£o da arquitetura. O modelo foi pr√©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como Colossal Clean Crawled Corpus, onde foi estabelecido um novo estado-da-arte para tarefas como sumariza√ß√£o, question answering e classifica√ß√£o de texto. . . (Raffel et al. 2020) . 12-in-1: Multi-Task Vision and Language Representation Learning . . Os esfor√ßos de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avan√ßos na √°rea de lingu√≠stica e vis√£o computacional, mesmo quando os conhecimentos necess√°rios para abordar essas tarefas possuem interse√ß√£o. Um novo artigo (que ser√° apresentado na CVPR) prop√µe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de lingu√≠stica e vis√£o computacional, gerando uma modelo mais gen√©rico para as mesmas. O m√©todo reduz a quantidade de par√¢metros e apresenta um bom desempenho em problemas como recupera√ß√£o de imagens baseadas em legendas, e question answering visual. . . (Lu et al. 2020) . BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations . . Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT √© capaz de gerar representa√ß√µes que generalizam para outras √°reas, al√©m de texto e vis√£o computacional. Os autores apresentam um modelo denominado BERT-gen, que tira proveito de representa√ß√µes mono e multi-modais para obter desempenhos superiores em bases de dados de gera√ß√µes de perguntas baseadas em imagens. . . (Scialom et al. 2020) . Criatividade e Sociedade üé® . The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence . . Gary Marcus publicou recentemente um trabalho onde ele explica a s√©rie de passos que, na opini√£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo √© priorizar a constru√ß√£o de sistemas h√≠bridos e orientados √† conhecimento, guiados por modelos cognitivos, ao inv√©s da proposi√ß√£o de modelos com mais par√¢metros que exigem mais dados e poder computacional. . 10 Breakthrough Technologies 2020 . . A revista MIT Technology Review publicou a lista dos 10 avan√ßos tecnol√≥gicos que segundo eles far√£o a diferen√ßa na resolu√ß√£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista ‚Äî sem ordem espec√≠fica ‚Äî inclui a internet n√£o-hack√°vel, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, mol√©culas descobertas por sistemas de IA, mega-constela√ß√µes de sat√©lites artificias, supremacia qu√¢ntica, IA em aparelhos celulares, privacidade diferencial e climate attribution. . Time to rethink the publication process in machine learning . . Yoshua Bengio escreveu recentemente sobre suas preocupa√ß√µes em rela√ß√£o aos atuais ciclos acelerados de publica√ß√µes em Aprendizado de M√°quina. O ponto principal √© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e s√£o apenas incrementais, deixando o investimento de tempo na revis√£o e verifica√ß√£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes s√£o aqueles que precisam lidar com as consequ√™ncias negativas da press√£o e estresse gerados por essa situa√ß√£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas a√ß√µes para ajudar no processo de desacelera√ß√£o das publica√ß√µes para o bem da ci√™ncia. . Ferramentas e Bases de Dados ‚öôÔ∏è . Implementa√ß√£o da PointerGenerator network com a AllenNLP . . Redes Pointer-Generator buscam aprimorar o mecanismo de aten√ß√£o de modelos sequence-to-sequence e s√£o utilizadas para melhorar o desempenho em tarefas como sumariza√ß√£o abstrata. Se voc√™ gostaria de utilizando essa t√©cnica com a framework AllenNLP, saiba que o Kundan Krishna desenvolveu um m√≥dulo que permite a execu√ß√£o de um modelo pr√©-treinado dessa categoria, al√©m do treinamento de um novo modelo do zero. . . Question answering para diferentes idiomas . . Com a dissemina√ß√£o de modelos baseados no Transformer e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esfor√ßo significativo na constru√ß√£o e libera√ß√£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder compartilhou uma lista de datasets que podem ser utilizados no desenvolvimento de m√©todos para question answering em diversas l√≠nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), KorQuAD, SberQuAD, FQuAD, Arabic-SQuAD, SQuAD-it e Spanish SQuAD. . PyTorch Lightning . . A PyTorch Lightning √© uma ferramenta que possibilita a abstra√ß√£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), al√©m do uso de precis√£o de 16 bits. Fazer essas configura√ß√µes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em v√°rias GPUs/TPUs sem a necessidade de altera√ß√£o do c√≥digo. . Graph Neural Networks no TF2 . . O time de pesquisa da Microsoft liberou uma biblioteca com a implementa√ß√£o de diversas arquiteturas de Graph Neural Networks (GNNs). A biblioteca, baseada na vers√£o 2.0 do TensorFlow, fornece funcionalidades para manipula√ß√£o de dados que podem ser utilizadas diretamente nas itera√ß√µes de treino/avalia√ß√£o. . Pre-training SmallBERTa‚Ää‚Äî‚ÄäA tiny model to train on a tiny dataset . . Voc√™ j√° pensou em treinar o seu pr√≥prio modelo de linguagem do zero, mas nunca teve o poder computacional necess√°rio para isso? Se j√°, ent√£o o Aditya Malte pode lhe ajudar com esse excelente notebook no Colab que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida. . √âtica em IA üö® . Why faces don‚Äôt always tell the truth about feelings . H√° algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emo√ß√µes em contextos visuais ou textuais. Um novo artigo reabre o debate que t√©cnicas de IA que tentam reconhecer emo√ß√µes diretamente de imagens faciais n√£o est√£o fazendo seu trabalho direito. O argumento principal, formulado por psic√≥logos proeminentes na √°rea, √© que n√£o existe evid√™ncia da exist√™ncia de express√µes universais que possam ser utilizadas na detec√ß√£o de emo√ß√µes de maneira independente. Seria necess√°ria uma melhor compreens√£o de tra√ßos de personalidade e movimentos corporais por parte do modelo, dentre outras caracter√≠sticas, para que seja poss√≠vel detectar as emo√ß√µes humanas de maneira mais precisa. . Differential Privacy and Federated Learning Explicadas . . Uma das considera√ß√µes √©ticas que devem ser levadas em considera√ß√£o durante a constru√ß√£o de sistemas de IA √© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atrav√©s da differential privacy ou do federated learning. Se voc√™ quiser saber mais sobre esses dois t√≥picos, Jordan Harrod produziu uma excelente introdu√ß√£o nesse v√≠deo, que inclui uma sess√£o hands-on utilizando notebooks do Colab. . Artigos e Postagens ‚úçÔ∏è . A Deep Dive into the Reformer . . Madison May realizou uma postagem em seu blog que fornece uma an√°lise mais profunda do Reformer, um novo modelo baseado no Transformer, proposto recentemente pela Google AI. O Reformer j√° havia aparecido numa edi√ß√£o anterior da Newsletter. . Uma plataforma de blogs gratuita . A fastpages permite a cria√ß√£o e configura√ß√£o autom√°tica de um blog utilizando a GitHub pages de maneira gratuita. Essa solu√ß√£o simplifica o processo de publica√ß√£o e tamb√©m oferece suporte √† utiliza√ß√£o de documentos exportados e Jupyter notebooks. . Dicas para entrevistas na Google . . Pablo Castro, do time da Google Brain, publicou uma excelente postagem destacando as principais dicas para aqueles interessados em aplicar para uma posi√ß√£o na Google. Os t√≥picos abordados incluem dicas sobre o processo de entrevistas, como prepara√ß√£o, o que esperar durante e o que acontece depois delas. . Transformers are Graph Neural Networks . . Graph Neural Networks (GNNs) e Transformers mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente artigo em seu blog, evidenciando a conex√£o entre GNNs e Transformers, e as diversas maneiras pelas quais esses m√©todos podem ser combinados e utilizados em conjunto. . . Representa√ß√£o de uma frase como um grafo completo de palavras‚Ää‚Äî‚Ääfonte . CNNs e Equivari√¢ncia . . Fabian Fuchs e Ed Wagstaff discutiram a import√¢ncia da equivari√¢ncia e como as Convolutional Neural Networks (CNNs) garantem essa propriedade. O conceito √© apresentado e discutido posteriormente no contexto de CNNs em rela√ß√£o √† transla√ß√£o. . Self-supervised learning com imagens . A t√©cnica de self-supervised learning foi amplamente discutida nas edi√ß√µes anteriores da Newsletter devido ao seu papel em modelos recentes para language modeling. Esse blog post, feito pelo Jonathan Whitaker, fornece uma explica√ß√£o intuitiva da t√©cnica de aprendizado no contexto de imagens. Se voc√™ deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tamb√©m publicou um artigo interessante descrevendo o conceito de maneira visual. . Educa√ß√£o üéì . Stanford CS330: Deep Multi-Task and Meta-Learning . A universidade de Stanford liberou recentemente suas v√≠deo-aulas, numa playlist no YouTube, para o novo curso em deep multi-task e meta-learning. Os assuntos apresentados incluem bayesian meta-learning, lifelong learning, uma vis√£o geral sobre aprendizado por refor√ßo, model-based reinforcement learning, entre outros. . PyTorch Notebooks . A dar.ai liberou recentemente um compilado de notebooks apresentando uma introdu√ß√£o √† redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos t√≥picos j√° dispon√≠veis incluem como implementar um modelo de regress√£o log√≠stica do zero, assim como a programa√ß√£o de redes neurais feed-forward e recorrentes. Notebooks no Colab est√£o dispon√≠veis no GitHub. . The fastai book (draft) . Jeremy Howard e Sylvain Gugger liberaram uma lista com alguns notebooks para um futuro curso que introduz conceitos de Deep Learning e como implementar diferentes m√©todos utilizando o PyTorch e a biblioteca da fastai. . Cursos gratuitos de Ci√™ncia de Dados . . O Kaggle disponibilizou uma s√©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontap√© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdu√ß√£o ao Aprendizado de M√°quina e ao Python, Visualiza√ß√£o de Dados, Feature Engineering, Deep Learning, entre outros. . . Um outro excelente curso online de Ci√™ncia de Dados disponibiliza notas de aulas, slides e notebooks sobre t√≥picos que v√£o desde an√°lise explorat√≥ria at√© interpreta√ß√£o de modelos para Processamento de Linguagem Natural. . . 8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch . . A nepture.ai publicou um excelente artigo que cont√©m discuss√µes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na cria√ß√£o do PyTorch e nas ferramentas constru√≠das com base na biblioteca. . Visualizando Adaptive Sparse Attention Models . . Sashs Rush compartilhou um notebook impressionante que explica e mostra os detalhes t√©cnicos sobre como produzir sa√≠das esparsas com a softmax e induzir esparsidade nos componentes de aten√ß√£o do modelo Transformer, auxiliando na atribui√ß√£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade. . . Visualizando a distribui√ß√£o de probabilidade da sa√≠da da softmax . Men√ß√µes Honrosas ‚≠êÔ∏è . Voc√™ pode conferir a edi√ß√£o da passada da üóû Newsletter aqui. . . Conor Bell escreveu esse script em Python que permite a visualiza√ß√£o e prepara√ß√£o de uma base de dados que pode ser utilizada no modelo StyleGAN. . . Manu Romero compartilhou um modelo de POS tagging para o espanhol. O modelo est√° dispon√≠vel para uso utilizando a biblioteca Transformers da Hugging Face. Ser√° interessante acompanhar a divulga√ß√£o de modelos para outros idiomas. . Esse reposit√≥rio cont√©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem rela√ß√£o com o BERT e abordam diversos problemas como compress√£o de modelos, tarefas de dom√≠nios espec√≠ficos, entre outros. . . Connor Shorten publicou um v√≠deo de 15 minutos explicando um novo framework que busca reduzir o efeito das ‚Äúshortcut‚Äù features no self-supervised representation learning. Essa √© uma tarefa importante porqu√™, caso n√£o seja realizada corretamente, o modelo pode falhar em aprender representa√ß√µes sem√¢nticas √∫teis e potencialmente se tornar ineficiente durante o transfer learning. . Sebastian Ruder publicou uma nova edi√ß√£o da newsletter NLP News, que apresenta t√≥picos e recursos como an√°lises de artigos de ML e NLP em 2019, e apresenta√ß√µes sobre os fundamentos do Deep Learning e Transfer Learning. Confira aqui. . Inscreva-se üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada! .",
            "url": "https://victorgarritano.github.io/personal_blog/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "relUrl": "/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html",
            "date": " ‚Ä¢ Mar 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://victorgarritano.github.io/personal_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://victorgarritano.github.io/personal_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it‚Äôs in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://victorgarritano.github.io/personal_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}