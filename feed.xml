<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://victorgarritano.github.io/personal_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://victorgarritano.github.io/personal_blog/" rel="alternate" type="text/html" /><updated>2020-03-28T22:12:59-05:00</updated><id>https://victorgarritano.github.io/personal_blog/feed.xml</id><title type="html">fastpages</title><subtitle>An easy to use blogging platform with support for Jupyter Notebooks.</subtitle><entry><title type="html">NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML</title><link href="https://victorgarritano.github.io/personal_blog/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html" rel="alternate" type="text/html" title="NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML" /><published>2020-03-02T00:00:00-06:00</published><updated>2020-03-02T00:00:00-06:00</updated><id>https://victorgarritano.github.io/personal_blog/2020/03/02/NLP_Newsletter%5BPT-BR%5D_BERTology_Primer_fastpages_T5</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html">&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches. --&gt;&lt;/p&gt;

&lt;p&gt;Seja muito bem-vindo √† sexta edi√ß√£o da &lt;em&gt;NLP Newsletter&lt;/em&gt;. Agradecemos por todo o suporte e dedica√ß√£o √† leitura dos temas mais recentes em ML e NLP. Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Algumas atualiza√ß√£oes sobre a NLP Newsletter e o dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations ü§ó. You can also contribute [here](https://github.com/dair-ai/dair-ai.github.io/issues/11). --&gt;&lt;/p&gt;

&lt;p&gt;N√≥s estamos traduzindo a Newsletter para outros idiomas, como o Portugu√™s Brasileiro, Chin√™s, √Årabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as tradu√ß√µes ü§ó. Voc√™ tamb√©m pode contribuir &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;aqui&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- A month ago, we officially launched our new [website](https://dair.ai/). You can look at our [GitHub organization](https://github.com/dair-ai) for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our [issues](https://github.com/dair-ai/dair-ai.github.io/issues) section. --&gt;&lt;/p&gt;

&lt;p&gt;No m√™s passado, n√≥s realizamos o lan√ßamento oficial do nosso novo &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. Voc√™ pode dar uma olhada em nossa [organiza√ß√£o no GitHub] (https://github.com/dair-ai) para mais informa√ß√µes sobre os projetos em andamento. Se voc√™ est√° interessado em saber mais sobre as contribui√ß√µes j√° realizadas para a dar.ai, ou mesmo contribuir para a democratiza√ß√£o das tecnologias, ensino e pesquisa sobre Intelig√™ncia Artificial, veja nossa se√ß√£o de &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publica√ß√µes-&quot;&gt;Publica√ß√µes üìô&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from *sequence labeling* to *question answering*. One of those models called BERT [(Devlin et al. 2019)](https://arxiv.org/abs/1810.04805) is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new [paper](https://arxiv.org/abs/2002.12327) titled ‚Äú**A Primer in BERTology: What we know about how BERT works**‚Äù aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it. --&gt;&lt;/p&gt;

&lt;p&gt;Modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como &lt;em&gt;sequence labeling&lt;/em&gt; e &lt;em&gt;question answering&lt;/em&gt;. Um desses modelos, o BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt;, vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;artigo&lt;/a&gt; entitulado ‚Äú&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;‚Äù busca come√ßar a responder quest√µes sobre as raz√µes que possibilitam o BERT funcionar t√£o bem em tantas tarefas de NLP. Alguns dos t√≥picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo √© representado, al√©m de m√©todos que outros pesquisadores est√£o utilizando para melhorar o processo de aprendizado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Google AI recently published a [method](https://arxiv.org/abs/1910.10683) that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this ‚Äú*framework provides a consistent training objective both for pre-training and fine-tuning*‚Äù. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4) and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification. --&gt;&lt;/p&gt;

&lt;p&gt;A Google AI publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;m√©todo&lt;/a&gt; que incorpora todas as li√ß√µes aprendidas e melhorias do &lt;em&gt;Transfer Learning&lt;/em&gt; para NLP num &lt;em&gt;franework&lt;/em&gt; unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho prop√µe que a maioria das tarefas de NLP podem ser formuladas no formato &lt;em&gt;text-to-text&lt;/em&gt;, onde tanto a entrada quanto a sa√≠da do problema apresentam-se na forma de texto. Os autores alegam que ‚Äúesse framework fornece uma fun√ß√£o objetivo para treinamento que √© consistente tanto na fase de pr√©-treinamento quanto no &lt;em&gt;fine-tuning&lt;/em&gt;‚Äù. O T5 √© essencialmente um &lt;em&gt;encoder-decoder&lt;/em&gt; baseado no &lt;em&gt;Transformer&lt;/em&gt;, com v√°rias melhorias, em especial nos componentes de aten√ß√£o da arquitetura. O modelo foi pr√©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt;, onde foi estabelecido um novo estado-da-arte para tarefas como sumariza√ß√£o, &lt;em&gt;question answering&lt;/em&gt; e classifica√ß√£o de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Current research uses independent tasks and datasets to perform vision-and-language research even when the ‚Äú*visually-grounded language understanding skill*s‚Äù required to perform these tasks overlap. A new [paper](https://arxiv.org/abs/1912.02315) (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering. --&gt;&lt;/p&gt;

&lt;p&gt;Os esfor√ßos de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avan√ßos na √°rea de lingu√≠stica e vis√£o computacional, mesmo quando os conhecimentos necess√°rios para abordar essas tarefas possuem interse√ß√£o. Um novo &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;artigo&lt;/a&gt; (que ser√° apresentado na CVPR) prop√µe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de lingu√≠stica e vis√£o computacional, gerando uma modelo mais gen√©rico para as mesmas. O m√©todo reduz a quantidade de par√¢metros e apresenta um bom desempenho em problemas como recupera√ß√£o de imagens baseadas em legendas, e &lt;em&gt;question answering&lt;/em&gt; visual.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets. --&gt;&lt;/p&gt;

&lt;p&gt;Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT √© capaz de gerar representa√ß√µes que generalizam para outras √°reas, al√©m de texto e vis√£o computacional. Os autores apresentam um modelo denominado &lt;em&gt;BERT-gen&lt;/em&gt;, que tira proveito de representa√ß√µes mono e multi-modais para obter desempenhos superiores em bases de dados de gera√ß√µes de perguntas baseadas em imagens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade üé®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Gary Marcus recently published a [paper](https://arxiv.org/abs/2002.06177) where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Gary‚Äôs central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power. --&gt;&lt;/p&gt;

&lt;p&gt;Gary Marcus publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;trabalho&lt;/a&gt; onde ele explica a s√©rie de passos que, na opini√£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo √© priorizar a constru√ß√£o de sistemas h√≠bridos e orientados √† conhecimento, guiados por modelos cognitivos, ao inv√©s da proposi√ß√£o de modelos com mais par√¢metros que exigem mais dados e poder computacional.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- MIT Technology Review published a list of the [10 breakthroughs](https://www.technologyreview.com/lists/technologies/2020/) they have identified that will make a difference in solving problems that could change the way we live and work. The list‚Ää‚Äî‚Ääin no particular order‚Ää‚Äî‚Ääincludes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution. --&gt;&lt;/p&gt;

&lt;p&gt;A revista &lt;em&gt;MIT Technology Review&lt;/em&gt; publicou a lista dos &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 avan√ßos&lt;/a&gt; tecnol√≥gicos que segundo eles far√£o a diferen√ßa na resolu√ß√£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista ‚Äî sem ordem espec√≠fica ‚Äî inclui a internet &lt;em&gt;n√£o-hack√°vel&lt;/em&gt;, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, mol√©culas descobertas por sistemas de IA, mega-constela√ß√µes de sat√©lites artificias, supremacia qu√¢ntica, IA em aparelhos celulares, privacidade diferencial e &lt;em&gt;climate attribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Yoshua Bengio recently [wrote](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/) on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science. --&gt;&lt;/p&gt;

&lt;p&gt;Yoshua Bengio &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;escreveu&lt;/a&gt; recentemente sobre suas preocupa√ß√µes em rela√ß√£o aos atuais ciclos acelerados de publica√ß√µes em Aprendizado de M√°quina. O ponto principal √© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e s√£o apenas incrementais, deixando o investimento de tempo na revis√£o e verifica√ß√£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes s√£o aqueles que precisam lidar com as consequ√™ncias negativas da press√£o e estresse gerados por essa situa√ß√£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas a√ß√µes para ajudar no processo de desacelera√ß√£o das publica√ß√µes para o bem da ci√™ncia.&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-Ô∏è&quot;&gt;Ferramentas e Bases de Dados ‚öôÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Implementa√ß√£o da PointerGenerator network com a AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to [improve](https://arxiv.org/abs/1704.04368) [*abstractive summarization*](https://arxiv.org/abs/1704.04368). If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a [library](https://github.com/kukrishna/pointer-generator-pytorch-allennlp) that allows you to run a pretrained model (provided) or train your own model. --&gt;&lt;/p&gt;

&lt;p&gt;Redes &lt;em&gt;Pointer-Generator&lt;/em&gt; buscam aprimorar o mecanismo de aten√ß√£o de modelos &lt;em&gt;sequence-to-sequence&lt;/em&gt; e s√£o utilizadas para &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;melhorar o desempenho&lt;/a&gt; em tarefas como &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;sumariza√ß√£o abstrata&lt;/a&gt;. Se voc√™ gostaria de utilizando essa t√©cnica com a &lt;em&gt;framework&lt;/em&gt; AllenNLP, saiba que o Kundan Krishna desenvolveu um &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;m√≥dulo&lt;/a&gt; que permite a execu√ß√£o de um modelo pr√©-treinado dessa categoria, al√©m do treinamento de um novo modelo do zero.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering para diferentes idiomas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder [shared](https://twitter.com/seb_ruder/status/1231713840502657025?s=20) a list of datasets that can be used for question answering research in different languages: [DuReader](https://www.aclweb.org/anthology/W18-2605/), [KorQuAD](https://arxiv.org/abs/1909.07005), [SberQuAD](https://arxiv.org/abs/1912.09723), [FQuAD](https://arxiv.org/abs/2002.06071), [Arabic-SQuAD](https://arxiv.org/abs/1906.05394), [SQuAD-it](https://github.com/crux82/squad-it), and [Spanish SQuAD](https://arxiv.org/abs/1912.05200v2). --&gt;&lt;/p&gt;

&lt;p&gt;Com a dissemina√ß√£o de modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esfor√ßo significativo na constru√ß√£o e libera√ß√£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;compartilhou&lt;/a&gt; uma lista de &lt;em&gt;datasets&lt;/em&gt; que podem ser utilizados no desenvolvimento de m√©todos para &lt;em&gt;question answering&lt;/em&gt; em diversas l√≠nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; e &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- PyTorch Lightning is a [tool](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code. --&gt;&lt;/p&gt;

&lt;p&gt;A PyTorch Lightning √© uma &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;ferramenta&lt;/a&gt; que possibilita a abstra√ß√£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), al√©m do uso de precis√£o de 16 bits. Fazer essas configura√ß√µes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em v√°rias GPUs/TPUs sem a necessidade de altera√ß√£o do c√≥digo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks no TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- A Microsoft Research team releases a [library](https://github.com/microsoft/tf2-gnn) that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops. --&gt;&lt;/p&gt;

&lt;p&gt;O time de pesquisa da Microsoft liberou uma &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;biblioteca&lt;/a&gt; com a implementa√ß√£o de diversas arquiteturas de &lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt;. A biblioteca, baseada na vers√£o 2.0 do TensorFlow, fornece funcionalidades para manipula√ß√£o de dados que podem ser utilizadas diretamente nas itera√ß√µes de treino/avalia√ß√£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTa‚Ää‚Äî‚ÄäA tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Have you ever wanted to train your own language model from scratch but didn‚Äôt have enough resources to do so? If so, then Aditya Malte have you covered with this great [Colab notebook](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb) that teaches you how to train an LM from scratch with a smaller dataset. --&gt;&lt;/p&gt;

&lt;p&gt;Voc√™ j√° pensou em treinar o seu pr√≥prio modelo de linguagem do zero, mas nunca teve o poder computacional necess√°rio para isso? Se j√°, ent√£o o Aditya Malte pode lhe ajudar com esse excelente &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook no Colab&lt;/a&gt; que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida.&lt;/p&gt;

&lt;h1 id=&quot;√©tica-em-ia-&quot;&gt;√âtica em IA üö®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces don‚Äôt always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;H√° algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emo√ß√µes em contextos visuais ou textuais. Um novo &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;artigo&lt;/a&gt; reabre o debate que t√©cnicas de IA que tentam reconhecer emo√ß√µes diretamente de imagens faciais n√£o est√£o fazendo seu trabalho direito. O argumento principal, formulado por psic√≥logos proeminentes na √°rea, √© que n√£o existe evid√™ncia da exist√™ncia de express√µes universais que possam ser utilizadas na detec√ß√£o de emo√ß√µes de maneira independente. Seria necess√°ria uma melhor compreens√£o de tra√ßos de personalidade e movimentos corporais por parte do modelo, dentre outras caracter√≠sticas, para que seja poss√≠vel detectar as emo√ß√µes humanas de maneira mais precisa.&lt;/p&gt;

&lt;!-- For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new [article](https://www.nature.com/articles/d41586-020-00507-5) reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explicadas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this [video](https://www.youtube.com/watch?v=MOcTGM_UteM) which also includes a hands-on practice session with the use of a Colab notebook. --&gt;&lt;/p&gt;

&lt;p&gt;Uma das considera√ß√µes √©ticas que devem ser levadas em considera√ß√£o durante a constru√ß√£o de sistemas de IA √© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atrav√©s da &lt;em&gt;differential privacy&lt;/em&gt; ou do &lt;em&gt;federated learning&lt;/em&gt;. Se voc√™ quiser saber mais sobre esses dois t√≥picos, Jordan Harrod produziu uma excelente introdu√ß√£o nesse &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;v√≠deo&lt;/a&gt;, que inclui uma sess√£o &lt;em&gt;hands-on&lt;/em&gt; utilizando &lt;em&gt;notebooks&lt;/em&gt; do Colab.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-Ô∏è&quot;&gt;Artigos e Postagens ‚úçÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Madison May wrote a [new blog post](https://www.pragmatic.ml/reformer-deep-dive/) that provides a deep dive into [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a [previous issue](https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057) of the newsletter. --&gt;&lt;/p&gt;

&lt;p&gt;Madison May realizou uma &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;postagem&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt; que fornece uma an√°lise mais profunda do &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, um novo modelo baseado no &lt;em&gt;Transformer&lt;/em&gt;, proposto recentemente pela Google AI. O &lt;em&gt;Reformer&lt;/em&gt; j√° havia aparecido numa &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;edi√ß√£o anterior&lt;/a&gt; da Newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma plataforma de blogs gratuita&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; permite a cria√ß√£o e configura√ß√£o autom√°tica de um &lt;em&gt;blog&lt;/em&gt; utilizando a &lt;em&gt;GitHub pages&lt;/em&gt; de maneira gratuita. Essa solu√ß√£o simplifica o processo de publica√ß√£o e tamb√©m oferece suporte √† utiliza√ß√£o de documentos exportados e &lt;em&gt;Jupyter notebooks&lt;/em&gt;.&lt;/p&gt;

&lt;!-- allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Dicas para entrevistas na Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Pablo Castro, from the Google Brain team, published an [excellent blog post](https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html) highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview. --&gt;&lt;/p&gt;

&lt;p&gt;Pablo Castro, do time da Google Brain, publicou uma &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excelente postagem&lt;/a&gt; destacando as principais dicas para aqueles interessados em aplicar para uma posi√ß√£o na Google. Os t√≥picos abordados incluem dicas sobre o processo de entrevistas, como prepara√ß√£o, o que esperar durante e o que acontece depois delas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great [article](https://graphdeeplearning.github.io/post/transformers-are-gnns/) explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model. --&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt; e &lt;em&gt;Transformers&lt;/em&gt; mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;artigo&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt;, evidenciando a conex√£o entre GNNs e &lt;em&gt;Transformers&lt;/em&gt;, e as diversas maneiras pelas quais esses m√©todos podem ser combinados e utilizados em conjunto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Representa√ß√£o de uma frase como um grafo completo de palavras‚Ää‚Äî&lt;/em&gt;‚Ää&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs e Equivari√¢ncia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Fabian Fuchs and Ed Wagstaff [discuss](https://fabianfuchsml.github.io/equivariance1of2/) the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation. --&gt;&lt;/p&gt;

&lt;p&gt;Fabian Fuchs e Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutiram&lt;/a&gt; a import√¢ncia da equivari√¢ncia e como as &lt;em&gt;Convolutional Neural Networks (CNNs)&lt;/em&gt; garantem essa propriedade. O conceito √© apresentado e discutido posteriormente no contexto de CNNs em rela√ß√£o √† transla√ß√£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning com imagens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;A t√©cnica de &lt;em&gt;self-supervised learning&lt;/em&gt; foi amplamente discutida nas edi√ß√µes anteriores da Newsletter devido ao seu papel em modelos recentes para &lt;em&gt;language modeling&lt;/em&gt;. Esse &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt;, feito pelo Jonathan Whitaker, fornece uma explica√ß√£o intuitiva da t√©cnica de aprendizado no contexto de imagens. Se voc√™ deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tamb√©m publicou um &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;artigo interessante&lt;/a&gt; descrevendo o conceito de maneira visual.&lt;/p&gt;

&lt;!-- Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This [blog post](https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/) by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent [blog post](https://amitness.com/2020/02/illustrated-self-supervised-learning/) describing the concept in a visual way. --&gt;

&lt;h1 id=&quot;educa√ß√£o-&quot;&gt;Educa√ß√£o üéì&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;A universidade de Stanford liberou recentemente suas v√≠deo-aulas, numa &lt;em&gt;playlist&lt;/em&gt; no YouTube, para o &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;novo curso em &lt;em&gt;deep multi-task e meta-learning&lt;/em&gt;&lt;/a&gt;. Os assuntos apresentados incluem &lt;em&gt;bayesian meta-learning&lt;/em&gt;, &lt;em&gt;lifelong learning&lt;/em&gt;, uma vis√£o geral sobre aprendizado por refor√ßo, &lt;em&gt;model-based reinforcement learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Stanford recently released video recordings, in the form of a YouTube playlist, for their new [course on deep multi-task and meta-learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5). Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;A dar.ai liberou recentemente um &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;compilado de &lt;em&gt;notebooks&lt;/em&gt;&lt;/a&gt; apresentando uma introdu√ß√£o √† redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos t√≥picos j√° dispon√≠veis incluem como implementar um modelo de regress√£o log√≠stica do zero, assim como a programa√ß√£o de redes neurais &lt;em&gt;feed-forward&lt;/em&gt; e recorrentes. Notebooks no Colab est√£o dispon√≠veis no GitHub.&lt;/p&gt;

&lt;!-- dair.ai releases a [series of notebooks](https://github.com/dair-ai/pytorch_notebooks) that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;Jeremy Howard e Sylvain Gugger liberaram uma &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;lista&lt;/a&gt; com alguns &lt;em&gt;notebooks&lt;/em&gt; para um futuro curso que introduz conceitos de &lt;em&gt;Deep Learning&lt;/em&gt; e como implementar diferentes m√©todos utilizando o PyTorch e a biblioteca da fastai.&lt;/p&gt;

&lt;!-- Jeremy Howard and Sylvain Gugger release a [comprehensive list](https://github.com/fastai/fastbook) of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cursos gratuitos de Ci√™ncia de Dados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- In case you missed it, Kaggle provides a series of [free micro-courses](https://www.kaggle.com/learn/overview) to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others. --&gt;&lt;/p&gt;

&lt;p&gt;O Kaggle disponibilizou uma s√©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontap√© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdu√ß√£o ao Aprendizado de M√°quina e ao Python, Visualiza√ß√£o de Dados, &lt;em&gt;Feature Engineering&lt;/em&gt;, &lt;em&gt;Deep Learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Here is another excellent [online data science course](https://lewtun.github.io/dslectures/) that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing. --&gt;&lt;/p&gt;

&lt;p&gt;Um outro &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;excelente curso online&lt;/a&gt; de Ci√™ncia de Dados disponibiliza notas de aulas, &lt;em&gt;slides&lt;/em&gt; e &lt;em&gt;notebooks&lt;/em&gt; sobre t√≥picos que v√£o desde an√°lise explorat√≥ria at√© interpreta√ß√£o de modelos para Processamento de Linguagem Natural.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- ***8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem*** --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- nepture.ai published an [extensive article](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;utm_medium=tweet&amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem) that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it. --&gt;&lt;/p&gt;

&lt;p&gt;A nepture.ai publicou um &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;excelente artigo&lt;/a&gt; que cont√©m discuss√µes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na cria√ß√£o do PyTorch e nas ferramentas constru√≠das com base na biblioteca.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizando Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Sasha Rush shares an impressive [Colab notebook](http://Visualizing%20Adaptive%20Sparse%20Attention%20Models) that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once. --&gt;&lt;/p&gt;

&lt;p&gt;Sashs Rush compartilhou um &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook impressionante&lt;/a&gt; que explica e mostra os detalhes t√©cnicos sobre como produzir sa√≠das esparsas com a softmax e induzir esparsidade nos componentes de aten√ß√£o do modelo &lt;em&gt;Transformer&lt;/em&gt;, auxiliando na atribui√ß√£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizando a distribui√ß√£o de probabilidade da sa√≠da da softmax&lt;/em&gt;
&lt;!-- *Visualizing probability distribution of a softmax output* --&gt;&lt;/p&gt;

&lt;h1 id=&quot;men√ß√µes-honrosas-Ô∏è&quot;&gt;Men√ß√µes Honrosas ‚≠êÔ∏è&lt;/h1&gt;

&lt;!-- You can access the previous issue of the üóû NLP Newsletter [here](https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82). --&gt;

&lt;p&gt;Voc√™ pode conferir a edi√ß√£o da passada da üóû Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Conor Bell wrote this nice [python script](https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4) that allows you to view and prepare a dataset that can be used for a StyleGAN model. --&gt;&lt;/p&gt;

&lt;p&gt;Conor Bell escreveu esse &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script em Python&lt;/a&gt; que permite a visualiza√ß√£o e prepara√ß√£o de uma base de dados que pode ser utilizada no modelo StyleGAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Manu Romero [contributes](https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos) a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages. --&gt;&lt;/p&gt;

&lt;p&gt;Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;compartilhou&lt;/a&gt; um modelo de POS tagging para o espanhol. O modelo est√° dispon√≠vel para uso utilizando a biblioteca &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face. Ser√° interessante acompanhar a divulga√ß√£o de modelos para outros idiomas.&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;Esse &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;reposit√≥rio&lt;/a&gt; cont√©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem rela√ß√£o com o BERT e abordam diversos problemas como compress√£o de modelos, tarefas de dom√≠nios espec√≠ficos, entre outros.&lt;/p&gt;

&lt;!-- This [repo](https://github.com/tomohideshibata/BERT-related-papers) contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Connor Shorten published a short [15-minute video](https://www.youtube.com/watch?time_continue=79&amp;v=-Bh_7tzyoR4&amp;feature=emb_logo) explaining a new general framework that aims to reduce the effect of ‚Äúshortcut‚Äù features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting. --&gt;&lt;/p&gt;

&lt;p&gt;Connor Shorten publicou um &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;v√≠deo de 15 minutos&lt;/a&gt; explicando um novo &lt;em&gt;framework&lt;/em&gt; que busca reduzir o efeito das &lt;em&gt;‚Äúshortcut‚Äù features&lt;/em&gt; no &lt;em&gt;self-supervised representation learning&lt;/em&gt;. Essa √© uma tarefa importante porqu√™, caso n√£o seja realizada corretamente, o modelo pode falhar em aprender representa√ß√µes sem√¢nticas √∫teis e potencialmente se tornar ineficiente durante o &lt;em&gt;transfer learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;Sebastian Ruder publicou uma nova edi√ß√£o da newsletter &lt;em&gt;NLP News&lt;/em&gt;, que apresenta t√≥picos e recursos como an√°lises de artigos de ML e NLP em 2019, e apresenta√ß√µes sobre os fundamentos do &lt;em&gt;Deep Learning&lt;/em&gt; e &lt;em&gt;Transfer Learning&lt;/em&gt;. Confira &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out [here](http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;</content><author><name>VictorGarritano</name></author><category term="nlp_newsletter" /><summary type="html">Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_6.png%22%7D" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_6.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fastpages Notebook Blog Post</title><link href="https://victorgarritano.github.io/personal_blog/jupyter/2020/02/20/test.html" rel="alternate" type="text/html" title="Fastpages Notebook Blog Post" /><published>2020-02-20T00:00:00-06:00</published><updated>2020-02-20T00:00:00-06:00</updated><id>https://victorgarritano.github.io/personal_blog/jupyter/2020/02/20/test</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/jupyter/2020/02/20/test.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-02-20-test.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;About&quot;&gt;About&lt;a class=&quot;anchor-link&quot; href=&quot;#About&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This notebook is a demonstration of some of capabilities of &lt;a href=&quot;https://github.com/fastai/fastpages&quot;&gt;fastpages&lt;/a&gt; with notebooks.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;fastpages&lt;/code&gt; you can save your jupyter notebooks into the &lt;code&gt;_notebooks&lt;/code&gt; folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Front-Matter&quot;&gt;Front Matter&lt;a class=&quot;anchor-link&quot; href=&quot;#Front-Matter&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Title
&amp;gt; Awesome summary

- toc:true- branch: master- badges: true- comments: true
- author: Hamel Husain &amp;amp; Jeremy Howard
- categories: [fastpages, jupyter]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Setting &lt;code&gt;toc: true&lt;/code&gt; will automatically generate a table of contents&lt;/li&gt;
&lt;li&gt;Setting &lt;code&gt;badges: true&lt;/code&gt; will automatically include GitHub and Google Colab links to your notebook.&lt;/li&gt;
&lt;li&gt;Setting &lt;code&gt;comments: true&lt;/code&gt; will enable commenting on your blog post, powered by &lt;a href=&quot;https://github.com/utterance/utterances&quot;&gt;utterances&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More details and options for front matter can be viewed on the &lt;a href=&quot;https://github.com/fastai/fastpages#front-matter-related-options&quot;&gt;front matter section&lt;/a&gt; of the README.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Markdown-Shortcuts&quot;&gt;Markdown Shortcuts&lt;a class=&quot;anchor-link&quot; href=&quot;#Markdown-Shortcuts&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A &lt;code&gt;#hide&lt;/code&gt; comment at the top of any code cell will hide &lt;strong&gt;both the input and output&lt;/strong&gt; of that cell in your blog post.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;#hide_input&lt;/code&gt; comment at the top of any code cell will &lt;strong&gt;only hide the input&lt;/strong&gt; of that cell.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;The comment #hide_input was used to hide the code that produced this.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;put a &lt;code&gt;#collapse-hide&lt;/code&gt; flag at the top of any cell if you want to &lt;strong&gt;hide&lt;/strong&gt; that cell by default, but give the reader the option to show it:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#collapse-hide&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;alt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;put a &lt;code&gt;#collapse-show&lt;/code&gt; flag at the top of any cell if you want to &lt;strong&gt;show&lt;/strong&gt; that cell by default, but give the reader the option to hide it:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot; open=&quot;&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#collapse-show&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/cars.json&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/movies.json&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sp500&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/sp500.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stocks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/stocks.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/flights-5k.json&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Interactive-Charts-With-Altair&quot;&gt;Interactive Charts With Altair&lt;a class=&quot;anchor-link&quot; href=&quot;#Interactive-Charts-With-Altair&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Charts made with Altair remain interactive.  Example charts taken from &lt;a href=&quot;https://github.com/uwdata/visualization-curriculum&quot;&gt;this repo&lt;/a&gt;, specifically &lt;a href=&quot;https://github.com/uwdata/visualization-curriculum/blob/master/altair_interaction.ipynb&quot;&gt;this notebook&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Example-1:-DropDown&quot;&gt;Example 1: DropDown&lt;a class=&quot;anchor-link&quot; href=&quot;#Example-1:-DropDown&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# single-value selection over [Major_Genre, MPAA_Rating] pairs&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# use specific hard-wired values as the initial selected values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection_single&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Select&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Major_Genre&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;MPAA_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Major_Genre&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Drama&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;MPAA_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;R&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Major_Genre&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binding_select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;genres&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;MPAA_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binding_radio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mpaa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;c1&quot;&gt;# scatter plot, modify opacity based on selection&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Rotten_Tomatoes_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;IMDB_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tooltip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Title:N&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;

&lt;div id=&quot;altair-viz-1a49e83878ce4d678d7b162f3d6b510f&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-1a49e83878ce4d678d7b162f3d6b510f&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;data&quot;: {&quot;url&quot;: &quot;https://vega.github.io/vega-datasets/data/movies.json&quot;}, &quot;mark&quot;: &quot;circle&quot;, &quot;encoding&quot;: {&quot;opacity&quot;: {&quot;condition&quot;: {&quot;value&quot;: 0.75, &quot;selection&quot;: &quot;Select&quot;}, &quot;value&quot;: 0.05}, &quot;tooltip&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Title&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Rotten_Tomatoes_Rating&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;IMDB_Rating&quot;}}, &quot;selection&quot;: {&quot;Select&quot;: {&quot;type&quot;: &quot;single&quot;, &quot;fields&quot;: [&quot;Major_Genre&quot;, &quot;MPAA_Rating&quot;], &quot;init&quot;: {&quot;Major_Genre&quot;: &quot;Drama&quot;, &quot;MPAA_Rating&quot;: &quot;R&quot;}, &quot;bind&quot;: {&quot;Major_Genre&quot;: {&quot;input&quot;: &quot;select&quot;, &quot;options&quot;: [&quot;Action&quot;, &quot;Adventure&quot;, &quot;Black Comedy&quot;, &quot;Comedy&quot;, &quot;Concert/Performance&quot;, &quot;Documentary&quot;, &quot;Drama&quot;, &quot;Horror&quot;, &quot;Musical&quot;, &quot;Romantic Comedy&quot;, &quot;Thriller/Suspense&quot;, &quot;Western&quot;]}, &quot;MPAA_Rating&quot;: {&quot;input&quot;: &quot;radio&quot;, &quot;options&quot;: [&quot;G&quot;, &quot;PG&quot;, &quot;PG-13&quot;, &quot;R&quot;, &quot;NC-17&quot;, &quot;Not Rated&quot;]}}}}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Example-2:-Tooltips&quot;&gt;Example 2: Tooltips&lt;a class=&quot;anchor-link&quot; href=&quot;#Example-2:-Tooltips&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection_interval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;scales&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encodings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Rotten_Tomatoes_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;IMDB_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minExtent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# use min extent to stabilize axis title placement&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tooltip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Title:N&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Release_Date:N&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;IMDB_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Rotten_Tomatoes_Rating:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;

&lt;div id=&quot;altair-viz-c022b476f4fb482ca6f609bf6ed082d2&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-c022b476f4fb482ca6f609bf6ed082d2&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;data&quot;: {&quot;url&quot;: &quot;https://vega.github.io/vega-datasets/data/movies.json&quot;}, &quot;mark&quot;: &quot;circle&quot;, &quot;encoding&quot;: {&quot;tooltip&quot;: [{&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Title&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Release_Date&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;IMDB_Rating&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Rotten_Tomatoes_Rating&quot;}], &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Rotten_Tomatoes_Rating&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;minExtent&quot;: 30}, &quot;field&quot;: &quot;IMDB_Rating&quot;}}, &quot;height&quot;: 400, &quot;selection&quot;: {&quot;selector001&quot;: {&quot;type&quot;: &quot;interval&quot;, &quot;bind&quot;: &quot;scales&quot;, &quot;encodings&quot;: [&quot;x&quot;]}}, &quot;width&quot;: 600, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Example-3:-More-Tooltips&quot;&gt;Example 3: More Tooltips&lt;a class=&quot;anchor-link&quot; href=&quot;#Example-3:-More-Tooltips&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# select a point for which to provide details-on-demand&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection_single&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encodings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# limit selection to x-axis value&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;mouseover&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# select on mouseover events&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nearest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# select data point nearest the cursor&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;none&amp;#39;&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# empty selection includes no data points&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# define our base line chart of stock prices&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;date:T&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;price:Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;log&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;symbol:N&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# base line chart&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# add a rule mark to serve as a guide line&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_rule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;#aaa&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;date:T&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# add circle marks for selected time points, hide unselected points&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add white stroked text to provide a legible background for labels&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stroke&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strokeWidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;price:Q&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add text labels for stock prices&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;price:Q&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stocks&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;700&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;

&lt;div id=&quot;altair-viz-9283d3681fd24aafa3d1e2f9ad193ecf&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-9283d3681fd24aafa3d1e2f9ad193ecf&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;symbol&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;date&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;, &quot;scale&quot;: {&quot;type&quot;: &quot;log&quot;}}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;rule&quot;, &quot;color&quot;: &quot;#aaa&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;date&quot;}}, &quot;transform&quot;: [{&quot;filter&quot;: {&quot;selection&quot;: &quot;selector002&quot;}}]}, {&quot;mark&quot;: &quot;circle&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;symbol&quot;}, &quot;opacity&quot;: {&quot;condition&quot;: {&quot;value&quot;: 1, &quot;selection&quot;: &quot;selector002&quot;}, &quot;value&quot;: 0}, &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;date&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;, &quot;scale&quot;: {&quot;type&quot;: &quot;log&quot;}}}, &quot;selection&quot;: {&quot;selector002&quot;: {&quot;type&quot;: &quot;single&quot;, &quot;encodings&quot;: [&quot;x&quot;], &quot;on&quot;: &quot;mouseover&quot;, &quot;nearest&quot;: true, &quot;empty&quot;: &quot;none&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;left&quot;, &quot;dx&quot;: 5, &quot;dy&quot;: -5, &quot;stroke&quot;: &quot;white&quot;, &quot;strokeWidth&quot;: 2}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;symbol&quot;}, &quot;text&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;date&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;, &quot;scale&quot;: {&quot;type&quot;: &quot;log&quot;}}}, &quot;transform&quot;: [{&quot;filter&quot;: {&quot;selection&quot;: &quot;selector002&quot;}}]}, {&quot;mark&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;left&quot;, &quot;dx&quot;: 5, &quot;dy&quot;: -5}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;symbol&quot;}, &quot;text&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;date&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;price&quot;, &quot;scale&quot;: {&quot;type&quot;: &quot;log&quot;}}}, &quot;transform&quot;: [{&quot;filter&quot;: {&quot;selection&quot;: &quot;selector002&quot;}}]}], &quot;data&quot;: {&quot;url&quot;: &quot;https://vega.github.io/vega-datasets/data/stocks.csv&quot;}, &quot;height&quot;: 400, &quot;width&quot;: 700, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Data-Tables&quot;&gt;Data Tables&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Tables&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;You can display tables per the usual way in your blog:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://vega.github.io/vega-datasets/data/movies.json&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# display table with pandas&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Worldwide_Gross&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s1&quot;&gt;&amp;#39;Production_Budget&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Distributor&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;MPAA_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;IMDB_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Rotten_Tomatoes_Rating&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Title&lt;/th&gt;
      &lt;th&gt;Worldwide_Gross&lt;/th&gt;
      &lt;th&gt;Production_Budget&lt;/th&gt;
      &lt;th&gt;Distributor&lt;/th&gt;
      &lt;th&gt;MPAA_Rating&lt;/th&gt;
      &lt;th&gt;IMDB_Rating&lt;/th&gt;
      &lt;th&gt;Rotten_Tomatoes_Rating&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;The Land Girls&lt;/td&gt;
      &lt;td&gt;146083.0&lt;/td&gt;
      &lt;td&gt;8000000.0&lt;/td&gt;
      &lt;td&gt;Gramercy&lt;/td&gt;
      &lt;td&gt;R&lt;/td&gt;
      &lt;td&gt;6.1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;First Love, Last Rites&lt;/td&gt;
      &lt;td&gt;10876.0&lt;/td&gt;
      &lt;td&gt;300000.0&lt;/td&gt;
      &lt;td&gt;Strand&lt;/td&gt;
      &lt;td&gt;R&lt;/td&gt;
      &lt;td&gt;6.9&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;I Married a Strange Person&lt;/td&gt;
      &lt;td&gt;203134.0&lt;/td&gt;
      &lt;td&gt;250000.0&lt;/td&gt;
      &lt;td&gt;Lionsgate&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;6.8&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Let's Talk About Sex&lt;/td&gt;
      &lt;td&gt;373615.0&lt;/td&gt;
      &lt;td&gt;300000.0&lt;/td&gt;
      &lt;td&gt;Fine Line&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;13.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Slam&lt;/td&gt;
      &lt;td&gt;1087521.0&lt;/td&gt;
      &lt;td&gt;1000000.0&lt;/td&gt;
      &lt;td&gt;Trimark&lt;/td&gt;
      &lt;td&gt;R&lt;/td&gt;
      &lt;td&gt;3.4&lt;/td&gt;
      &lt;td&gt;62.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Images&quot;&gt;Images&lt;a class=&quot;anchor-link&quot; href=&quot;#Images&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Local-Images&quot;&gt;Local Images&lt;a class=&quot;anchor-link&quot; href=&quot;#Local-Images&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;You can reference local images and they will be copied and rendered on your blog automatically.  You can include these with the following markdown syntax:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;![](my_icons/fastai_logo.png)&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/my_icons/fastai_logo.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Remote-Images&quot;&gt;Remote Images&lt;a class=&quot;anchor-link&quot; href=&quot;#Remote-Images&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Remote images can be included with the following markdown syntax:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;![](https://image.flaticon.com/icons/svg/36/36686.svg)&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://image.flaticon.com/icons/svg/36/36686.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Animated-Gifs&quot;&gt;Animated Gifs&lt;a class=&quot;anchor-link&quot; href=&quot;#Animated-Gifs&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Animated Gifs work, too!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Captions&quot;&gt;Captions&lt;a class=&quot;anchor-link&quot; href=&quot;#Captions&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;You can include captions with markdown images like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://www.fast.ai/images/fastai_paper/show_batch.png&quot; alt=&quot;&quot; title=&quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Other-Elements&quot;&gt;Other Elements&lt;a class=&quot;anchor-link&quot; href=&quot;#Other-Elements&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Tweetcards&quot;&gt;Tweetcards&lt;a class=&quot;anchor-link&quot; href=&quot;#Tweetcards&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Typing &lt;code&gt;&amp;gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20&lt;/code&gt; will render this:

&lt;center&gt;
    &lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Altair 4.0 is released! &lt;a href=&quot;https://t.co/PCyrIOTcvv&quot;&gt;https://t.co/PCyrIOTcvv&lt;/a&gt;&lt;br /&gt;Try it with:&lt;br /&gt;&lt;br /&gt;  pip install -U altair&lt;br /&gt;&lt;br /&gt;The full list of changes is at &lt;a href=&quot;https://t.co/roXmzcsT58&quot;&gt;https://t.co/roXmzcsT58&lt;/a&gt; ...read on for some highlights. &lt;a href=&quot;https://t.co/vWJ0ZveKbZ&quot;&gt;pic.twitter.com/vWJ0ZveKbZ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jake VanderPlas (@jakevdp) &lt;a href=&quot;https://twitter.com/jakevdp/status/1204765621767901185?ref_src=twsrc%5Etfw&quot;&gt;December 11, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Youtube-Videos&quot;&gt;Youtube Videos&lt;a class=&quot;anchor-link&quot; href=&quot;#Youtube-Videos&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Typing &lt;code&gt;&amp;gt; youtube: https://youtu.be/XfoYk_Z5AkI&lt;/code&gt; will render this:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/XfoYk_Z5AkI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Boxes-/-Callouts&quot;&gt;Boxes / Callouts&lt;a class=&quot;anchor-link&quot; href=&quot;#Boxes-/-Callouts&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Typing &lt;code&gt;&amp;gt; Warning: There will be no second warning!&lt;/code&gt; will render this:
&lt;div class=&quot;flash flash-error&quot;&gt;
    &lt;svg class=&quot;octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Warning: &lt;/strong&gt;There will be no second warning!
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Typing &lt;code&gt;&amp;gt; Important: Pay attention! It's important.&lt;/code&gt; will render this:
&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Pay attention! It&amp;#8217;s important.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Typing &lt;code&gt;&amp;gt; Tip: This is my tip.&lt;/code&gt; will render this:
&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M16 8.5l-6 6-3-3L8.5 10l1.5 1.5L14.5 7 16 8.5zM5.7 12.2l.8.8H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h7c.55 0 1 .45 1 1v6.5l-.8-.8c-.39-.39-1.03-.39-1.42 0L5.7 10.8a.996.996 0 000 1.41v-.01zM4 4h5V3H4v1zm0 2h5V5H4v1zm0 2h3V7H4v1zM3 9H2v1h1V9zm0-2H2v1h1V7zm0-2H2v1h1V5zm0-2H2v1h1V3z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;This is my tip.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Typing &lt;code&gt;&amp;gt; Note: Take note of this.&lt;/code&gt; will render this:
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Take note of this.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Typing &lt;code&gt;&amp;gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine.&lt;/code&gt; will render in the docs:
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;A doc link to &lt;a href=&quot;https://www.fast.ai/&quot;&gt;an example website: fast.ai&lt;/a&gt; should also work fine.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Footnotes&quot;&gt;Footnotes&lt;a class=&quot;anchor-link&quot; href=&quot;#Footnotes&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;You can have footnotes in notebooks, however the syntax is different compared to markdown documents. &lt;a href=&quot;https://github.com/fastai/fastpages/blob/master/_fastpages_docs/NOTEBOOK_FOOTNOTES.md&quot;&gt;This guide provides more detail about this syntax&lt;/a&gt;, which looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;For example, here is a footnote {% fn 1 %}.
And another {% fn 2 %}
{{ 'This is the footnote.' | fndetail: 1 }}
{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, here is a footnote &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;And another &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. This is the footnote.&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. This is the other footnote. You can even have a &lt;a href=&quot;www.github.com&quot;&gt;link&lt;/a&gt;!&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/images/chart-preview.png" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/images/chart-preview.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">An Example Markdown Post</title><link href="https://victorgarritano.github.io/personal_blog/markdown/2020/01/14/test-markdown-post.html" rel="alternate" type="text/html" title="An Example Markdown Post" /><published>2020-01-14T00:00:00-06:00</published><updated>2020-01-14T00:00:00-06:00</updated><id>https://victorgarritano.github.io/personal_blog/markdown/2020/01/14/test-markdown-post</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/markdown/2020/01/14/test-markdown-post.html">&lt;h1 id=&quot;example-markdown-post&quot;&gt;Example Markdown Post&lt;/h1&gt;

&lt;h2 id=&quot;basic-setup&quot;&gt;Basic setup&lt;/h2&gt;

&lt;p&gt;Jekyll requires blog post files to be named according to the following format:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;YEAR-MONTH-DAY-filename.md&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;highlighter-rouge&quot;&gt;YEAR&lt;/code&gt; is a four-digit number, &lt;code class=&quot;highlighter-rouge&quot;&gt;MONTH&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;DAY&lt;/code&gt; are both two-digit numbers, and &lt;code class=&quot;highlighter-rouge&quot;&gt;filename&lt;/code&gt; is whatever file name you choose, to remind yourself what this post is about. &lt;code class=&quot;highlighter-rouge&quot;&gt;.md&lt;/code&gt; is the file extension for markdown files.&lt;/p&gt;

&lt;p&gt;The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äú&lt;em&gt;level 1 heading&lt;/em&gt;‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line &lt;code class=&quot;highlighter-rouge&quot;&gt;## File names&lt;/code&gt; above.&lt;/p&gt;

&lt;h2 id=&quot;basic-formatting&quot;&gt;Basic formatting&lt;/h2&gt;

&lt;p&gt;You can use &lt;em&gt;italics&lt;/em&gt;, &lt;strong&gt;bold&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;code font text&lt;/code&gt;, and create &lt;a href=&quot;https://www.markdownguide.org/cheat-sheet/&quot;&gt;links&lt;/a&gt;. Here‚Äôs a footnote &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here‚Äôs a horizontal rule:&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lists&quot;&gt;Lists&lt;/h2&gt;

&lt;p&gt;Here‚Äôs a list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;item 1&lt;/li&gt;
  &lt;li&gt;item 2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And a numbered list:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;item 1&lt;/li&gt;
  &lt;li&gt;item 2&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;boxes-and-stuff&quot;&gt;Boxes and stuff&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a quotation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;You can include alert boxes&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;‚Ä¶and‚Ä¶&lt;/p&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;You can include info boxes&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;images&quot;&gt;Images&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/personal_blog/images/logo.png&quot; alt=&quot;&quot; title=&quot;fast.ai's logo&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;You can format text and code per usual&lt;/p&gt;

&lt;p&gt;General preformatted text:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Do a thing
do_thing()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Python code and output:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Prints '2'
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Formatting text as shell commands:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;hello world&quot;&lt;/span&gt;
./some_script.sh &lt;span class=&quot;nt&quot;&gt;--option&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;
wget https://example.com/cat_photo1.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Formatting text as YAML:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;another_key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;another&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;value&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;tables&quot;&gt;Tables&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column 1&lt;/th&gt;
      &lt;th&gt;Column 2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;A thing&lt;/td&gt;
      &lt;td&gt;Another thing&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;tweetcards&quot;&gt;Tweetcards&lt;/h2&gt;

&lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Altair 4.0 is released! &lt;a href=&quot;https://t.co/PCyrIOTcvv&quot;&gt;https://t.co/PCyrIOTcvv&lt;/a&gt;&lt;br /&gt;Try it with:&lt;br /&gt;&lt;br /&gt;  pip install -U altair&lt;br /&gt;&lt;br /&gt;The full list of changes is at &lt;a href=&quot;https://t.co/roXmzcsT58&quot;&gt;https://t.co/roXmzcsT58&lt;/a&gt; ...read on for some highlights. &lt;a href=&quot;https://t.co/vWJ0ZveKbZ&quot;&gt;pic.twitter.com/vWJ0ZveKbZ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jake VanderPlas (@jakevdp) &lt;a href=&quot;https://twitter.com/jakevdp/status/1204765621767901185?ref_src=twsrc%5Etfw&quot;&gt;December 11, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;This is the footnote.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Example Markdown Post</summary></entry></feed>