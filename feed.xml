<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://victorgarritano.github.io/personal_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://victorgarritano.github.io/personal_blog/" rel="alternate" type="text/html" /><updated>2020-04-02T08:48:06-05:00</updated><id>https://victorgarritano.github.io/personal_blog/feed.xml</id><title type="html">Garritano‚Äôs Blog</title><entry><title type="html">NLP Newsletter [PT-BR] #8: NeRF, CORD-19, Stanza, Text Generation 101, Notebooks, SECNLP, Dreamer,‚Ä¶</title><link href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/22/NLP_Newsletter-PT-BR-_8.html" rel="alternate" type="text/html" title="NLP Newsletter [PT-BR] #8: NeRF, CORD-19, Stanza, Text Generation 101, Notebooks, SECNLP, Dreamer,‚Ä¶" /><published>2020-03-22T00:00:00-05:00</published><updated>2020-03-22T00:00:00-05:00</updated><id>https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/22/NLP_Newsletter%5BPT-BR%5D_8</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/22/NLP_Newsletter-PT-BR-_8.html">&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*SptuncVzQw49OZFlDVaQdw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;atualiza√ß√µes-da-dairai&quot;&gt;Atualiza√ß√µes da dair.ai&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;N√≥s melhoramos a categoriza√ß√£o de todos os &lt;em&gt;TL;DR&lt;/em&gt;‚Äôs e resumos j√° inclu√≠dos no &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;reposit√≥rio&lt;/a&gt; do &lt;em&gt;NLP Paper Summaries&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Todos os &lt;em&gt;issues&lt;/em&gt; e tradu√ß√µes da &lt;em&gt;Newsletter&lt;/em&gt; passaram a ser mantidos &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;aqui&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Tamb√©m foram introduzidos nessas semanas os &lt;a href=&quot;https://github.com/dair-ai/notebooks&quot;&gt;Notebooks&lt;/a&gt;, focando no compartilhamento de &lt;em&gt;notebooks&lt;/em&gt; de Ci√™ncia de Dados com a comunidade. Se voc√™ tem algum que gostaria de compartilhar, entre em contato conosco!&lt;/li&gt;
  &lt;li&gt;N√≥s disponibilizamos um &lt;a href=&quot;https://colab.research.google.com/drive/1YuL0iqxaz09qR0_2Fgyi2wQHgil_Seqg&quot;&gt;tutorial&lt;/a&gt; que demonstra como realizar uma classifica√ß√£o de emo√ß√µes utilizando a &lt;em&gt;TextVectorization&lt;/em&gt; ‚Äî uma funcionalidade experimental do TensorFlow 2.1 que auxilia no tratamento de texto em redes neurais.
&lt;!-- - We have added better categorization for all TL;DR and summaries included in the NLP paper summaries [repo](https://github.com/dair-ai/nlp_paper_summaries). --&gt;
&lt;!-- - All issues and translations of the NLP Newsletter are being maintained [here](https://github.com/dair-ai/nlp_newsletter). --&gt;
&lt;!-- - This week we also introduced [Notebooks,](https://github.com/dair-ai/notebooks) a hub for easily sharing data science notebooks with the community at large. If you have any notebooks that you would love to share with the community get in touch. --&gt;
&lt;!-- - We shared a [tutorial](https://colab.research.google.com/drive/1YuL0iqxaz09qR0_2Fgyi2wQHgil_Seqg) that provides steps on how to perform multiclass emotion classification using TextVectorization‚Ää‚Äî‚Ääan experimental feature in TensorFlow 2.1.0 that helps to manage text in a neural network. --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pesquisas-e-publica√ß√µes-&quot;&gt;Pesquisas e Publica√ß√µes üìô&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Surveys on Contextual Embeddings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Esse &lt;a href=&quot;https://arxiv.org/abs/2003.07278v1&quot;&gt;artigo&lt;/a&gt; fornece um compilado de metodologias para o aprendizado de &lt;em&gt;embeddings&lt;/em&gt; contextualizados. Tamb√©m est√£o inclusos uma revis√£o dos casos de uso da t√©cnica para &lt;em&gt;transfer learning&lt;/em&gt;, m√©todos de compress√£o de modelos e an√°lises.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Outro &lt;a href=&quot;https://arankomatsuzaki.files.wordpress.com/2020/03/written_report.pdf&quot;&gt;trabalho&lt;/a&gt; traz uma cole√ß√£o de m√©todos utilizados para a melhoria de modelos de linguagem baseados no &lt;em&gt;Transformer&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
E aqui est√° outra &lt;a href=&quot;https://arxiv.org/pdf/2003.08271.pdf&quot;&gt;colet√¢nea&lt;/a&gt; de modelos de linguagem pr√©-treinados, que prop√µe uma taxonomia para modelos dessa natureza em NLP.&lt;/p&gt;

&lt;!-- This [paper](https://arxiv.org/abs/2003.07278v1) provides a light survey of approaches for learning contextual embeddings. It also includes a review of its applications for transfer learning, model compression methods, and model analyses. Another report involves a [summary](https://arankomatsuzaki.files.wordpress.com/2020/03/written_report.pdf) of methods used to improve Transformer based language models. Here is also another [comprehensive survey](https://arxiv.org/pdf/2003.08271.pdf) on pretrained language models which provides a taxonomy of NLP pretrained models. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1jLfdem3xZ0I3EVSyOy48g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2003.08271.pdf&quot;&gt;&lt;em&gt;Qiu et al., 2020&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizing Neural Networks with the Grand Tour&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;em&gt;Grand Tour&lt;/em&gt; √© um m√©todo linear (em contraste com outras t√©cnicas n√£o-lineares, como o t-SNE) que realiza a proje√ß√£o de bases de dados de dimens√£o alta para duas dimens√µes. Neste novo &lt;a href=&quot;https://distill.pub/2020/grand-tour/&quot;&gt;artigo&lt;/a&gt; do Distill, Li et al. (2020) prop√µem a utiliza√ß√£o das habilidades do &lt;em&gt;Grand Tour&lt;/em&gt; para visualizar o comportamento de uma rede neural durante o processo de treinamento. Comportamentos de interesse nas an√°lises incluem as mudan√ßas de pesos e como essas afetam o processo de treinamento, comunica√ß√£o entre camadas do modelo e o efeito de exemplos adversariais ao serem apresentados para a rede neural.&lt;/p&gt;

&lt;!-- The Grand Tour is a linear approach (differs from the non-linear methods such as t-SNE) that projects a high-dimensional dataset to two dimensions. In a new Distill [article](https://distill.pub/2020/grand-tour/), Li et al. (2020) propose to use the Grand Tour capabilities to visualize the behavior of a neural network as it trains. Behaviors of interest in the analysis include weight changes and how it affects the training process, layer-to-layer communication in the neural network, the effect of adversarial examples when they are presented to the neural network. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*XYCRZOzslb-ZRYlmtHV0Ng.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fonte:&lt;/em&gt; &lt;a href=&quot;https://distill.pub/2020/grand-tour/&quot;&gt;&lt;em&gt;Distill&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Meta-Learning Initializations for Low-Resource Drug Discovery&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Diversos trabalhos demonstram como o &lt;em&gt;meta-learning&lt;/em&gt; pode viabilizar a ado√ß√£o de t√©cnicas de &lt;em&gt;Deep Learning&lt;/em&gt; para melhorar &lt;em&gt;benchmarks&lt;/em&gt; de &lt;em&gt;few-shot learning&lt;/em&gt;. Essa ideia √© particularmente √∫til quando nos deparamos com situa√ß√µes onde a quantidade de dados dispon√≠veis √© limitada, como no caso do desenvolvimento de novos medicamentos. Um &lt;a href=&quot;https://arxiv.org/abs/2003.05996&quot;&gt;artigo recente&lt;/a&gt; aplicou uma t√©cnica de &lt;em&gt;meta-learning&lt;/em&gt; denominada &lt;em&gt;Model-Agnostic-Meta-Learning (MAML)&lt;/em&gt;, e suas variantes, para predizer propriedades qu√≠micas em cen√°rios de escassez de dados. Os resultados obtidos demonstraram que a abordagem utilizada tem um desempenho similar a outros m√©todos multi-tarefa pr√©-treinados.&lt;/p&gt;

&lt;!-- It has been widely reported that meta-learning can enable the application of deep learning to improve on few-shot learning benchmarks. This is particularly useful when you have situations where there is limited data as is typically the case in drug discovery. A recent [work](https://arxiv.org/abs/2003.05996) applied a meta-learning approach called Model-Agnostic-Meta-Learning (MAML) and other variants to predict chemical properties and activities in low-resource settings. Results show that the meta-learning approaches perform comparably to multi-task pre-training baselines. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Um trabalho bastante interessante envolvendo pesquisadores da UC Berkeley, Google Research e da UC San Diego desenvolveu um m√©todo (&lt;a href=&quot;http://www.matthewtancik.com/nerf&quot;&gt;NeRF&lt;/a&gt;) para a cria√ß√£o de novas perspectivas em cen√°rios complexos. Tomando um conjunto de imagens RGB como base de dados, o modelo utiliza coordenadas 5D (localiza√ß√£o espacial e dire√ß√£o) para o treinamento de uma rede neural profunda totalmente conectada, otimizando uma &lt;em&gt;continuous volumetric scene function&lt;/em&gt;, e retornando a densidade de volume e radi√¢ncia para aquela localiza√ß√£o. Os diversos valores de sa√≠da s√£o combinados ao longo de um &lt;em&gt;camera ray&lt;/em&gt; e renderizados como &lt;em&gt;pixels&lt;/em&gt;. Essas sa√≠das renderizadas s√£o utilizadas para otimizar representa√ß√µes de cenas atrav√©s da minimiza√ß√£o do erro de renderiza√ß√£o para todos os &lt;em&gt;camera rays&lt;/em&gt; das imagens RGB. Comparada com outras abordagens para a tarefa, a NeRF √© quantitativa e qualitativamente melhor, al√©m de conseguir resolver algumas inconsist√™ncias das outras abordagens, como a aus√™ncia de pequenos detalhes e &lt;em&gt;flickering&lt;/em&gt; indesejado.&lt;/p&gt;

&lt;!-- An exciting work involving researchers from UC Berkeley, Google Research, and UC San Diego present a method ([NeRF](http://www.matthewtancik.com/nerf)) for synthesizing novel views of complex scenes. Using a collection of RGB image inputs, the model takes 5D coordinates (spatial location and direction), train a fully-connected DNN to optimize *a continuous volumetric scene function*, and outputs the volume density and view-dependent emitted RGB radiance for that location. The output values are composed together along a camera ray and rendered as pixels. These rendered differentiable outputs are used to optimize the scene representations *by minimizing the error of renderings all camera rays* from RGB images. Compared to other top-performing approaches for view synthesis, NeRF is qualitatively and quantitatively better and addresses inconsistencies in rendering such as lack of fine details and unwanted flickering artifacts. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*E6RQL5jdtHXR98BJJREDYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing Dreamer: Scalable Reinforcement Learning Using World Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;a href=&quot;https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html&quot;&gt;Dreamer&lt;/a&gt; √© um agente de Aprendizado por Refor√ßo que busca resolver algumas limita√ß√µes (como imediatismo e inefici√™ncia computacional) observados em agentes baseados em modelos para resolver tarefas com alto n√≠vel de dificuldade. Esse agente, proposto por pesquisadores da DeepMind e da Google AI, √© treinado para modelar o mundo no qual est√° inserido e desenvolver a habilidade de aprender comportamentos focados no longo prazo utilizando o &lt;em&gt;backpropagation&lt;/em&gt;. Resultados estado-da-arte foram obtidos em 20 tarefas de controle, baseadas nas imagens de entrada fornecidas. Al√©m disso, o modelo √© eficiente e pode operar de forma paralela, tornando-o mais interessante do ponto de vista computacional. As tr√™s tarefas envolvidas no treinamento do agente, com objetivos distintos, s√£o sintetizadas na Figura abaixo.&lt;/p&gt;

&lt;!-- [Dreamer](https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html) is a reinforcement learning (RL) agent proposed to address some limitations (e.g. shortsightedness and computational inefficiency) present in model-free and model-based agents for solving difficult tasks. This RL agent, proposed by DeepMind and Google AI researchers, is trained to model the world that also provides the ability to learn long-sighted behaviors via backpropagation using the model predictions. SoTA results are achieved on 20 continuous control tasks based on the provided image inputs. In addition, the model is data-efficient and makes predictions in parallel, making it more computationally efficient. The three tasks involved in training the agent that achieve the different goals are summarized in the figure below: --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*DOlPDgvNu1kpTeogcLve-A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fonte:&lt;/em&gt; &lt;a href=&quot;https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html&quot;&gt;&lt;em&gt;Google AI Blog&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-√©tica-e-sociedade-&quot;&gt;Criatividade, √âtica e Sociedade üåé&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;COVID-19 Open Research Dataset (CORD-19)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Num esfor√ßo para encorajar a utiliza√ß√£o da IA na luta contra a COVID-19, o &lt;em&gt;Allen Institute of AI&lt;/em&gt; publicou o &lt;a href=&quot;https://pages.semanticscholar.org/coronavirus-research&quot;&gt;COVID-19 Open Research Dataset (CORD-19)&lt;/a&gt;, um recurso publicamente dispon√≠vel que busca promover colabora√ß√£o global. A base de dados cont√©m milhares de artigos que permitem a obten√ß√£o de &lt;em&gt;insights&lt;/em&gt;, atrav√©s do emprego de t√©cnicas de NLP, que podem ajudar na luta contra o &lt;a href=&quot;https://www.who.int/emergencies/diseases/novel-coronavirus-2019&quot;&gt;coronav√≠rus&lt;/a&gt;.&lt;/p&gt;

&lt;!-- In an effort to encourage the use of AI to fight COVID-19, the Allen Institute of AI published the [COVID-19 Open Research Dataset (CORD-19)](https://pages.semanticscholar.org/coronavirus-research), a free and open resource to promote global research collaboration. The dataset contains thousands of scholarly articles that can allow NLP inspired research to obtain insights that can help in the fight against [COVID-19](https://www.who.int/emergencies/diseases/novel-coronavirus-2019). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;SECNLP: A survey of embeddings in clinical natural language processing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1532046419302436&quot;&gt;SECNLP&lt;/a&gt; √© um trabalho que inclui uma revis√£o detalhada de uma ampla gama de t√©cnicas de NLP aplicadas no contexto de sa√∫de. O trabalho foca principalmente em m√©todos de &lt;em&gt;embedding&lt;/em&gt;, problemas/desafios que essas representa√ß√µes buscam resolver, e uma discuss√£o sobre poss√≠veis dire√ß√µes de pesquisas.&lt;/p&gt;

&lt;!-- [SECNLP](https://www.sciencedirect.com/science/article/pii/S1532046419302436) is a survey paper that includes a detailed overview of a wide variety of NLP methods and techniques applied in the clinical domain. The overview emphasizes mostly on embedding methods, problems/challenges addressed with embeddings, and discussion of future research directions. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AI for 3D Generative Design&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Essa &lt;a href=&quot;https://blog.insightdatascience.com/ai-for-3d-generative-design-17503d0b3943&quot;&gt;postagem&lt;/a&gt; apresenta uma abordagem utilizada para a gera√ß√£o de objetos 3D a partir de descri√ß√µes em linguagem natural. A ideia √© criar um solu√ß√£o que permita ao designer repetir o processo de cria√ß√£o de maneira mais √°gil e explorar mais possibilidades. Ap√≥s a cria√ß√£o de uma base de conhecimento do ‚Äúespa√ßo de design‚Äù composto de modelos 3D e descri√ß√µes textuais, dois &lt;em&gt;autoencoders&lt;/em&gt; (como exemplificado na Figura abaixo) s√£o utilizados para codificar o conhecimento de maneira intuitiva. Com isso, o modelo √© capaz de gerar um design 3D a partir de uma legenda, como voc√™ pode conferir nessa &lt;a href=&quot;https://insight2020a.streamlit.io/starstorms9/shape/&quot;&gt;demonstra√ß√£o&lt;/a&gt;.&lt;/p&gt;

&lt;!-- This [article](https://blog.insightdatascience.com/ai-for-3d-generative-design-17503d0b3943) covers an approach that was used to generate 3D objects from natural language descriptions. The idea is to create a solution that allows a designer to quickly reiterate in the design process and be able to explore more broadly the design space. After creating a knowledge base of the design space consisting of 3D models and text descriptions, two autoencoders (see figure below) were used to encode that knowledge in a way that can be interacted with intuitively. The model put together can then accept a text description and generate a 3D design, try it out in this [demo](https://insight2020a.streamlit.io/starstorms9/shape/). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*pbBv2Wa5QX7lUufY.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.insightdatascience.com/ai-for-3d-generative-design-17503d0b3943&quot;&gt;&lt;em&gt;Fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-Ô∏è&quot;&gt;Ferramentas e Bases de Dados ‚öôÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanza‚Ää‚Äî‚ÄäA Python NLP Library for Many Human Languages&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O grupo de NLP de Stanford disponibilizou a &lt;a href=&quot;https://stanfordnlp.github.io/stanza/&quot;&gt;Stanza&lt;/a&gt; (denominada anteriormente como StanfordNLP), uma biblioteca em Python que oferece ferramentas de an√°lise para mais de 70 idiomas. As funcionalidades incluem Tokeniza√ß√£o, &lt;em&gt;Multi-Word Token Expansion&lt;/em&gt;, Lematiza√ß√£o, POS &lt;em&gt;tagging&lt;/em&gt;, Reconhecimento de Entidades Nomeadas e muito mais. A biblioteca √© baseada no PyTorch, com suporte a utiliza√ß√£o em GPUs e modelos de redes neurais pr√©-treinadas. A &lt;a href=&quot;https://github.com/explosion/spacy-stanza&quot;&gt;Explosion&lt;/a&gt; j√° criou um &lt;em&gt;wrapper&lt;/em&gt; para a Stanza, possibilitando sua utiliza√ß√£o como um componente do Pipeline do spaCy.&lt;/p&gt;

&lt;!-- The Stanford NLP Group releases [Stanza](https://stanfordnlp.github.io/stanza/) (formerly StanfordNLP), a Python NLP library that provides out-of-the-box text analytic tools for more than 70 languages. Capabilities include tokenization, multi-word token expansion, lemmatization, POS, NER, and much more. The tool is built on top of the PyTorch library with support for using GPU and pretrained neural models. [Explosion](https://github.com/explosion/spacy-stanza) has also built a wrapper around Stanza that allows you to interact with Stanza models as a spaCy pipeline. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;GridWorld Playground&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro criou esse &lt;a href=&quot;https://gridworld-playground.glitch.me/&quot;&gt;site interessante&lt;/a&gt; que implementa um &lt;em&gt;playground&lt;/em&gt; para a cria√ß√£o de ambientes em grade, com o objetivo de observar e verificar como agentes de aprendizado por refor√ßo tentam chegar ao objetivo, utilizando a t√©cnica do &lt;em&gt;Q-Learning&lt;/em&gt;. Dentre as funcionalidades, est√£o inclusas a habilidade de mudar os par√¢metros do ambiente e de aprendizado em tempo real, mudar a posi√ß√£o dos agentes, e transferir &lt;em&gt;value functions&lt;/em&gt; entre os dois.&lt;/p&gt;

&lt;!-- Pablo Castro created an interesting [website](https://gridworld-playground.glitch.me/) that provides a playground for creating a Grid World environment to observe and test how a reinforcement learning agent tries to solve the Grid World. Some features include the ability to change the learning/environment parameters in real-time, change the position of the agent, and transfer values between two agents. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*D1e6ixTEONl21t0UNmcaog.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;X-Stance: A Multilingual Multi-Target Dataset for Stance Detection&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A tarefa de &lt;a href=&quot;http://nlpprogress.com/english/stance_detection.html&quot;&gt;&lt;em&gt;Stance detection&lt;/em&gt;&lt;/a&gt; consiste na identifica√ß√£o do posicionamento de um sujeito frente a uma declara√ß√£o de um ator, podendo ser utilizada na avalia√ß√£o de not√≠cias falsas. Jannis Vamvas e Rico Sennrich disponibilizaram recentemente uma &lt;a href=&quot;https://arxiv.org/abs/2003.08385&quot;&gt;base de dados de larga-escala&lt;/a&gt; composta por textos escritos por candidatos das elei√ß√µes na Su√≠√ßa. M√∫ltiplos idiomas est√£o dispon√≠veis na base, o que possibilita a avalia√ß√£o da tarefa de detec√ß√£o de posicionamento em contextos multil√≠ngues. Os autores tamb√©m propuseram a utiliza√ß√£o de um modelo BERT multil√≠ngue, que apresentou um desempenho satisfat√≥rio nos cen√°rios &lt;em&gt;zero-shot cross-lingual&lt;/em&gt; and &lt;em&gt;cross-target transfer&lt;/em&gt;.&lt;/p&gt;

&lt;!-- [*Stance detection*](http://nlpprogress.com/english/stance_detection.html) is the extraction of a subject's reaction made to an actor‚Äôs claim which can be used for fake news assessment. Jannis Vamvas and Rico Sennrich recently published a [large-scale stance detection dataset](https://arxiv.org/abs/2003.08385) consisting of written text by electoral candidates in Switzerland. Multiple languages are available in the texts which could potentially lead to cross-lingual evaluations on the task of stance detection. The authors also propose the use of a multilingual BERT that achieves satisfactory performance on zero-shot cross-lingual and cross-target transfer. Learning across targets, in particular, is a challenging task so the authors used a simple technique involving standardized targets to train a single model on all the issues at once. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Create interactive textual heatmaps for Jupyter notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andreas Madsen criou uma biblioteca Python chamada &lt;a href=&quot;https://github.com/AndreasMadsen/python-textualheatmap&quot;&gt;TextualHeatMap&lt;/a&gt;, que pode ser utilizada para gerar visualiza√ß√µes que auxiliam no entendimento de quais partes de uma frase est√£o sendo utilizadas pelo modelo na hora de predizer a pr√≥xima palavra, como ocorre em modelos de linguagem.&lt;/p&gt;

&lt;!-- Andreas Madsen created a Python library called [TextualHeatMap](https://github.com/AndreasMadsen/python-textualheatmap) that can be used to render visualizations that help to understand what parts of a sentence the model is using to predict the next word such as in language models. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*IY3tKkznwarnRxdo.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fonte:&lt;/em&gt; &lt;a href=&quot;https://github.com/AndreasMadsen/python-textualheatmap&quot;&gt;&lt;em&gt;textualheatmap&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-Ô∏è&quot;&gt;Artigos e Postagens ‚úçÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;How to generate text: using different decoding methods for language generation with Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Hugging Face publicou um &lt;a href=&quot;https://huggingface.co/blog/how-to-generate&quot;&gt;artigo&lt;/a&gt; revisando diferentes t√©cnicas utilizadas para a gera√ß√£o de texto, focando em abordagens baseadas no &lt;em&gt;Transformer&lt;/em&gt;. S√£o discutidos m√©todos como &lt;em&gt;beam search&lt;/em&gt; e varia√ß√µes de processos de amostragem (‚Äúsimples‚Äù, ‚ÄúTop-K‚Äù e ‚ÄúTop-p‚Äù). J√° foram publicadas diversas outras postagens sobre esse mesmo assunto, por√©m nessa os autores dedicaram um bom tempo explicando os aspectos pr√°ticos das t√©cnicas e como elas podem ser utilizadas na biblioteca &lt;em&gt;Transformers&lt;/em&gt;.&lt;/p&gt;

&lt;!-- HuggingFace published an [article](https://huggingface.co/blog/how-to-generate) explaining the different methods used for language generation in particular for Transformer based approaches. Among those techniques discussed are greedy search, beam search, sampling, top-k sampling, and top-p (nucleus) sampling. There are many articles like this out there but the authors spent more time explaining the practical side of these methods and how they can be applied via code snippets. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Training RoBERTa from Scratch‚Ää‚Äî‚ÄäThe Missing Guide&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Motivado pela falta de um guia acess√≠vel para o treinamento do zero (&lt;em&gt;from scratch&lt;/em&gt;) de modelos de linguagem baseados no BERT utilizando a biblioteca &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face, Marcin Zablocki disponibilizou esse &lt;a href=&quot;https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/&quot;&gt;tutorial detalhado&lt;/a&gt;. O guia mostra como treinar um modelo de linguagem para o Polon√™s e traz v√°rias dicas sobre como evitar erros comuns, prepara√ß√£o dos dados, configura√ß√µes de pr√©-processamento, tokeniza√ß√£o, treinamento, monitoramento do processo de treino e compartilhamento do modelo.&lt;/p&gt;

&lt;!-- Motivated by the lack of a comprehensive guide for training a BERT-like language model from scratch using the Transformer‚Äôs library, Marcin Zablocki shares this detailed [tutorial](https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/). The guide shows how to train a transformer language model for the Polish language with tips on what common mistakes to avoid, data preparation, pretraining configuration, tokenization, training, monitoring training process, and sharing the model. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*PFPgjeUmzvazglqg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;educa√ß√£o-&quot;&gt;Educa√ß√£o üéì&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Getting started with JAX (MLPs, CNNs &amp;amp; RNNs)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Robert Lange publicou recentemente um &lt;a href=&quot;https://roberttlange.github.io/posts/2020/03/blog-post-10/&quot;&gt;tutorial&lt;/a&gt; ilustrando como treinar uma &lt;em&gt;Gated Recurrent Unit (GRU)&lt;/em&gt; com a nova biblioteca da Google, a JAX. Na &lt;a href=&quot;https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html&quot;&gt;edi√ß√£o passada da &lt;em&gt;Newsletter&lt;/em&gt;&lt;/a&gt;, n√≥s abordamos alguns t√≥picos relacionados √† biblioteca.&lt;/p&gt;

&lt;!-- Robert Lange recently published a comprehensive [tutorial](https://roberttlange.github.io/posts/2020/03/blog-post-10/) on how to train a GRU-RNN with JAX. In our [previous newsletter](https://medium.com/dair-ai/nlp-newsletter-nlp-paper-summaries-learning-to-simulate-transformers-notebooks-med7-measuring-de61fadd9db0), we also shared a couple of resources related to JAX. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP for Developers: Word Embeddings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman publicou o primeiro epis√≥dio da sua nova s√©rie, a &lt;em&gt;NLP for Developers&lt;/em&gt;, que cobrir√° as melhores pr√°ticas relacionadas √† aplica√ß√£o de diversos m√©todos de NLP. O &lt;a href=&quot;http://youtube.com/watch?v=oUpuABKoElw&amp;amp;feature=emb_logo&quot;&gt;primeiro v√≠deo&lt;/a&gt; apresenta uma introdu√ß√£o √† &lt;em&gt;word embeddings&lt;/em&gt; e como s√£o utilizados, al√©m de dicas para evitar erros comuns quando trabalhamos com esse tipo de representa√ß√£o.&lt;/p&gt;

&lt;!-- Rachael Tatman published the first episode of a new series called NLP for Developers that will cover best practices on how to apply a wide range of NLP methods. The [first episode](http://youtube.com/watch?v=oUpuABKoElw&amp;feature=emb_logo) includes an introduction to word embedding and how they are used and other common issues to avoid when applying them. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Thomas Wolf: An Introduction to Transfer Learning and HuggingFace&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Thomas Wolf apresentou essa &lt;a href=&quot;https://www.youtube.com/watch?v=rEGB7-FlPRs&amp;amp;feature=youtu.be&quot;&gt;palestra&lt;/a&gt; sobre &lt;em&gt;Transfer Learning&lt;/em&gt; no &lt;em&gt;meetup&lt;/em&gt; NLP Zurich, fornecendo uma excelente introdu√ß√£o ao assunto para o contexto de NLP. A palestra apresenta uma vis√£o geral dos momentos mais importantes para a √°rea, al√©m de uma introdu√ß√£o √†s bibliotecas &lt;em&gt;Transformers&lt;/em&gt; e &lt;em&gt;Tokenizers&lt;/em&gt;, dois dos m√≥dulos mais populares da Hugging Face.&lt;/p&gt;

&lt;!-- Thomas Wolf presented his [talk](https://www.youtube.com/watch?v=rEGB7-FlPRs&amp;feature=youtu.be) on Transfer Learning for the NLP Zurich meetup providing a great introduction to transfer learning in NLP. The talk includes an overview of recent NLP breakthroughs and an introduction to Transformers and Tokenizers, two of the most popular libraries released by the HuggingFace team and contributors. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*PMZ8ptBWo4wZ432kr-FXqA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;men√ß√µes-honrosas-Ô∏è&quot;&gt;Men√ß√µes Honrosas ‚≠êÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
Voc√™ sabia que o Google Sheets fornece uma ferramenta de tradu√ß√£o gratuita? Amit Chaudhary compartilhou uma &lt;a href=&quot;https://amitness.com/2020/02/back-translation-in-google-sheets/&quot;&gt;postagem&lt;/a&gt; que mostra como utilizar essa funcionalidade para ‚Äútradu√ß√£o reversa‚Äù, fornecendo uma maneira de aumentar a sua base de dados em tarefas de NLP.&lt;/p&gt;

&lt;!-- Did you know that Google Sheets provides a free translation feature? Amit Chaudhary shares an [article](https://amitness.com/2020/02/back-translation-in-google-sheets/) that shows how to leverage the feature for back translation which is useful for augmenting your limited text corpus for NLP. --&gt;

&lt;p&gt;&lt;br /&gt;
A New York NLP estar√° organizando um &lt;a href=&quot;https://www.meetup.com/NY-NLP/events/269502774/&quot;&gt;&lt;em&gt;meetup&lt;/em&gt; online&lt;/a&gt; para uma palestra intitulada &lt;em&gt;‚ÄúUsing Wikipedia and Wikidata for NLP‚Äù&lt;/em&gt; onde ser√° discutido como se beneficiar dos dados dessas plataformas para diferentes projetos e casos de uso de NLP.&lt;/p&gt;

&lt;!-- New York NLP will be hosting an [online meetup](https://www.meetup.com/NY-NLP/events/269502774/) for a talk titled ‚ÄúUsing Wikipedia and Wikidata for NLP‚Äù where the presenter will talk about how to leverage Wikipedia for different NLP projects and use cases. --&gt;

&lt;p&gt;&lt;br /&gt;
Lavanya Shukla escreveu esse &lt;a href=&quot;https://app.wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw&quot;&gt;tutorial&lt;/a&gt; sobre como utilizar a &lt;em&gt;PyTorch Lightning&lt;/em&gt; para otimizar hiper-par√¢metros de uma rede neural enquanto utilizamos funcionalidades de estrutura e estilo de c√≥digo fornecidas pela biblioteca. O modelo resultante e seu desempenho utilizando diferentes combina√ß√µes de hiper-par√¢metros podem ser visualizados utilizando o &lt;em&gt;logger&lt;/em&gt; WandB, que pode ser passado como par√¢metro para o objeto respons√°vel pelo treinamento do modelo.&lt;/p&gt;

&lt;!-- Lavanya Shukla wrote this nice [tutorial](https://app.wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw) on how to use PyTorch Lightning to optimize hyperparameters of a neural network while at the same time taking advantage of the simple code structures/styles provided in PyTorch Lightning. The resulting model and its performance using different hyper-parameters are visualized using the results produced by the WandB logger which can be provided as a logger parameter to a trainer object. --&gt;

&lt;p&gt;&lt;br /&gt;
Um grupo de pesquisadores publicou um &lt;a href=&quot;https://arxiv.org/abs/2003.07845&quot;&gt;estudo&lt;/a&gt; investigando de maneira mais aprofundada porqu√™ a t√©cnica de &lt;em&gt;batch normalization (BN)&lt;/em&gt; tende a prejudicar a performance de modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; para diferentes tarefas de NLP. Com base nos comportamentos observados, os autores propuseram uma nova abordagem denominada &lt;em&gt;power normalization&lt;/em&gt;. A t√©cnica proposta apresenta um desempenho superior tanto ao &lt;em&gt;BN&lt;/em&gt; quanto ao &lt;em&gt;layer normalization&lt;/em&gt;, outra t√©cnica amplamente utilizada atualmente.&lt;/p&gt;

&lt;!-- A group of researchers published a [study](https://arxiv.org/abs/2003.07845) investigating more in detail why batch normalization (BN) tends to degrade performance in Transformer based methods applied to different NLP tasks. Based on those findings, the authors propose a new approach called power normalization to deal with issues found in BN. The method outperforms both BN and layer normalization (commonly used these days) on a variety of NLP tasks. --&gt;

&lt;p&gt;&lt;br /&gt;
Esse &lt;a href=&quot;https://www.datasciencecentral.com/profiles/blogs/10-timeless-reference-books&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt; apresenta uma extensa lista de livros para ajudar voc√™ a iniciar seus estudos e experi√™ncias com Aprendizado de M√°quina.&lt;/p&gt;

&lt;!-- This [blog post](https://www.datasciencecentral.com/profiles/blogs/10-timeless-reference-books) contains a long list of books to get you started with ML. --&gt;

&lt;hr /&gt;

&lt;p&gt;Se voc√™ conhece bases de dados, projetos, postagens, tutoriais ou artigos que gostaria de ver na pr√≥xima edi√ß√£o da &lt;em&gt;Newsletter&lt;/em&gt;, sinta-se a vontade para nos contactar atrav√©s do e-mail ellfae@gmail.com ou de uma &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;mensagem direta no twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;</content><author><name>VictorGarritano</name></author><summary type="html">Essa edi√ß√£o cobre t√≥picos como produ√ß√£o de novas perspectivas em cen√°rios complexos, tutoriais para gera√ß√£o de textos, e colet√¢neas de artigos sobre embeddings contextualizados e modelos de linguagem pr√©-treinados.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_8.png%22%7D" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_8.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NLP Newsletter [PT-BR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</title><link href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html" rel="alternate" type="text/html" title="NLP Newsletter [PT-BR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶" /><published>2020-03-16T00:00:00-05:00</published><updated>2020-03-16T00:00:00-05:00</updated><id>https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter%5BPT-BR%5D_NLP_7</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/16/NLP_Newsletter-PT-BR-_NLP_7.html">&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Seja muito bem-vindo a s√©tima edi√ß√£o da NLP Newsletter. Esperamos que voc√™ tenha um dia incr√≠vel e que voc√™ as pessoas que voc√™ ama estejam em seguran√ßa nessas semanas dif√≠ceis. N√≥s decidimos publicar essa edi√ß√£o na esperan√ßa de trazer mais alegria aos nossos leitores. Sendo assim, por favor leia a &lt;em&gt;Newsletter&lt;/em&gt; durante o seu tempo livre. Nesse momento, √© importante mantermos o foco no que √© a verdadeira prioridade - nossa fam√≠lia e amigos. ‚ù§Ô∏è üíõ üíö&lt;/p&gt;

&lt;!-- Welcome to the 7th issue of the NLP Newsletter. I hope you are having a wonderful day and that you and your loved ones are safe in these difficult times. We decided to publish this newsletter to bring some joy to our readers so please read when you have free time. For now, let‚Äôs keep focused on the things that are of top priority‚Äî our families and friends. ‚ù§Ô∏è üíõ üíö --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Algumas atualiza√ß√µes sobre a NLP Newsletter e a dar.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Todas as tradu√ß√µes em franc√™s e em chin√™s das edi√ß√µes anteriores est√£o agora &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;dispon√≠veis&lt;/a&gt;. Descubra como voc√™ pode contribuir com a tradu√ß√£o das edi√ß√µes anteriores (assim como as futuras!) da Newsletter nesse &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nota do tradutor&lt;/strong&gt;: As tradu√ß√µes de todas as edi√ß√µes da Newsletter, exceto a 3¬™, para portugu√™s tamb√©m est√£o dispon√≠veis!&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ***A few updates about the NLP Newsletter and dair.ai***
All French and Chinese translations for the previous issues of the NLP Newsletter are now [available](https://github.com/dair-ai/nlp_newsletter). Find out how you can contribute to the translation of previous and upcoming issues of the NLP Newsletter at this [link](https://github.com/dair-ai/dair-ai.github.io/issues/11). --&gt;

&lt;p&gt;&lt;br /&gt;
N√≥s criamos recentemente dois reposit√≥rios no Github que cont√™m &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;resumos de artigos de NLP&lt;/a&gt; e &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;notebooks utilizando PyTorch&lt;/a&gt; para que voc√™ possa come√ßar a ter experi√™ncia com redes neurais.&lt;/p&gt;

&lt;!-- We recently created two GitHub repositories that contain [NLP paper summaries](https://github.com/dair-ai/nlp_paper_summaries) and [PyTorch notebooks](https://github.com/dair-ai/pytorch_notebooks) to get you started with neural networks. --&gt;

&lt;h1 id=&quot;pesquisas-e-publica√ß√µes-&quot;&gt;Pesquisas e Publica√ß√µes üìô&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Measuring Compositional Generalization&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
No contexto de Aprendizado de M√°quina, &lt;em&gt;compositional generalization&lt;/em&gt; se refere a habilidade de representar o conhecimento aprendido com a base de dados e aplic√°-lo a novos e diferentes contextos. At√© o presente momento, n√£o estava claro como medir essa composicionalidade nas redes neurais. Recentemente, o time de IA da Google &lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;apresentou&lt;/a&gt; um dos maiores &lt;em&gt;benchmarks&lt;/em&gt; para &lt;em&gt;compositional generalization&lt;/em&gt;, utilizando tarefas como &lt;em&gt;question answering&lt;/em&gt; e &lt;em&gt;semantic parsing&lt;/em&gt;. A imagem abaixo apresenta um exemplo do modelo proposto utilizando os chamados &lt;em&gt;√°tomos&lt;/em&gt; (unidades utilizadas para se gerar os exemplos) para que sejam produzidos &lt;em&gt;compostos&lt;/em&gt; (novas combina√ß√µes dos √°tomos). A ideia deste trabalho √© construir bases de treino e teste que combinam exemplos que possuem a mesma distribui√ß√£o pelos diferentes &lt;em&gt;√°tomos&lt;/em&gt; mas com distribui√ß√µes diferentes sobre os &lt;em&gt;compostos&lt;/em&gt;. Os autores argumentam que essa √© uma maneira mais confi√°vel de se testar a &lt;em&gt;compositional generalization&lt;/em&gt;.&lt;/p&gt;

&lt;!-- In the context of machine learning, compositional generalization is the ability to learn to represent meaning and in turn sequences (novel combinations) from what‚Äôs learned in the training set. To this date, it is not clear how to properly measure compositionality in neural networks. A Google AI team [proposes](https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html) one of the largest benchmarks for compositional generalization using tasks such as question answering and semantic parsing. The picture below shows an example of the proposed model using atoms (produce, direct, etc.) to produce novel compounds, i.e., combinations of atoms. The idea of this work is to produce a train-test split that contains examples that share similar atoms (building blocks to generate examples) distribution but different compound distribution (the composition of atoms). The authors claim that is a more reliable way to test for compositional generalization. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cr√©dito: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores testaram uma s√©rie de &lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;procedimentos de refinamento&lt;/a&gt; (&lt;em&gt;fine-tuning&lt;/em&gt;) com o objetivo de compreender melhor o efeito das diferentes estrat√©gias de inicializa√ß√£o de pesos e pol√≠ticas de &lt;em&gt;early stopping&lt;/em&gt; no desempenho de modelos de linguagem. Atrav√©s de exaustivos experimentos de refinamento do BERT, foi constatado que &lt;em&gt;seeds&lt;/em&gt; aleat√≥rias distintas produzem resultados bastante discrepantes. Em particular, o estudo reporta que certas inicializa√ß√µes de pesos de fato conferem ao modelo um bom desempenho em diversas tarefas. Todas as bases e testes realizados foram disponibilizadas, para uso de outros pesquisadores interessados em entender as din√¢micas que ocorrem durante o &lt;em&gt;fine-tuning&lt;/em&gt; de maneira mais aprofundada.&lt;/p&gt;

&lt;!-- Researchers ran a comprehensive [set of fine-tuning trials](https://arxiv.org/abs/2002.06305) to better understand the effect of weight initialization and early stopping in the performance of language models. Through various experiments that involved fine-tuning BERT hundreds of times, it was found that distinct random seeds produce very different results. In particular, the study reports that some weight initialization does perform well across a set of tasks. All the experimental data and trials were publicly released for other researchers that are interested in further understanding different dynamics during fine-tuning. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Zoom In: An Introduction to Circuits&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores da OpenAI publicaram uma &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;postagem&lt;/a&gt; discutindo o estado atual da tarefa de interpretabilidade de redes neurais, assim como uma nova abordagem para a interpreta√ß√£o das mesmas. Inspirada pela biologia celular, os autores buscaram entender modelos de vis√£o computacional e o que eles aprendem de maneira bastante aprofundada, atrav√©s da inspe√ß√£o dos pesos do modelo. Basicamente, o estudo apresentou algumas conclus√µes, obtidas a partir dos experimentos realizados, as quais eles acreditam que possam ser utilizadas como base para uma melhor interpreta√ß√£o das redes neurais.&lt;/p&gt;

&lt;!-- OpenAI researchers published a [piece](https://distill.pub/2020/circuits/zoom-in/) discussing the state of interpretability of neural networks and the proposal of a new approach to interpreting them. Inspired by cellular biology, the authors delve deep into understanding vision models and what they learn by inspecting the weights of neural networks. Essentially, the study presented a few claims along with collected evidence that they believe could pave the way to better interpret neural networks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP Research Highlights‚Ää‚Äî‚ÄäIssue #1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Numa nova iniciativa da dar.ai, a &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;, s√£o fornecidas descri√ß√µes detalhadas de t√≥picos atuais e bem importantes da pesquisa em NLP. A ideia √© que essa iniciativa seja utilizada para acompanhar os avan√ßos da √°rea atrav√©s de resumos acess√≠veis desses trabalhos. Na primeira edi√ß√£o trimestral, os t√≥picos abordados tratam sobre melhorias em modelos de linguagem e em agentes conversacionais para sistemas de reconhecimento de voz. Os resumos s√£o mantidos &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- In a new dair.ai series called [NLP Research Highlights](https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040), we provide detailed descriptions of current interesting and important NLP research. This will serve as a way to keep track of NLP progress via approachable summaries of these works. In the first quarterly issue, topics range from improving language models to improving conversational agents to state-of-the-art speech recognition systems. These summaries will also be maintained [here](https://github.com/dair-ai/nlp_paper_summaries). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nos √∫ltimos meses, as &lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt; (redes neurais que operam sobre redes) foram um assunto recorrente nas edi√ß√µes da &lt;em&gt;Newsletter&lt;/em&gt;, devido a sua efetividade em tarefas n√£o s√≥ da √°rea de NLP como tamb√©m em gen√¥mica e materiais. Um &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;artigo&lt;/a&gt; publicado recentemente, prop√µe um &lt;em&gt;framework&lt;/em&gt; geral baseado em &lt;em&gt;GNNs&lt;/em&gt; que √© capaz de realizar simula√ß√µes f√≠sicas em diferentes cen√°rios, como fluidos e materiais male√°veis. Os autores argumentam que eles obtiveram um desempenho estado-da-arte nesses diferentes contextos e que a abordagem proposta √© possivelmente o melhor simulador treinado da atualmente. Os experimentos realizados incluem a simula√ß√£o de materiais como fluidos viscosos sobre a √°gua e outras intera√ß√µes com objetos r√≠gidos. Tamb√©m foi testado um modelo pr√©-treinado em tarefas &lt;em&gt;out-of-distribution&lt;/em&gt; e os resultados obtidos foram bastante promissores, evidenciando o potencial de generaliza√ß√£o para outros cen√°rios.&lt;/p&gt;

&lt;!-- In the past few months, we have been featuring a lot about Graph Neural Networks (GNNs) due to their effectiveness not only in NLP but in other areas such as genomics and materials. In a recent [paper](https://arxiv.org/abs/2002.09405), researchers propose a general framework based on graph networks that is able to learn simulations in different domains such as fluids and deformable materials. The authors claim that they achieve state-of-the-art performance across different domains and that their general-purpose approach is potentially the best-learned physics simulator to date. Experiments include the simulation of materials such as goop over water and other interactions with rigid obstacles. They also tested a pre-trained model on out-of-distribution tasks and found promising results that show the generalization of the framework to larger domains. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Modelos BERT para idiomas espec√≠ficos&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O BERT √Årabe (AraBERT) est√° agora dispon√≠vel na biblioteca de &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face. Voc√™ pode acessar o modelo &lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;aqui&lt;/a&gt; e o artigo &lt;a href=&quot;https://arxiv.org/abs/2003.00104&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Recentemente, uma vers√£o em japon√™s do BERT tamb√©m foi &lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;disponibilizada&lt;/a&gt;. Uma vers√£o em polon√™s tamb√©m est√° dispon√≠vel, batizada como &lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Arabic BERT (AraBERT) is now available in the Hugging Face Transformer library. You can access the model [here](https://huggingface.co/aubmindlab/bert-base-arabert) and the paper [here](https://arxiv.org/abs/2003.00104). Recently, a Japanese version of BERT was also [released](https://github.com/akirakubo/bert-japanese-aozora). And there is also a Polish version of BERT called [Polbert](https://github.com/kldarek/polbert). --&gt;

&lt;h1 id=&quot;criatividade-√©tica-e-sociedade-&quot;&gt;Criatividade, √âtica e Sociedade üåé&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Computational predictions of protein structures associated with COVID-19&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A DeepMind publicou suas &lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;predi√ß√µes de estruturas&lt;/a&gt; das prote√≠nas que se ligam ao v√≠rus causador da COVID-19. As predi√ß√µes foram obtidas diretamente do sistema AlphaFold, embora n√£o tenham sido verificadas experimentalmente. A ideia √© que essa publica√ß√µes encorajem outras contribui√ß√µes que busquem entender melhor e v√≠rus e suas fun√ß√µes.&lt;/p&gt;

&lt;!-- DeepMind releases [computationally-predicted structures](https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19) for proteins linked with the virus related to COVID-19. The predictions are directly obtained from the AlphaFold systems but haven‚Äôt been experimentally verified. The idea with this release is to encourage contributions that aim to better understand the virus and how it functions. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Court cases that sound like the weirdest fights&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane compartilhou os &lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;resultados&lt;/a&gt; de um divertido experimento onde um modelo do GPT-2 foi refinado para gerar processos judiciais contra objetos inanimados. Foi disponibilizado ao modelo uma lista de processos do governo sobre apreens√µes de objetivos contrabandeados e artefatos perigosos, e foram geradas acusa√ß√µes como as apresentadas na imagem abaixo.&lt;/p&gt;

&lt;!-- Janelle Shane shares the [results](https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights) of a fun experiment where a GPT-2 model is fine-tuned to generate cases against inanimate objects. The model was fed a list of cases where the government was seizing contraband or dangerous goods and it generated cases like the ones shown in the picture below. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Toward Human-Centered Design for ML Frameworks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;publicou&lt;/a&gt; os resultados de uma grande pesquisa com 645 pessoas que utilizaram a vers√£o do TensorFlow para JavaScript. O objetivo era entender quais eram as funcionalidades mais importantes da biblioteca para desenvolvedores fora da √°rea de ML, assim como a sua experi√™ncia com as atuais bibliotecas de Aprendizado de M√°quina. Uma das conclus√µes obtidas mostra que a falta de entendimento conceitual de ML dificulta a utiliza√ß√£o de bibliotecas espec√≠ficas para esse grupo de usu√°rios. Os participantes do estudo tamb√©m reportaram a necessidade de instru√ß√µes mais acess√≠veis sobre como aplicar modelos de ML em diferentes problemas e um suporte mais expl√≠cito para modifica√ß√µes do usu√°rio.&lt;/p&gt;

&lt;!-- Google AI [published](https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html) the results of a large-scale survey of 645 people who used TensorFlow.js. They aimed to find out from non-ML software developers what are the most important features and their overall experience with using current ML frameworks. Findings include that the ‚Äúlack of conceptual understanding of ML‚Äù hinders the use of ML frameworks for this particular set of users. Participants in the study also reported the need for better instructions on how to apply the ML models to different problems and more explicit support for modification. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Face and hand tracking in the browser with MediaPipe and TensorFlow.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Este excelente &lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;artigo do TensorFlow&lt;/a&gt; apresenta um passo-a-passo para habilitar um sistema de &lt;em&gt;tracking&lt;/em&gt; do rosto e das m√£os diretamente no navegador utilizando o TensorFlow.js e o MediaPipe.&lt;/p&gt;

&lt;!-- This awesome [TensorFlow article](https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111) provides a walkthrough of how to enable real-time face and hand tracking on the browser using TensorFlow.js and MediaPipe. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cr√©ditos: Blog do TensorFlow&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-Ô∏è&quot;&gt;Ferramentas e Bases de Dados ‚öôÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NLP Paper Summaries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
N√≥s criamos recentemente um [reposit√≥rio]https://github.com/dair-ai/nlp_paper_summaries) contendo uma lista de resumos de artigos de NLP cuidadosamente formulados, para alguns dos mais interessantes e importantes &lt;em&gt;papers&lt;/em&gt; da √°rea nos √∫ltimos anos. O foco principal da iniciativa √© expandir a acessibilidade do p√∫blico-geral √† t√≥picos e pesquisas de NLP.&lt;/p&gt;

&lt;!-- We recently created a [repository](https://github.com/dair-ai/nlp_paper_summaries) containing a list of carefully curated NLP paper summaries for some of the most interesting and important NLP papers in the past few years. The focus is to feature paper summaries and blog posts of important papers to help improve the approachability and accessibility of NLP topics and research. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- ***A differentiable computer vision library for PyTorch.*** --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Uma biblioteca de vis√£o computacional diferenci√°vel em PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; √© uma biblioteca constru√≠da sobre o PyTorch que permite a utiliza√ß√£o de uma s√©rie de operadores para vis√£o computacional diferenci√°vel utilizando o PyTorch. Algumas das funcionalidades incluem transforma√ß√µes em images, &lt;em&gt;depth estimation&lt;/em&gt;, processamento de imagens em baixo-n√≠vel, dentre v√°rias outras. O m√≥dulo √© fortemente inspirado no OpenCV, com a diferen√ßa de ser focado em pesquisa, ao inv√©s de aplica√ß√µes prontas para produ√ß√£o.&lt;/p&gt;

&lt;!-- [Kornia](https://github.com/kornia/kornia) is an open-source library built on top of PyTorch that allows researchers to use a set of operators for performing differentiable computer vision using PyTorch. Some capabilities include image transformations, depth estimation, and low-level image processing, to name a few. It is heavily inspired by OpenCV but the difference is that it is meant to be used for research as opposed to building production-ready applications. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;DIET (Dual Intent and Entity Transformer)&lt;/em&gt; √© uma arquitetura multi-tarefa de &lt;em&gt;natural language understanding (NLU)&lt;/em&gt; &lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;proposta&lt;/a&gt; pela Rasa. A &lt;em&gt;framework&lt;/em&gt; foca no treinamento multi-tarefa, com o objetivo de melhorar o desempenho nos problemas de classifica√ß√£o de inten√ß√µes e reconhecimento de entidades nomeadas. Outros benef√≠cios do DIET incluem a flexibilidade de utiliza√ß√£o de qualquer &lt;em&gt;embedding&lt;/em&gt; pr√©-treinado, como o BERT e o GloVe. O foco principal, entretanto, √© disponibilizar um modelo que ultrapassa o estado-da-arte atual nessas tarefas e que seja mais r√°pido de treinar (o &lt;em&gt;speedup&lt;/em&gt; reportado foi de 6x!). O modelo est√° dispon√≠vel na biblioteca &lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;rasa&lt;/a&gt;.&lt;/p&gt;

&lt;!-- DIET (Dual Intent and Entity Transformer) is a natural language understanding (NLU) multitask architecture [proposed](https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/) by Rasa. The framework focuses on multitask training to improve results on both intent classification and entity recognition. Other benefits of DIET include the ability to use any of the current pre-trained embeddings such as BERT and GloVe. However, the focus was to provide a model that improves the current state-of-the-art performance on those tasks and is faster to train (6X speedup reported). The model is available in the [Rasa Open Source python library](https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;framework DIET&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- ***Lost in (language-specific) BERT models?*** --&gt;
&lt;strong&gt;&lt;em&gt;Perdido no meio dos modelos BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt; √© uma plataforma que possui a capacidade de buscar por mais de 30 modelos baseados no BERT, em 18 idiomas e 28 tarefas, totalizando 177 entradas em sua base de dados. Dessa forma, se voc√™ quiser descobrir o estado-da-arte para a tarefa de classifica√ß√£o de sentimentos utilizando modelos BERT, basta procurar por &lt;em&gt;‚Äúsentiment‚Äù&lt;/em&gt; na barra de busca (como exemplificado abaixo).&lt;/p&gt;

&lt;!-- [BERT Lang Street](https://bertlang.unibocconi.it/) is a neat website that provides the ability to search over 30 BERT-based models with 18 languages and 28 tasks with a total of 177 entries. For instance, if you wanted to find out the state-of-the-art results for sentiment classification using BERT models, you can just search for ‚Äúsentiment‚Äù in the search bar (example shown in the screenshot below). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Med7&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O Andrey Kormilitzin disponibilizou o &lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt;, que √© um modelo para NLP (em particular Reconhecimento de Entidades Nomeadas (NER)) em relat√≥rios m√©dicos eletr√¥nicos. O modelo √© capaz de identificar at√© 7 categorias de entidades e est√° dispon√≠vel para uso com a biblioteca spaCy.&lt;/p&gt;

&lt;!-- Andrey Kormilitzin releases [Med7](https://github.com/kormilitzin/med7) which is a model for performing clinical NLP (in particular named entity recognition (NER) tasks) on electronic health records. The model can identify up to seven categories and is available for use with the spaCy library. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- ***An Open Source Library for Quantum Machine Learning*** --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Uma biblioteca em c√≥digo-aberto para Quantum Machine Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt; √© uma biblioteca que fornece uma s√©rie de funcionalidades para a prototipagem r√°pida de modelos qu√¢nticos de ML, possibilitando a aplica√ß√£o destes em problemas em √°reas como a medicina e materiais.&lt;/p&gt;

&lt;!-- [TensorFlow Quantum](https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html) is an open-source library that provides a toolbox for rapid prototyping of quantum ML research that allows the application of ML models to approach problems ranging from medicine to materials. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fast and Easy Infinitely Wide Networks with Neural Tangents&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;em&gt;Neural Tangents&lt;/em&gt; √© uma biblioteca que permite aos pesquisadores construir e treinar modelos de dimens√£o infinita e redes neurais utilizando a JAX. Leia a postagem de lan√ßamento &lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;aqui&lt;/a&gt; e acesse a biblioteca &lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Neural Tangents is an open-source library that allows researchers to build and train infinite-width models and finite neural networks using JAX. Read the blog post of the release [here](https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html) and get access to the library [here](https://github.com/google/neural-tangents). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-Ô∏è&quot;&gt;Artigos e Postagens ‚úçÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;From PyTorch to JAX: towards neural net frameworks that purify stateful code&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke publicou um &lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;artigo&lt;/a&gt; com um passo-a-passo que ilustra a constru√ß√£o e treinamento de redes neurais utilizado o JAX. A postagem busca comparar o funcionamento interno das redes com o PyTorch e o JAX, o que auxilia num melhor entendimento dos benef√≠cios e diferen√ßas entra as duas bibliotecas.&lt;/p&gt;

&lt;!-- Sabrina J. Mielke published an [article](https://sjmielke.com/jax-purify.htm) that provides a walkthrough of how to build and train neural networks using JAX. The article focuses on comparing the inner workings of PyTorch and JAX when building neural networks, which helps to better understand some of the benefits and differences of JAX. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Why do we still use 18-year old BLEU?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nesse &lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt;, Ehud Reiter discorre sobre porqu√™ n√≥s ainda utilizamos t√©cnicas de avalia√ß√£o antigas como BLUE para mensurar o desempenho de modelos de NLP em tarefas como tradu√ß√£o autom√°tica (&lt;em&gt;machine translation&lt;/em&gt;). Como um pesquisador da √°rea, ele conta sobre as implica√ß√µes para t√©cnicas que realizam a avalia√ß√£o em tarefas de NLP mais recentes.&lt;/p&gt;

&lt;!-- In this [blog post](https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/), Ehud Reiter talks about why we still use old evaluation techniques like BLUE for evaluating NLP models for tasks like machine translation. As a researcher in the space, he also expresses the implications for techniques that perform the evaluation on more recent tasks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing BART&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; √© um novo modelo proposto pelo Facebook que consiste num &lt;em&gt;denoising autoencoder&lt;/em&gt; para o pr√©-treinamento de modelos &lt;em&gt;sequence-to-sequence&lt;/em&gt;, que pode melhorar o desempenho dos mesmos em tarefas como sumariza√ß√£o abstrata. Sam Shleifer disponibilizou um &lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;resumo interessante&lt;/a&gt; do BART e como ele realizou a integra√ß√£o do modelo na biblioteca Transformers da Hugging Face.&lt;/p&gt;

&lt;!-- [BART](https://arxiv.org/abs/1910.13461) is a new model proposed by Facebook that involves a denoising autoencoder for pretraining seq2seq models that improve performance on downstream text generation tasks such as abstractive summarization. Sam Shleifer provides a [nice summary](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html) of BART and how he integrated it into the Hugging Face Transformers repo. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A Survey of Long-Term Context in Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May escreveu recentemente um &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;compilado&lt;/a&gt; bastante interessante descrevendo estrat√©gias para melhorar abordagens baseadas em Transformers, que incluem &lt;em&gt;Sparse Transformers&lt;/em&gt;, &lt;em&gt;Adaptive Span Transformers&lt;/em&gt;, &lt;em&gt;Transformer-XL&lt;/em&gt;, &lt;em&gt;compressive Transformers&lt;/em&gt;, &lt;em&gt;Reformer&lt;/em&gt;, e &lt;em&gt;routing transformer&lt;/em&gt;. Alguns dos modelos j√° haviam aparecido em &lt;a href=&quot;https://medium.com/dair-ai&quot;&gt;publica√ß√µes&lt;/a&gt; da dar.ai e na lista de &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;resumos de artigos&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Madison May recently wrote an interesting [survey](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/) describing ways to improve Transformer based approaches, which include Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer, and routing transformer. We also touched on some of these topics in the dair.ai [publication](https://medium.com/dair-ai) and in this list of [paper summaries](https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;‚ÄúMind your language, GPT-2‚Äù: how to control style and content in automatic text writing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Apesar da flu√™ncia impressionante na escrita autom√°tica de texto evidenciada no ano passado, continua sendo um desafio controlar atributos como estrutura ou conte√∫do em textos gerados por modelos neurais. Numa &lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;postagem recente&lt;/a&gt;, Manuel Tonneau discute o progresso atual e as perspectivas na √°rea de gera√ß√£o de texto parametriz√°vel, como o modelo GPT-2 da Hugging Face refinado no arXiv e o T5 da Google, al√©m do CTRL da Salesforce e do PPLM do time de IA da Uber.&lt;/p&gt;

&lt;!-- Despite the impressive fluency automatic text writing has exhibited in the past year, it is still challenging to control attributes like structure or content of the machine-written text. In a [recent blog post](https://creatext.ai/blog-posts/controllable-text-generation), Manuel Tonneau discusses the recent progress and the perspectives in the field of controllable text generation, from Hugging Face‚Äôs GPT-2 model fine-tuned on arXiv to Google‚Äôs T5, with mentions of Salesforce‚Äôs CTRL and Uber AI‚Äôs PPLM. --&gt;

&lt;h1 id=&quot;educa√ß√£o-&quot;&gt;Educa√ß√£o üéì&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Talk: The Future of NLP in Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em uma de nossas edi√ß√µes anteriores, foi apresentado o &lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt;, uma biblioteca funcional de &lt;em&gt;Deep Learning&lt;/em&gt; focada na compatibilidade com outras j√° existentes. Essa &lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;apresenta√ß√£o&lt;/a&gt;, utilizada pela Ines Montani na PyCon Colombia, introduz a biblioteca mais profundamente.&lt;/p&gt;

&lt;!-- In one of our previous newsletters, we featured [THiNC](https://thinc.ai/) which is a functional deep learning library focused on compatibility with other existing libraries. This [set of slides](https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9) introduces a bit more of the library which was used in the talk by Ines Montani for PyCon Colombia. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Hugging Face publicou uma cole√ß√£o de &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;notebooks no Colab&lt;/a&gt; que auxilia no in√≠cio da utiliza√ß√£o de sua biblioteca Transformers. Alguns notebooks incluem o uso de tokeniza√ß√£o, configura√ß√£o de &lt;em&gt;pipelines&lt;/em&gt; de NLP, e o treinamento de modelos de linguagem em bases de dados pr√≥prias.&lt;/p&gt;

&lt;!-- HuggingFace published a set of [Colab notebooks](https://github.com/huggingface/transformers/tree/master/notebooks) that help to get started with their popular Transformers library. Some notebooks include using tokenization, setting up NLP pipelines, and training a language model on custom data. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.0 in 7 hours&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Confira esse &lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;curso gr√°tis de ~7 horas&lt;/a&gt; sobre o TensorFlow 2.0, onde s√£o cobertos t√≥picos como o b√°sico de redes neurais, NLP com redes neurais recorrentes (RNNs) e uma introdu√ß√£o ao Aprendizado por Refor√ßo.&lt;/p&gt;

&lt;!-- Check out this [~7-hour free course](https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/) on TensorFlow 2.0 containing topics that range from basic neural networks to NLP with RNNs to an introduction to reinforcement learning. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMind: The Podcast&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A DeepMind liberou todos os epis√≥dios (numa &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;playlist no YouTube&lt;/a&gt;) do seu &lt;em&gt;podcast&lt;/em&gt; com cientistas, pesquisadores e engenheiros, onde s√£o discutidos t√≥picos como *Artificial General Intelligence, neuroci√™ncia e rob√≥tica.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cursos de Machine Learning and Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Berkeley est√° disponibilizando publicamente o &lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;plano de estudos&lt;/a&gt; do seu curso em ‚Äú&lt;em&gt;Deep Unsupervised Learning&lt;/em&gt;‚Äù, focado principalmente nos aspectos te√≥ricos do &lt;em&gt;self-supervised learning&lt;/em&gt; e em modelos generativos. Outros t√≥picos incluem modelos de vari√°veis latentes, modelos autorregressivos e &lt;em&gt;flow models&lt;/em&gt;. As aulas e os &lt;em&gt;slides&lt;/em&gt; tamb√©m est√£o dispon√≠veis.&lt;/p&gt;

&lt;!-- Berkeley is publicly releasing the [entire syllabus](https://sites.google.com/view/berkeley-cs294-158-sp20/home) for its course on ‚ÄúDeep Unsupervised Learning‚Äù mainly focusing on the theoretical aspects of self-supervised learning and generative models. Some topics include latent variable models, autoregressive models, flow models, and self-supervised learning, to name a few. Youtube videos and slides are available. --&gt;

&lt;p&gt;&lt;br /&gt;
N√≥s tamb√©m encontramos essa &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;lista impressionante&lt;/a&gt; de cursos avan√ßados de ML, NLP e &lt;em&gt;Deep Learning&lt;/em&gt; dispon√≠vel de maneira online.&lt;/p&gt;

&lt;!-- We also found this [impressive list](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/) of advanced online courses on machine learning, NLP and deep learning. --&gt;

&lt;p&gt;&lt;br /&gt;
E aqui est√° um outro curso intitulado &lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;‚ÄúIntroduction to Machine Learning‚Äù&lt;/a&gt; que aborda assuntos como regress√£o supervisionada, avalia√ß√£o de desempenho, &lt;em&gt;random forests&lt;/em&gt;, ajuste de par√¢metros, dicas pr√°ticas e muito mais.&lt;/p&gt;

&lt;!-- And here is another course called [‚ÄúIntroduction to Machine Learning](https://compstat-lmu.github.io/lecture_i2ml/index.html)‚Äù which includes topics such as supervised regression, performance evaluation, random forests, parameter tuning, practical advice, and much more. --&gt;

&lt;h1 id=&quot;men√ß√µes-honrosas-Ô∏è&quot;&gt;Men√ß√µes Honrosas ‚≠êÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
A edi√ß√£o anterior da Newsletter (6¬™ edi√ß√£o) est√° dispon√≠vel &lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5/&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- The previous NLP Newsletter (Issue #6) is available [here](https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040). --&gt;

&lt;p&gt;&lt;br /&gt;
Connon Shorten publicou um &lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;v√≠deo&lt;/a&gt; explicando o modelo ELECTRA, que prop√µe a utiliza√ß√£o de uma t√©cnica chamada &lt;em&gt;replaced token detection&lt;/em&gt; como forma de pr√©-treinar Transformers de maneira mais eficiente. Se voc√™ tiver interesse em saber mais, n√≥s tamb√©m escrevemos um breve resumo do modelo &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Connon Shorten published a [video](https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;feature=emb_logo) explaining the ELECTRA model which proposes a technique called replaced token detection to pre-train Transformers more efficiently. If you are interested, we also wrote a short summary of the model [here](https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a). --&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman est√° trabalhando numa nova s√©rie denominada &lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;NLP for Developers&lt;/a&gt; onde o objetivo √© discutir diferentes m√©todos de NLP de maneira mais aprofundada, quando utiliz√°-los e como lidar com dificuldades comuns apresentadas por essas t√©cnicas.&lt;/p&gt;

&lt;!-- Rachael Tatman is working on a new series called [NLP for Developers](https://www.youtube.com/watch?v=-G36q8_cYsc&amp;feature=emb_logo) where the idea is to talk more in-depth about different NLP methods, when to use them and explaining common issues that you may run into. --&gt;

&lt;p&gt;&lt;br /&gt;
A DeepMind liberou o &lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo‚Ää‚Äî‚ÄäThe Movie&lt;/a&gt; no YouTube para celebrar o 4¬∫ anivers√°rio da vit√≥ria do modelo sobre o Lee Sedol no jogo de Go.&lt;/p&gt;

&lt;!-- DeepMind releases [AlphaGo‚Ää‚Äî‚ÄäThe Movie](https://youtu.be/WXuK6gekU1Y) on YouTube to celebrate the 4th anniversary of AlphaGo beating Lee Sedol at the game of Go. --&gt;

&lt;p&gt;&lt;br /&gt;
A OpenMined est√° com &lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;vagas abertas&lt;/a&gt; para os cargos de &lt;em&gt;Research Engineer e Research Scientist&lt;/em&gt;, que parecem ser boas oportunidades para se envolver com &lt;em&gt;privacy-preserving AI&lt;/em&gt;.&lt;/p&gt;

&lt;!-- OpenMined has [open positions](https://blog.openmined.org/introducing-openmined-research/) for Research Engineer and Research Scientist roles which is a good opportunity to get involved with privacy-preserving AI. --&gt;

&lt;hr /&gt;

&lt;p&gt;Se voc√™ conhecer bases de dados, projetos, postagens, tutoriais ou artigos que voc√™ gostaria de ver na pr√≥xima edi√ß√£o da &lt;em&gt;Newsletter&lt;/em&gt;, sinta-se a vontade para nos contactar atrav√©s do e-mail ellfae@gmail.com ou de uma &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;mensagem direta no twitter&lt;/a&gt;.&lt;/p&gt;

&lt;!-- If you have any datasets, projects, blog posts, tutorials, or papers that you wish to share in the next iteration of the NLP Newsletter, please free to reach out to me at ellfae@gmail.com or ****[**DM on Twitter**](https://twitter.com/omarsar0). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;</content><author><name>VictorGarritano</name></author><summary type="html">Nessa edi√ß√£o, s√£o abordados assuntos como melhorias na avalia√ß√£o da compositional generalization, bibliotecas de vis√£o computacional baseadas no PyTorch e um simulador f√≠sico estado-da-arte.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_7.png%22%7D" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_7.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML</title><link href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html" rel="alternate" type="text/html" title="NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML" /><published>2020-03-02T00:00:00-06:00</published><updated>2020-03-02T00:00:00-06:00</updated><id>https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/02/NLP_Newsletter%5BPT-BR%5D_BERTology_Primer_fastpages_T5</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/03/02/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5.html">&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches. --&gt;&lt;/p&gt;

&lt;p&gt;Seja muito bem-vindo √† sexta edi√ß√£o da &lt;em&gt;NLP Newsletter&lt;/em&gt;. Agradecemos por todo o suporte e dedica√ß√£o √† leitura dos temas mais recentes em ML e NLP. Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Algumas atualiza√ß√£oes sobre a NLP Newsletter e o dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations ü§ó. You can also contribute [here](https://github.com/dair-ai/dair-ai.github.io/issues/11). --&gt;&lt;/p&gt;

&lt;p&gt;N√≥s estamos traduzindo a Newsletter para outros idiomas, como o Portugu√™s Brasileiro, Chin√™s, √Årabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as tradu√ß√µes ü§ó. Voc√™ tamb√©m pode contribuir &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;aqui&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- A month ago, we officially launched our new [website](https://dair.ai/). You can look at our [GitHub organization](https://github.com/dair-ai) for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our [issues](https://github.com/dair-ai/dair-ai.github.io/issues) section. --&gt;&lt;/p&gt;

&lt;p&gt;No m√™s passado, n√≥s realizamos o lan√ßamento oficial do nosso novo &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. Voc√™ pode dar uma olhada em nossa &lt;a href=&quot;https://github.com/dair-ai&quot;&gt;organiza√ß√£o no GitHub&lt;/a&gt; para mais informa√ß√µes sobre os projetos em andamento. Se voc√™ est√° interessado em saber mais sobre as contribui√ß√µes j√° realizadas para a dar.ai, ou mesmo contribuir para a democratiza√ß√£o das tecnologias, ensino e pesquisa sobre Intelig√™ncia Artificial, veja nossa se√ß√£o de &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publica√ß√µes-&quot;&gt;Publica√ß√µes üìô&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from *sequence labeling* to *question answering*. One of those models called BERT [(Devlin et al. 2019)](https://arxiv.org/abs/1810.04805) is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new [paper](https://arxiv.org/abs/2002.12327) titled ‚Äú**A Primer in BERTology: What we know about how BERT works**‚Äù aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it. --&gt;&lt;/p&gt;

&lt;p&gt;Modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como &lt;em&gt;sequence labeling&lt;/em&gt; e &lt;em&gt;question answering&lt;/em&gt;. Um desses modelos, o BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt;, vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;artigo&lt;/a&gt; entitulado ‚Äú&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;‚Äù busca come√ßar a responder quest√µes sobre as raz√µes que possibilitam o BERT funcionar t√£o bem em tantas tarefas de NLP. Alguns dos t√≥picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo √© representado, al√©m de m√©todos que outros pesquisadores est√£o utilizando para melhorar o processo de aprendizado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Google AI recently published a [method](https://arxiv.org/abs/1910.10683) that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this ‚Äú*framework provides a consistent training objective both for pre-training and fine-tuning*‚Äù. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4) and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification. --&gt;&lt;/p&gt;

&lt;p&gt;A Google AI publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;m√©todo&lt;/a&gt; que incorpora todas as li√ß√µes aprendidas e melhorias do &lt;em&gt;Transfer Learning&lt;/em&gt; para NLP num &lt;em&gt;franework&lt;/em&gt; unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho prop√µe que a maioria das tarefas de NLP podem ser formuladas no formato &lt;em&gt;text-to-text&lt;/em&gt;, onde tanto a entrada quanto a sa√≠da do problema apresentam-se na forma de texto. Os autores alegam que ‚Äúesse framework fornece uma fun√ß√£o objetivo para treinamento que √© consistente tanto na fase de pr√©-treinamento quanto no &lt;em&gt;fine-tuning&lt;/em&gt;‚Äù. O T5 √© essencialmente um &lt;em&gt;encoder-decoder&lt;/em&gt; baseado no &lt;em&gt;Transformer&lt;/em&gt;, com v√°rias melhorias, em especial nos componentes de aten√ß√£o da arquitetura. O modelo foi pr√©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt;, onde foi estabelecido um novo estado-da-arte para tarefas como sumariza√ß√£o, &lt;em&gt;question answering&lt;/em&gt; e classifica√ß√£o de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Current research uses independent tasks and datasets to perform vision-and-language research even when the ‚Äú*visually-grounded language understanding skill*s‚Äù required to perform these tasks overlap. A new [paper](https://arxiv.org/abs/1912.02315) (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering. --&gt;&lt;/p&gt;

&lt;p&gt;Os esfor√ßos de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avan√ßos na √°rea de lingu√≠stica e vis√£o computacional, mesmo quando os conhecimentos necess√°rios para abordar essas tarefas possuem interse√ß√£o. Um novo &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;artigo&lt;/a&gt; (que ser√° apresentado na CVPR) prop√µe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de lingu√≠stica e vis√£o computacional, gerando uma modelo mais gen√©rico para as mesmas. O m√©todo reduz a quantidade de par√¢metros e apresenta um bom desempenho em problemas como recupera√ß√£o de imagens baseadas em legendas, e &lt;em&gt;question answering&lt;/em&gt; visual.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets. --&gt;&lt;/p&gt;

&lt;p&gt;Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT √© capaz de gerar representa√ß√µes que generalizam para outras √°reas, al√©m de texto e vis√£o computacional. Os autores apresentam um modelo denominado &lt;em&gt;BERT-gen&lt;/em&gt;, que tira proveito de representa√ß√µes mono e multi-modais para obter desempenhos superiores em bases de dados de gera√ß√µes de perguntas baseadas em imagens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade üé®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Gary Marcus recently published a [paper](https://arxiv.org/abs/2002.06177) where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Gary‚Äôs central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power. --&gt;&lt;/p&gt;

&lt;p&gt;Gary Marcus publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;trabalho&lt;/a&gt; onde ele explica a s√©rie de passos que, na opini√£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo √© priorizar a constru√ß√£o de sistemas h√≠bridos e orientados √† conhecimento, guiados por modelos cognitivos, ao inv√©s da proposi√ß√£o de modelos com mais par√¢metros que exigem mais dados e poder computacional.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- MIT Technology Review published a list of the [10 breakthroughs](https://www.technologyreview.com/lists/technologies/2020/) they have identified that will make a difference in solving problems that could change the way we live and work. The list‚Ää‚Äî‚Ääin no particular order‚Ää‚Äî‚Ääincludes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution. --&gt;&lt;/p&gt;

&lt;p&gt;A revista &lt;em&gt;MIT Technology Review&lt;/em&gt; publicou a lista dos &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 avan√ßos&lt;/a&gt; tecnol√≥gicos que segundo eles far√£o a diferen√ßa na resolu√ß√£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista ‚Äî sem ordem espec√≠fica ‚Äî inclui a internet &lt;em&gt;n√£o-hack√°vel&lt;/em&gt;, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, mol√©culas descobertas por sistemas de IA, mega-constela√ß√µes de sat√©lites artificias, supremacia qu√¢ntica, IA em aparelhos celulares, privacidade diferencial e &lt;em&gt;climate attribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Yoshua Bengio recently [wrote](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/) on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science. --&gt;&lt;/p&gt;

&lt;p&gt;Yoshua Bengio &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;escreveu&lt;/a&gt; recentemente sobre suas preocupa√ß√µes em rela√ß√£o aos atuais ciclos acelerados de publica√ß√µes em Aprendizado de M√°quina. O ponto principal √© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e s√£o apenas incrementais, deixando o investimento de tempo na revis√£o e verifica√ß√£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes s√£o aqueles que precisam lidar com as consequ√™ncias negativas da press√£o e estresse gerados por essa situa√ß√£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas a√ß√µes para ajudar no processo de desacelera√ß√£o das publica√ß√µes para o bem da ci√™ncia.&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-Ô∏è&quot;&gt;Ferramentas e Bases de Dados ‚öôÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Implementa√ß√£o da PointerGenerator network com a AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to [improve](https://arxiv.org/abs/1704.04368) [*abstractive summarization*](https://arxiv.org/abs/1704.04368). If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a [library](https://github.com/kukrishna/pointer-generator-pytorch-allennlp) that allows you to run a pretrained model (provided) or train your own model. --&gt;&lt;/p&gt;

&lt;p&gt;Redes &lt;em&gt;Pointer-Generator&lt;/em&gt; buscam aprimorar o mecanismo de aten√ß√£o de modelos &lt;em&gt;sequence-to-sequence&lt;/em&gt; e s√£o utilizadas para &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;melhorar o desempenho&lt;/a&gt; em tarefas como &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;sumariza√ß√£o abstrata&lt;/a&gt;. Se voc√™ gostaria de utilizando essa t√©cnica com a &lt;em&gt;framework&lt;/em&gt; AllenNLP, saiba que o Kundan Krishna desenvolveu um &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;m√≥dulo&lt;/a&gt; que permite a execu√ß√£o de um modelo pr√©-treinado dessa categoria, al√©m do treinamento de um novo modelo do zero.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering para diferentes idiomas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder [shared](https://twitter.com/seb_ruder/status/1231713840502657025?s=20) a list of datasets that can be used for question answering research in different languages: [DuReader](https://www.aclweb.org/anthology/W18-2605/), [KorQuAD](https://arxiv.org/abs/1909.07005), [SberQuAD](https://arxiv.org/abs/1912.09723), [FQuAD](https://arxiv.org/abs/2002.06071), [Arabic-SQuAD](https://arxiv.org/abs/1906.05394), [SQuAD-it](https://github.com/crux82/squad-it), and [Spanish SQuAD](https://arxiv.org/abs/1912.05200v2). --&gt;&lt;/p&gt;

&lt;p&gt;Com a dissemina√ß√£o de modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esfor√ßo significativo na constru√ß√£o e libera√ß√£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;compartilhou&lt;/a&gt; uma lista de &lt;em&gt;datasets&lt;/em&gt; que podem ser utilizados no desenvolvimento de m√©todos para &lt;em&gt;question answering&lt;/em&gt; em diversas l√≠nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; e &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- PyTorch Lightning is a [tool](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code. --&gt;&lt;/p&gt;

&lt;p&gt;A PyTorch Lightning √© uma &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;ferramenta&lt;/a&gt; que possibilita a abstra√ß√£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), al√©m do uso de precis√£o de 16 bits. Fazer essas configura√ß√µes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em v√°rias GPUs/TPUs sem a necessidade de altera√ß√£o do c√≥digo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks no TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- A Microsoft Research team releases a [library](https://github.com/microsoft/tf2-gnn) that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops. --&gt;&lt;/p&gt;

&lt;p&gt;O time de pesquisa da Microsoft liberou uma &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;biblioteca&lt;/a&gt; com a implementa√ß√£o de diversas arquiteturas de &lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt;. A biblioteca, baseada na vers√£o 2.0 do TensorFlow, fornece funcionalidades para manipula√ß√£o de dados que podem ser utilizadas diretamente nas itera√ß√µes de treino/avalia√ß√£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTa‚Ää‚Äî‚ÄäA tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Have you ever wanted to train your own language model from scratch but didn‚Äôt have enough resources to do so? If so, then Aditya Malte have you covered with this great [Colab notebook](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb) that teaches you how to train an LM from scratch with a smaller dataset. --&gt;&lt;/p&gt;

&lt;p&gt;Voc√™ j√° pensou em treinar o seu pr√≥prio modelo de linguagem do zero, mas nunca teve o poder computacional necess√°rio para isso? Se j√°, ent√£o o Aditya Malte pode lhe ajudar com esse excelente &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook no Colab&lt;/a&gt; que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida.&lt;/p&gt;

&lt;h1 id=&quot;√©tica-em-ia-&quot;&gt;√âtica em IA üö®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces don‚Äôt always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
H√° algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emo√ß√µes em contextos visuais ou textuais. Um novo &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;artigo&lt;/a&gt; reabre o debate que t√©cnicas de IA que tentam reconhecer emo√ß√µes diretamente de imagens faciais n√£o est√£o fazendo seu trabalho direito. O argumento principal, formulado por psic√≥logos proeminentes na √°rea, √© que n√£o existe evid√™ncia da exist√™ncia de express√µes universais que possam ser utilizadas na detec√ß√£o de emo√ß√µes de maneira independente. Seria necess√°ria uma melhor compreens√£o de tra√ßos de personalidade e movimentos corporais por parte do modelo, dentre outras caracter√≠sticas, para que seja poss√≠vel detectar as emo√ß√µes humanas de maneira mais precisa.&lt;/p&gt;

&lt;!-- For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new [article](https://www.nature.com/articles/d41586-020-00507-5) reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explicadas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this [video](https://www.youtube.com/watch?v=MOcTGM_UteM) which also includes a hands-on practice session with the use of a Colab notebook. --&gt;&lt;/p&gt;

&lt;p&gt;Uma das considera√ß√µes √©ticas que devem ser levadas em considera√ß√£o durante a constru√ß√£o de sistemas de IA √© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atrav√©s da &lt;em&gt;differential privacy&lt;/em&gt; ou do &lt;em&gt;federated learning&lt;/em&gt;. Se voc√™ quiser saber mais sobre esses dois t√≥picos, Jordan Harrod produziu uma excelente introdu√ß√£o nesse &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;v√≠deo&lt;/a&gt;, que inclui uma sess√£o &lt;em&gt;hands-on&lt;/em&gt; utilizando &lt;em&gt;notebooks&lt;/em&gt; do Colab.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-Ô∏è&quot;&gt;Artigos e Postagens ‚úçÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Madison May wrote a [new blog post](https://www.pragmatic.ml/reformer-deep-dive/) that provides a deep dive into [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a [previous issue](https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057) of the newsletter. --&gt;&lt;/p&gt;

&lt;p&gt;Madison May realizou uma &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;postagem&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt; que fornece uma an√°lise mais profunda do &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, um novo modelo baseado no &lt;em&gt;Transformer&lt;/em&gt;, proposto recentemente pela Google AI. O &lt;em&gt;Reformer&lt;/em&gt; j√° havia aparecido numa &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;edi√ß√£o anterior&lt;/a&gt; da Newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma plataforma de blogs gratuita&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; permite a cria√ß√£o e configura√ß√£o autom√°tica de um &lt;em&gt;blog&lt;/em&gt; utilizando a &lt;em&gt;GitHub pages&lt;/em&gt; de maneira gratuita. Essa solu√ß√£o simplifica o processo de publica√ß√£o e tamb√©m oferece suporte √† utiliza√ß√£o de documentos exportados e &lt;em&gt;Jupyter notebooks&lt;/em&gt;.&lt;/p&gt;

&lt;!-- allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Dicas para entrevistas na Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Pablo Castro, from the Google Brain team, published an [excellent blog post](https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html) highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview. --&gt;&lt;/p&gt;

&lt;p&gt;Pablo Castro, do time da Google Brain, publicou uma &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excelente postagem&lt;/a&gt; destacando as principais dicas para aqueles interessados em aplicar para uma posi√ß√£o na Google. Os t√≥picos abordados incluem dicas sobre o processo de entrevistas, como prepara√ß√£o, o que esperar durante e o que acontece depois delas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great [article](https://graphdeeplearning.github.io/post/transformers-are-gnns/) explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model. --&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt; e &lt;em&gt;Transformers&lt;/em&gt; mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;artigo&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt;, evidenciando a conex√£o entre GNNs e &lt;em&gt;Transformers&lt;/em&gt;, e as diversas maneiras pelas quais esses m√©todos podem ser combinados e utilizados em conjunto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Representa√ß√£o de uma frase como um grafo completo de palavras‚Ää‚Äî&lt;/em&gt;‚Ää&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs e Equivari√¢ncia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Fabian Fuchs and Ed Wagstaff [discuss](https://fabianfuchsml.github.io/equivariance1of2/) the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation. --&gt;&lt;/p&gt;

&lt;p&gt;Fabian Fuchs e Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutiram&lt;/a&gt; a import√¢ncia da equivari√¢ncia e como as &lt;em&gt;Convolutional Neural Networks (CNNs)&lt;/em&gt; garantem essa propriedade. O conceito √© apresentado e discutido posteriormente no contexto de CNNs em rela√ß√£o √† transla√ß√£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning com imagens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A t√©cnica de &lt;em&gt;self-supervised learning&lt;/em&gt; foi amplamente discutida nas edi√ß√µes anteriores da Newsletter devido ao seu papel em modelos recentes para &lt;em&gt;language modeling&lt;/em&gt;. Esse &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt;, feito pelo Jonathan Whitaker, fornece uma explica√ß√£o intuitiva da t√©cnica de aprendizado no contexto de imagens. Se voc√™ deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tamb√©m publicou um &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;artigo interessante&lt;/a&gt; descrevendo o conceito de maneira visual.&lt;/p&gt;

&lt;!-- Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This [blog post](https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/) by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent [blog post](https://amitness.com/2020/02/illustrated-self-supervised-learning/) describing the concept in a visual way. --&gt;

&lt;h1 id=&quot;educa√ß√£o-&quot;&gt;Educa√ß√£o üéì&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A universidade de Stanford liberou recentemente suas v√≠deo-aulas, numa &lt;em&gt;playlist&lt;/em&gt; no YouTube, para o &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;novo curso em &lt;em&gt;deep multi-task e meta-learning&lt;/em&gt;&lt;/a&gt;. Os assuntos apresentados incluem &lt;em&gt;bayesian meta-learning&lt;/em&gt;, &lt;em&gt;lifelong learning&lt;/em&gt;, uma vis√£o geral sobre aprendizado por refor√ßo, &lt;em&gt;model-based reinforcement learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Stanford recently released video recordings, in the form of a YouTube playlist, for their new [course on deep multi-task and meta-learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5). Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A dar.ai liberou recentemente um &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;compilado de &lt;em&gt;notebooks&lt;/em&gt;&lt;/a&gt; apresentando uma introdu√ß√£o √† redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos t√≥picos j√° dispon√≠veis incluem como implementar um modelo de regress√£o log√≠stica do zero, assim como a programa√ß√£o de redes neurais &lt;em&gt;feed-forward&lt;/em&gt; e recorrentes. Notebooks no Colab est√£o dispon√≠veis no GitHub.&lt;/p&gt;

&lt;!-- dair.ai releases a [series of notebooks](https://github.com/dair-ai/pytorch_notebooks) that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard e Sylvain Gugger liberaram uma &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;lista&lt;/a&gt; com alguns &lt;em&gt;notebooks&lt;/em&gt; para um futuro curso que introduz conceitos de &lt;em&gt;Deep Learning&lt;/em&gt; e como implementar diferentes m√©todos utilizando o PyTorch e a biblioteca da fastai.&lt;/p&gt;

&lt;!-- Jeremy Howard and Sylvain Gugger release a [comprehensive list](https://github.com/fastai/fastbook) of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cursos gratuitos de Ci√™ncia de Dados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- In case you missed it, Kaggle provides a series of [free micro-courses](https://www.kaggle.com/learn/overview) to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others. --&gt;&lt;/p&gt;

&lt;p&gt;O Kaggle disponibilizou uma s√©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontap√© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdu√ß√£o ao Aprendizado de M√°quina e ao Python, Visualiza√ß√£o de Dados, &lt;em&gt;Feature Engineering&lt;/em&gt;, &lt;em&gt;Deep Learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Here is another excellent [online data science course](https://lewtun.github.io/dslectures/) that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing. --&gt;&lt;/p&gt;

&lt;p&gt;Um outro &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;excelente curso online&lt;/a&gt; de Ci√™ncia de Dados disponibiliza notas de aulas, &lt;em&gt;slides&lt;/em&gt; e &lt;em&gt;notebooks&lt;/em&gt; sobre t√≥picos que v√£o desde an√°lise explorat√≥ria at√© interpreta√ß√£o de modelos para Processamento de Linguagem Natural.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- ***8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem*** --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- nepture.ai published an [extensive article](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;utm_medium=tweet&amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem) that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it. --&gt;&lt;/p&gt;

&lt;p&gt;A nepture.ai publicou um &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;excelente artigo&lt;/a&gt; que cont√©m discuss√µes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na cria√ß√£o do PyTorch e nas ferramentas constru√≠das com base na biblioteca.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizando Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Sasha Rush shares an impressive [Colab notebook](http://Visualizing%20Adaptive%20Sparse%20Attention%20Models) that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once. --&gt;&lt;/p&gt;

&lt;p&gt;Sashs Rush compartilhou um &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook impressionante&lt;/a&gt; que explica e mostra os detalhes t√©cnicos sobre como produzir sa√≠das esparsas com a softmax e induzir esparsidade nos componentes de aten√ß√£o do modelo &lt;em&gt;Transformer&lt;/em&gt;, auxiliando na atribui√ß√£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizando a distribui√ß√£o de probabilidade da sa√≠da da softmax&lt;/em&gt;
&lt;!-- *Visualizing probability distribution of a softmax output* --&gt;&lt;/p&gt;

&lt;h1 id=&quot;men√ß√µes-honrosas-Ô∏è&quot;&gt;Men√ß√µes Honrosas ‚≠êÔ∏è&lt;/h1&gt;

&lt;!-- You can access the previous issue of the üóû NLP Newsletter [here](https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82). --&gt;

&lt;p&gt;Voc√™ pode conferir a edi√ß√£o da passada da üóû Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Conor Bell wrote this nice [python script](https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4) that allows you to view and prepare a dataset that can be used for a StyleGAN model. --&gt;&lt;/p&gt;

&lt;p&gt;Conor Bell escreveu esse &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script em Python&lt;/a&gt; que permite a visualiza√ß√£o e prepara√ß√£o de uma base de dados que pode ser utilizada no modelo StyleGAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Manu Romero [contributes](https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos) a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages. --&gt;&lt;/p&gt;

&lt;p&gt;Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;compartilhou&lt;/a&gt; um modelo de POS tagging para o espanhol. O modelo est√° dispon√≠vel para uso utilizando a biblioteca &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face. Ser√° interessante acompanhar a divulga√ß√£o de modelos para outros idiomas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Esse &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;reposit√≥rio&lt;/a&gt; cont√©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem rela√ß√£o com o BERT e abordam diversos problemas como compress√£o de modelos, tarefas de dom√≠nios espec√≠ficos, entre outros.&lt;/p&gt;

&lt;!-- This [repo](https://github.com/tomohideshibata/BERT-related-papers) contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- Connor Shorten published a short [15-minute video](https://www.youtube.com/watch?time_continue=79&amp;v=-Bh_7tzyoR4&amp;feature=emb_logo) explaining a new general framework that aims to reduce the effect of ‚Äúshortcut‚Äù features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting. --&gt;&lt;/p&gt;

&lt;p&gt;Connor Shorten publicou um &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;v√≠deo de 15 minutos&lt;/a&gt; explicando um novo &lt;em&gt;framework&lt;/em&gt; que busca reduzir o efeito das &lt;em&gt;‚Äúshortcut‚Äù features&lt;/em&gt; no &lt;em&gt;self-supervised representation learning&lt;/em&gt;. Essa √© uma tarefa importante porqu√™, caso n√£o seja realizada corretamente, o modelo pode falhar em aprender representa√ß√µes sem√¢nticas √∫teis e potencialmente se tornar ineficiente durante o &lt;em&gt;transfer learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder publicou uma nova edi√ß√£o da newsletter &lt;em&gt;NLP News&lt;/em&gt;, que apresenta t√≥picos e recursos como an√°lises de artigos de ML e NLP em 2019, e apresenta√ß√µes sobre os fundamentos do &lt;em&gt;Deep Learning&lt;/em&gt; e &lt;em&gt;Transfer Learning&lt;/em&gt;. Confira &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out [here](http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;</content><author><name>VictorGarritano</name></author><summary type="html">Essa edi√ß√£o cobre t√≥picos como extens√µes ao modelo Transformer, desacelera√ß√£o no processo de publica√ß√£o em Aprendizado de M√°quina, divulga√ß√£o de livros e projetos sobre ML e NLP e muito mais</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_6.png%22%7D" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_6.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶</title><link href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" rel="alternate" type="text/html" title="NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶" /><published>2020-02-16T00:00:00-06:00</published><updated>2020-02-16T00:00:00-06:00</updated><id>https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter%5BPT-BR%5D_PyTorch3D,_DeepSpeed,_Turing-NLG</id><content type="html" xml:base="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html">&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;publica√ß√µes-&quot;&gt;Publica√ß√µes üìô&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Turing-NLG: A 17-billion-parameter language model by Microsoft&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O Turing Natural Language Generation (T-NLG) √© um modelo de linguagem com 17 bilh√µes de par√¢metros &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;proposto&lt;/a&gt; pelo grupo de pesquisa em Intelig√™ncia Artificial da Microsoft. O T-NLG √© composto por 78 camadas, baseado na arquitetura dos Transformers, que superou o estado da arte anterior (atribuido ao &lt;a href=&quot;https://github.com/NVIDIA/Megatron-LM&quot;&gt;Megatron-LM&lt;/a&gt; da Nvidia) considerando a Perplexidade na base de dados WikiText-103. O modelo foi testado em diversas tarefas, como &lt;em&gt;question answering&lt;/em&gt; e sumariza√ß√£o abstrata, onde foram observados comportamentos interessantes e desej√°veis para modelos dessa categoria, como &lt;em&gt;‚Äúzero shot‚Äù question answering&lt;/em&gt; (onde o modelo responde a um pergunta sem estar ciente do contexto explicitamente), e baixa necessidade de bases previamente anotadas, para as tarefas citadas anteriormente. O modelo pode ser treinado gra√ßas √† biblioteca DeepSpeed, utilizando o esquema de otimiza√ß√£o ZeRO (que tamb√©m aparece nessa edi√ß√£o da &lt;em&gt;Newsletter&lt;/em&gt;).&lt;/p&gt;

&lt;!-- Turing Natural Language Generation (T-NLG) is a 17-billion-parameter language model [proposed](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) by Microsoft AI researchers. Besides being the largest known language model to date (depicted in the figure below), T-NLG is a 78-layers Transformer-based language model that outperforms the previous state-of-the-art results (held by NVIDIA‚Äôs [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)) on WikiText-103 perplexity. It was tested on a variety of tasks such as question answering and abstractive summarization while demonstrating desirable benefits such as zero short question capabilities and minimizing supervision, respectively. The model is made possible by a training optimization library called DeepSpeed with ZeRO, which is also featured later in this newsletter. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Modelos de Linguagem e suas quantidades de par√¢metros‚Ää‚Äî&lt;/em&gt;‚Ää&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Neural based Dependency Parsing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A pesquisadora Miryam de Lhoneux liberou recentemente sua tese de Doutorado, entitulada ‚Äú&lt;a href=&quot;http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;amp;dswid=7905&quot;&gt;Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages&lt;/a&gt;‚Äù. O trabalho desenvolvido prop√µe a utiliza√ß√£o de abordagens baseadas em redes neurais para a tarefa de an√°lise de depend√™ncias (&lt;a href=&quot;http://nlpprogress.com/english/dependency_parsing.html&quot;&gt;dependency parsing&lt;/a&gt;) em idiomas com diversas tipologias (l√≠nguas que apresentam padr√µes funcionais e estruturais diferentes). O trabalho apresentado pela autora indica que a incorpora√ß√£o de RNNs e camadas recursivas nos analisadores pode ser ben√©fica para a tarefa, uma vez que essas arquiteturas podem indicar o conhecimento lingu√≠stico necess√°rio √† an√°lise. Outras extens√µes incluem a utiliza√ß√£o de analisadores poliglotas e estrat√©gias de compartilhamento de par√¢metros para a an√°lise de linguagens correlacionadas ou descorrelacionadas.&lt;/p&gt;

&lt;!-- Miryam de Lhoneux released her Ph.D. thesis titled ‚Äú[Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages](http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;dswid=7905)‚Äù. This work is about using neural approaches for [dependency parsing](http://nlpprogress.com/english/dependency_parsing.html) in typologically diverse languages (i.e. languages that construct and express meaning in structurally different ways). This paper reports that RNNs and recursive layers could be beneficial for incorporating into parsers as they help to inform models with important linguistic knowledge needed for parsing. Other ideas include the use of polyglot parsing and parameter sharing strategies for parsing in related and unrelated languages. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;End-to-end Cloud-based Information Extraction with BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Um time de pesquisadores publicou um &lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;artigo&lt;/a&gt; descrevendo como modelos de Transformers, como o BERT, podem auxiliar sistemas de extra√ß√£o de informa√ß√£o de ponta a ponta em documentos de dom√≠nios espec√≠ficos, como documenta√ß√£o regulat√≥ria e de concess√£o de propriedades. Esse tipo de trabalho, al√©m de otimizar opera√ß√µes de neg√≥cios, demonstra a efici√™ncia e aplicabilidade de modelos baseados no BERT em cen√°rios onde bases de dados anotadas s√£o extremamente limitadas. Uma plataforma na nuvem √© apresentada e discutida, assim como os detalhes de sua implementa√ß√£o (ver figura abaixo).&lt;/p&gt;

&lt;!-- A team of researchers published a [paper](https://arxiv.org/abs/2002.01861) describing how Transformer models like BERT can help for end-to-end information extraction in domain-specific business documents such as regulatory filings and property lease agreements. Not only can this type of work help to optimize business operations but it also shows the applicability and effectiveness of BERT-based models on regimes with very low annotated data. An application, and its implementation details, that operates on the cloud is also proposed and discussed (see figure below). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering Benchmark&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em &lt;a href=&quot;https://arxiv.org/abs/2001.11770v1&quot;&gt;Wolfson et al. (2020)&lt;/a&gt;, apresenta-se um &lt;em&gt;benchmark&lt;/em&gt; para a tarefa de &lt;em&gt;question understanding&lt;/em&gt; e um m√©todo para decomposi√ß√£o de quest√µes, uma etapa necess√°ria para a determina√ß√£o de uma resposta apropriada. Os autores recorreram √† um servi√ßo de &lt;em&gt;crowdsourcing&lt;/em&gt; para a cria√ß√£o da base anotada de decomposi√ß√£o de quest√µes. Com objetivo de demonstar a viabilidade e aplicabilidade do m√©todo proposto, os autores mostraram que √© poss√≠vel melhorar o desempenho de modelos utilizando essa t√©cnica sobre a base de dados HotPotQA.&lt;/p&gt;

&lt;!-- [Wolfson et al. (2020)](https://arxiv.org/abs/2001.11770v1) published a question understanding benchmark and a method for breaking down a question that is necessary for computing an appropriate answer. They leverage crowdsourcing to annotate the required steps needed to break down questions. To show the feasibility and applicability of the approach, they improve on open-domain question answering using the HotPotQA dataset. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‚ÄúQuest√µes de diferentes fontes de informa√ß√µes exibem uma estrutura composicional semelhante. Quest√µes em linguagem natural (parte de cima) s√£o decompostas seguindo a metodologia QDMR (meio) e deterministicamente mapeadas para uma linguagem pseudo-formal (parte de baixo).‚Äù‚Ää‚Äî&lt;/em&gt;‚Ää&lt;a href=&quot;https://arxiv.org/pdf/2001.11770v1.pdf&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;
&lt;!-- Questions over different sources share a similar compositional structure. Natural language questions from multiple sources (top) are annotated with the QDMR formalism (middle) and deterministically mapped into a pseudo-formal language (parte inferior). --&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Radioactive data: tracing through training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Membros da equipe de pesquisa em IA do Facebook publicaram recentemente um &lt;a href=&quot;https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/&quot;&gt;trabalho interessante&lt;/a&gt; que prop√µe a marca√ß√£o de imagens (referenciadas como &lt;em&gt;radioactivate data&lt;/em&gt;) de tal maneira que seja poss√≠vel verificar se uma determinada base de dados foi utilizada no treinamento de um modelo de Aprendizado de M√°quina. Os autores conclu√≠ram que √© poss√≠vel utilizar uma marca√ß√£o mais robusta, que move as &lt;em&gt;features&lt;/em&gt; para uma determinada dire√ß√£o, e que pode ser empregada para auxiliar a detec√ß√£o de dados ‚Äúradioativos‚Äù, mesmo quando apenas &lt;strong&gt;1%&lt;/strong&gt; destes est√£o presentes na base de treinamento.&lt;/p&gt;

&lt;p&gt;Essa √© uma tarefa bem desafiadora, uma vez que qualquer modifica√ß√£o nos dados pode potencialmente prejudicar o desempenho do modelo. De acordo com os autores, o trabalho proposto pode ‚Äú&lt;em&gt;ajudar pesquisadores e engenheiros a monitorar quais bases de dados foram utilizadas no treinamento de um modelo, com o objetivo de compreender melhor como bases de dados de diferentes naturezas influenciam o desempenho de diversas redes neurais&lt;/em&gt;‚Äù. Parece uma tarefa crucial para aplica√ß√µes &lt;em&gt;mission-critical&lt;/em&gt;. Confira o artigo completo &lt;a href=&quot;https://arxiv.org/pdf/2002.00937.pdf&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Facebook AI researchers recently published [an interesting work](https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/) that aims to mark images (referred to as radioactive data) so as to verify if that particular data set was used for training the ML model. They found that it is possible to use a clever marker that moves features towards a direction, which the model uses to help detect the usage of radioactive data even when only 1 percent of the training data is radioactive. This is challenging since any change in the data can potentially degrade the model accuracy. According to the authors, this work can ‚Äú*help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks*‚Äù. It seems like an important approach in mission-critical ML applications. Check out the full paper [here](https://arxiv.org/pdf/2002.00937.pdf). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;REALM: Retrieval-Augmented Language Model Pre-Training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O &lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;REALM&lt;/a&gt; √© um m√©todo de recupera√ß√£o em larga escala baseado em redes neurais, que faz uso de bases de conhecimento textual para pr√©-treinar um modelo de linguagem de maneira n√£o-supervisionada. Essencialmente, o objetivo da abordagem √© capturar o conhecimento,  de uma maneira mais interpret√°vel, expondo o modelo √† conhecimentos gerais utilizados durante o processo de treinamento e infer√™ncia atrav√©s do &lt;em&gt;backpropagation&lt;/em&gt;. As bases onde o m√©todo foi testado e avaliado incluem &lt;em&gt;benchmarks&lt;/em&gt; de &lt;em&gt;open-domain question answering&lt;/em&gt;. Al√©m do aumento observado na acur√°cia do modelo, outros benef√≠cios incluem modularidade e interpretabilidade dos componentes.&lt;/p&gt;

&lt;!-- [REALM](https://kentonl.com/pub/gltpc.2020.pdf) is a large-scale neural-based retrieval approach that makes use of a corpus of textual knowledge to pre-train a language model in an unsupervised manner. This approach essentially aims to capture knowledge in a more interpretable way by exposing the model to world knowledge that is used for training and predictions via backpropagation. Tasks approached and evaluated using REALM include open-domain question answering benchmarks. Besides the improvements in the accuracy of the model, other benefits include the modularity and interpretability components. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade üé®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Durante a semana passada, uma &lt;a href=&quot;https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences&quot;&gt;peti√ß√£o&lt;/a&gt; circulou na internet, reivindicando a permiss√£o para apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas, como as relacionadas √† Aprendizado de M√°quina. Para saber mais, acesse &lt;a href=&quot;https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences&quot;&gt;change.org&lt;/a&gt;. Parece que Yoshua Bengio, um dos pioneiros do &lt;em&gt;Deep Learning&lt;/em&gt;, est√° convocando as pessoas √† assinar a peti√ß√£o. Ele deixou isso bem claro em seu novo &lt;a href=&quot;https://yoshuabengio.org/2020/02/10/fusce-risus/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;!-- The past week there was the circulation of a [petition](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences) to allow for remote paper and poster presentations at scientific conferences like ML related ones. Go read more about it on [change.org](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences). It seems Yoshua Bengio, a pioneer in deep learning, is advocating for people to go and sign the petition. He made this clear in his new [blog](https://yoshuabengio.org/2020/02/10/fusce-risus/). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Abstra√ß√£o e Desafios de Racioc√≠nio&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fran√ßois Chollet postou recentemente uma &lt;a href=&quot;https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview&quot;&gt;competi√ß√£o no Kaggle&lt;/a&gt; onde ele disponibilizou o &lt;em&gt;Abstraction and Reasoning Corpus (ARC)&lt;/em&gt;, uma base de dados que tem como objetivo encorajar os usu√°rios a desenvolver sistemas de IA para resolver tarefas √†s quais nunca foram expostos. A esperan√ßa √© que essa competi√ß√£o seja o pontap√© inicial para  a constru√ß√£o de modelos mais robustos de IA, capazes de resolver novos problemas por conta pr√≥pria de maneira mais eficiente e r√°pida, ajudando na resolu√ß√£o de aplica√ß√µes mais desafiadoras do mundo real como a melhoria de carros aut√¥nomos que operam em ambientes diversos e extremos.&lt;/p&gt;

&lt;!-- Fran√ßois Chollet has recently posted a [Kaggle competition](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview) where he released the Abstraction and Reasoning Corpus (ARC) that aims to encourage users to create AI systems that can solve reasoning tasks it has never been exposed to. The hope is to begin to build more robust AI systems that are able to better and quickly solve new problems on its own which could help to address the more challenging real-world applications such as improving self-driving cars that operate in extreme and diverse environments. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Publica√ß√µes de Aprendizado de M√°quina e Processamento de Linguagem Natural em 2019&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Marek Rei liberou sua &lt;a href=&quot;https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/&quot;&gt;an√°lise anual&lt;/a&gt; com estat√≠sticas das publica√ß√µes sobre ML e NLP em 2019. As confer√™ncias consideradas nas an√°lises foram ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, e AAAI.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Growing Neural Cellular Automata&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Morfog√™nese √© um processo de auto-organiza√ß√£o pelo qual alguns animais, como as salamandras, podem regenerar partes de seus corpos que sofreram danos. O processo √© robusto a perturba√ß√µes e adaptativo na natureza. Inspirado nesse esse fen√¥meno biol√≥gico e com a necessidade de uma melhor compreens√£o desse mecanismo, pesquisadores publicaram um &lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;trabalho&lt;/a&gt; entitulado ‚Äú&lt;em&gt;Growing Neural Cellular Automata&lt;/em&gt;‚Äù, que adota um modelo diferenci√°vel para o processo de morfog√™nese buscando replicar os comportamentos e propriedades de sistemas de auto-repara√ß√£o.&lt;/p&gt;

&lt;p&gt;Espera-se que o processo seja capaz de criar m√°quinas ‚Äúauto-repar√°veis‚Äù que possuam a mesma robustez e maleabilidade dos organismos biol√≥gicos. Al√©m disso, o m√©todo pode possibilitar um melhor entendimento do processo de regenera√ß√£o em si. √Åreas que podem se beneficiar com essa pesquisa incluem a medicina regenerativa e a modelagem de sistemas sociais e biol√≥gicos.&lt;/p&gt;

&lt;!-- Morphogenesis is a self-organization process by which some creatures such as salamanders can regenerate or repair body damage. The process is robust to perturbations and adaptive in nature. Inspired by this biological phenomenon and a need to understand the process better, researchers published a [paper](https://distill.pub/2020/growing-ca/) titled ‚ÄúGrowing Neural Cellular Automata‚Äù, which adopts a differentiable model for morphogenesis that aims to replicate behaviors and properties of self-repairing systems. The hope is to be able to build self-repairing machines that possess the same robustness and plasticity as biological life. In addition, it would help to better understand the process of regeneration itself. Applications that can benefit include regenerative medicine and modeling of social and biological systems. --&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualiza√ß√£o do mecanismo de Aten√ß√£o do Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt compartilhou esse &lt;a href=&quot;https://github.com/SIDN-IAP/attnvis&quot;&gt;reposit√≥rio bem interessante&lt;/a&gt; que mostra como construir rapidamente uma  visualiza√ß√£o simples e interativa da Aten√ß√£o do Transformer atrav√©s de uma aplica√ß√£o web utilizando as bibliotecas Hugging Face e d3.js.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*lMaZGDRJUI1Qcv7T5AdhlQ.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/SIDN-IAP/attnvis&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;SketchTransfer: A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- SketchTransfer proposes a new task to test the ability of deep neural networks to support invariance in the presence/absence of details. It has long been debated that deep networks cannot generalize to variations that have not yet been seen during training, something humans can do with relative ease such as dealing with the missing visual details when watching cartoons. The paper discusses and releases a dataset to help researchers carefully study the ‚Äúdetail-invariance‚Äù problem by providing unlabelled sketch images and labeled examples of real images. --&gt;

&lt;p&gt;&lt;br /&gt;
O SketchTransfer prop√µe uma nova tarefa que tem por objetivo testar a habilidade de redes neurais profundas em manter a capacidade invari√¢ncia frente a presen√ßa/aus√™ncia de detalhes. Um debate de longa data existe acerca da inabilidade de redes profundas em generalizar varia√ß√µes que n√£o foram vistas durante o treinamento, algo que os humanos conseguem fazer com relativa facilidade em situa√ß√µes como, por exemplo, a falta de alguns detalhes visuais quando assistimos desenhos. O trabalho discute e disponibiliza uma base de dados esbo√ßos n√£o-anotados e imagens reais anotadas, permitindo que os pesquisadores possam estudar o problema de ‚Äú&lt;em&gt;detail-invariance&lt;/em&gt;‚Äù de maneira bastante cuidadosa.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.11570.pdf&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;bibliotecas-e-bases-de-dados-Ô∏è&quot;&gt;Bibliotecas e Bases de Dados ‚öôÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;DeepSpeed + ZeRO&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Microsoft liberou um pacote para otimiza√ß√£o chamado DeepSpeed, compat√≠vel com o PyTorch, que possibilita o treinamento de modelos com 100 bilh√µes de par√¢metros. A biblioteca d√° destaque a 4 importantes aspectos do processo de treinamento: &lt;em&gt;opera√ß√£o em escala&lt;/em&gt;, &lt;em&gt;velocidade&lt;/em&gt;, &lt;em&gt;custo&lt;/em&gt;, e &lt;em&gt;usabilidade&lt;/em&gt;. A DeepSeed foi &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;liberada&lt;/a&gt; junto com o ZeRO, uma tecnologia que otimiza a utiliza√ß√£o da mem√≥ria, e que possibilita o emprego do Deep Learning em larga escala de maneira distribu√≠da com as atuais tecnologias de GPU, al√©m de melhorar o &lt;em&gt;throughput&lt;/em&gt; em 3-5 vezes em rela√ß√£o √† melhor solu√ß√£o atual. A tecnologia possibilita o treinamento de modelos de tamanho arbitr√°rio que podem ocupar a mem√≥ria total dispon√≠vel, distribu√≠da pelos diversos dispositivos na infra-estrutura.&lt;/p&gt;

&lt;!-- Microsoft open sources a training optimization library called DeepSpeed, which is compatible with PyTorch and can enable the ability to train a 100-billion-parameter model. The library focuses on four important aspects of training a model: *scale*, *speed*, *cost*, and *usability*. DeepSpeed was [released](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) together with ZeRO which is a memory optimization technology for enabling large-scale distributed deep learning in current GPU technology while improving throughput three to five times more than the best current system. ZeRO allows the training of models with any arbitrary size that can fit in the aggregated available memory in terms of shared model states. --&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!-- ***A library for conducting fast and efficient 3D deep learning research*** --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning em Superf√≠cies 3D de Maneira R√°pida e Eficiente&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- [PyTorch3D](https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/) is an open-source toolkit for 3D based deep learning research. This PyTorch library aims to help with the support and understanding of 3D data in deep learning systems. The library consists of fast and optimized implementations of frequently used 3D operators and loss functions. It also comes with a modular differentiable renderer which helps to conduct research on complex 3D inputs and supports high-quality 3D predictions. --&gt;
&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;PyTorch3D&lt;/a&gt; √© uma biblioteca em c√≥digo aberto para a pesquisa de Deep Learning aplicado √† superf√≠cies 3D. Esse pacote, baseado no PyTorch, busca auxiliar no suporte e entendimento de dados em 3D aplicados √† redes neurais. A biblioteca consiste de implementa√ß√µes eficientes e otimizadas de operadores e fun√ß√µes de custo 3D comumente utilizadas. Um renderizador diferenci√°vel modular tamb√©m est√° dispon√≠vel, que pode ser √∫til durante a pesquisa e explora√ß√£o de entradas 3D com padr√µes complexos e na gera√ß√£o de predi√ß√µes de alta qualidade.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!-- ***Managing Configuration of ML projects*** --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Gerenciamento de Configura√ß√µes para projetos de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hydra √© uma ferramenta de configura√ß√£o escrita em Python, que auxilia no gerenciamento de projetos complexos de ML de maneira mais eficiente. O prop√≥sito √© dar suporte a pesquisadores que utilizam o PyTorch, oferencedo a possibilidade de reutiliza√ß√£o de configura√ß√µes de projetos de maneira funcional. O benef√≠cio principal oferecido √© a possibilidade do programador &lt;em&gt;compor configura√ß√µes como comp√µe-se c√≥digo&lt;/em&gt;, o que permite a r√°pida altera√ß√£o de arquivos de configura√ß√£o. A Hydra pode ainda gerenciar automaticamente o diret√≥rio de trabalho que armazena as sa√≠das do seu projeto de ML, o que √© bem √∫til quando precisamos salvar e acessar diversos resultados provenientes de m√∫ltiplos &lt;em&gt;jobs&lt;/em&gt;. Para saber, mais visite o &lt;a href=&quot;https://hydra.cc/&quot;&gt;site&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Hydra is a Python-based configuration tool for more efficiently managing complex ML projects. It is meant to help PyTorch researchers by offering functional reuse of configurations for ML projects. The main benefit it offers is that it allows the programmer to *compose the configuration like composing code*, which means the configuration file can be easily overridden. Hydra can also help with automatically managing the working directory of your ML project outputs which is useful when needing to save and accessing the results of several experiments for multiple jobs. Learn more about it [here](https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710%27). --&gt;

&lt;!-- ***A Toolkit for Causal Inferencing with Bayesian Networks*** --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma biblioteca para Infer√™ncia Causal com Redes Bayesianas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- [CausalNex](https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html) is a toolkit for ‚Äúcausal inference with Bayesian Networks‚Äù. The tool aims to combine machine learning and causal reasoning for uncovering structural relationships in data. The authors also prepared an introductory guide on why and how to infer causation with Bayesian networks using the proposed Python library. --&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;CausalNex&lt;/a&gt; √© uma ferramenta que busca combinar o aprendizado de m√°quina e o racioc√≠nio causal, possibilitando a descoberta de relacionamentos estruturais na base de dados. Os autores prepararam um tutorial introdut√≥rio que mostra porqu√™ e como inferir causalidade com as Redes Bayesianas utilizando a biblioteca proposta.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Google Colab Pro agora est√° dispon√≠vel&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O Google Colab comecou a oferecer uma vers√£o Pro, que disponibiliza vantagens como o acesso exclusivo √† GPUs e TPUs, tempos de execu√ß√£o mais longos e mais mem√≥ria.&lt;/p&gt;

&lt;!-- Google Colab is now offering a Pro edition, which offers advantages such as exclusive access to faster GPUs and TPUs, longer runtimes, and more memory. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TyDi QA: A Multilingual Question Answering Benchmark&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O grupo de IA da Google introduziu recentemente a &lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;TyDi QA&lt;/a&gt;, uma base de dados multi-idiomas que busca encorajar pesquisadores a abordar a tarefa de &lt;em&gt;question answering&lt;/em&gt; em l√≠nguas mais tipologicamente diversas, ou seja, que apresentam padr√µes estrturais n√£o-convencionais. A libera√ß√£o da base visa motivar a constru√ß√£o de modelos mais robustos a idiomas tipologicamente distantes, como o √Årabe, Bengali, Coreano, Russo, Telugo e Tail√¢ndes, podendo generalizar para outros dialetos.&lt;/p&gt;

&lt;!-- Google AI releases [TyDi QA](https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html) which is a multilingual dataset that can encourage researchers to perform question answering on more typologically diverse languages that construct and express meaning in different ways. The idea is to motivate researchers to build more robust models on typologically distant languages, such as Arabic, Bengali, Korean, Russian, Telugu, and Thai, so as to generalize to even more languages. --&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering para Node.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A empresa Hugging Face liberou uma &lt;a href=&quot;https://github.com/huggingface/node-question-answering&quot;&gt;biblioteca para &lt;em&gt;question answering&lt;/em&gt;&lt;/a&gt; baseada no DistilBERT, dando continuidade a sua miss√£o de tornar a √°rea de Processamento de Linguagem Natural mais acess√≠vel. O modelo apresentado pode rodar num ambiente de produ√ß√£o utilizando Node.js com apenas 3 linhas de c√≥digo, se benficiando da implementa√ß√£o eficiente oferecida pela Tokenizers, tamb√©m desenvolvida pelo Hugging Face, e a vers√£o em Javascript do TensorFlow (TensorFlow.js).&lt;/p&gt;

&lt;!-- Hugging Face releases a [question answering library](https://github.com/huggingface/node-question-answering) based on DistilBERT and continues to make NLP more accessible. This model can run in production using Node.js with just 3 lines of code. The model leverages the fast implementation of Tokenizers, also built by Hugging Face, and TensorFlow.js (a popular library for using machine learning models with Javascript). --&gt;

&lt;h1 id=&quot;√©tica-em-ia-&quot;&gt;√âtica em IA üö®&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Identificando vi√©s subjetivo em texto&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Num &lt;a href=&quot;https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922&quot;&gt;podcast&lt;/a&gt; que contou com a participa√ß√£o de Diyi Yang, um pesquisador em ci√™ncia social computacional, foi discutido como sistemas de IA podem auxiliar na identifica√ß√£o de vi√©s subjetivo em informa√ß√µes textuais. Essa √© uma √°rea de pesquisa importante envolvendo Intelig√™ncia Artificial e NLP, especialmente quando discutimos sobre o consumo de textos, como t√≠tulos e chamadas de not√≠cias, e a facilidade que esses meios possuem para influenciar leitores com opini√µes subjetivas.&lt;/p&gt;

&lt;p&gt;Do ponto de vista da aplica√ß√£o, se torna de vital import√¢ncia a tarefa de identifica√ß√£o desse vi√©s de maneira autom√°tica, assim como a conscientiza√ß√£o dos leitores, para que esses se tornem mais atentos e criteriosos em rela√ß√£o ao conte√∫do que est√£o consumindo.&lt;/p&gt;

&lt;!-- This [podcast episode](https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922) features Diyi Yang, a researcher in computational social science, who talks about how AI systems can help to identify subjective bias in textual information. This is an important area of research involving AI systems and NLP especially when we discuss the consumption of text media such as news headlines that can be easily framed to bias consumers when in reality they should aim to be more objective. From an application perspective, it becomes critical to automatically identify the subjective bias present in text media so as to help consumers become more aware of the content they are consuming. The episode also discusses how AI can also perpetuate bias. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Artificial Intelligence, Values and Alignment&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A dissemina√ß√£o de sistemas de IA e a forma com que esses sistemas se alinham com os valores humanos √© uma √°rea de pesquisa envolvendo √©tica na Intelig√™ncia Artificial em crescente atividade. A DeepMind publicou recentemente um &lt;a href=&quot;https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment&quot;&gt;artigo&lt;/a&gt; que busca investigar de maneira mais profunda as quest√µes filos√≥ficas envolvidas no alinhamento da IA com os valores humanos. O trabalho discute duas frentes: a t√©cnica (&lt;em&gt;como codificar valores que permitem que agentes de IA produzam resultados confi√°veis&lt;/em&gt;), e a normativa (&lt;em&gt;quais princ√≠pios deveriam ser codificados pelos modelos&lt;/em&gt;), e como eles se relacionam e podem ser garantidos. O artigo encoraja uma abordagem baseada em princ√≠pios para o alinhamento de valores pelos sistemas de Intelig√™ncia, de modo a preservar um tratamento igualit√°rio e justo frente √†s diferen√ßas de convic√ß√µes e opini√µes.&lt;/p&gt;

&lt;!-- The rise of AI systems and how they align human values is an active area of research that involves ethics in AI systems. DeepMind recently released a [paper](https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment) that takes a deeper look at the philosophical questions surrounding AI alignment. The report focuses on discussing two parts, technical (i.e., *how to encode values that render reliable results from AI agents*) and normative (*what principles would be right to encode in AI*), and how they relate and can be ensured. The paper pushes for a principle-based approach for AI alignment and to preserve fair treatment despite the difference in beliefs and opinions. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Auditoria em Sistemas de IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A VentureBeat divulgou que pesquisadores da Google, numa colabora√ß√£o com outros grupos, criaram um &lt;em&gt;framework&lt;/em&gt; chamado SMACTR, que permite a auditoria de sistemas de IA. A motiva√ß√£o para esse trabalho envolve a aus√™ncia de ‚Äúpresta√ß√£o de contas‚Äù dos sistemas atuais, que s√£o colocados √† disposi√ß√£o do p√∫blico geral. A reportagem completa pode ser acessada &lt;a href=&quot;https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/&quot;&gt;aqui&lt;/a&gt;, assim como o &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3351095.3372873&quot;&gt;trabalho completo&lt;/a&gt;.&lt;/p&gt;

&lt;!-- A VentureBeat reports that Google Researchers, in collaboration with other groups, created a framework called SMACTR that allows engineers to audit AI systems. The reason for this work is to address the accountability gap that exists with current AI systems that are put in the wild to be used by consumers. Read the full report [here](https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/) and the full paper [here](https://dl.acm.org/doi/abs/10.1145/3351095.3372873). --&gt;

&lt;h1 id=&quot;artigos-e-postagens-Ô∏è&quot;&gt;Artigos e Postagens ‚úçÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Destila√ß√£o de modelos em sistemas de NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Num &lt;a href=&quot;https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf&quot;&gt;novo epis√≥dio&lt;/a&gt; do podcast &lt;em&gt;NLP Highlights&lt;/em&gt;, Thomas Wolf and Victor Sanh discutiram sobre a destila√ß√£o de modelos e como a t√©cnica pode ser utilizada como uma alternativa fact√≠vel para a compress√£o de grandes arquiteturas, como o BERT, para aplica√ß√µes escal√°veis de NLP em cen√°rios reais. A metodologia √© discutida no trabalho publicado pelos convidados, entitulado &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;&gt;DistilBERT&lt;/a&gt;, onde s√£o constru√≠dos modelos menores (baseados na mesma arquitetura do modelo original) que tentam simluar o comportamento do modelo com maior n√∫mero de par√¢metros, de acordo com suas sa√≠das. Essencialmente, o menor modelo (&lt;em&gt;student&lt;/em&gt;) tenta modelar a distribui√ß√£o de probabilidade do modelo maior (&lt;em&gt;teacher&lt;/em&gt;) baseado na distribui√ß√£o emp√≠rica gerado por suas sa√≠das.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT, ELMo, &amp;amp; GPT-2: How contextual are contextualized word representations?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O sucesso de m√©todos contextualizados como o BERT para resolu√ß√£o de uma ampla gama de tarefas complexas de NLP √© um assunto que est√° em voga no momento. Nesse &lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;post&lt;/a&gt;, Kawin Ethayarajh tenta responder a quest√£o que diz respeito √† qu√£o contextuais os modelos como BERT, o ELMo e o GPT-2 e seus respectivos &lt;em&gt;word embedddings&lt;/em&gt; contextualizados s√£o. As caracter√≠sticas exploradas incluem m√©tricas de contextualidade, especificidade de contexto, al√©m de compara√ß√µes entre representa√ß√µes vetoriais de palavras ‚Äúest√°ticas‚Äù e suas vers√µes contextualizadas.&lt;/p&gt;

&lt;!-- Recently there has been a lot of talk on the success of contextualized methods like BERT for approaching a wide variety of complex NLP tasks. In this [post](https://kawine.github.io/blog/nlp/2020/02/03/contextual.html), Kawin Ethayarajh attempts to answer the question of how contextual models like BERT, ELMo and GPT-2 and their contextualized word representation are? Topics include measures of contextuality, context-specificity, and comparisons between static embeddings and contextualized representations. --&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Esparsidade em Redes Neurais&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fran√ßois Lagunas, pesquisador na √°rea de ML, escreveu esse excelente &lt;a href=&quot;https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70&quot;&gt;post&lt;/a&gt; compartilhando seu otimismo em rela√ß√£o √† utiliza√ß√£o de tensores esparsos em modelos de redes neurais. A expectativa √© empregar alguma forma de esparsidade visando a redu√ß√£o do tamanho dos modelos atuais, que de certa forma est√£o se tornando impratic√°veis, dadas suas colossais quantidades de par√¢metros. Os Transformers, por exemplo, com seus bilh√µes de par√¢metros, poderiam se beneficar com o emprego dessa t√©cnica.&lt;/p&gt;

&lt;p&gt;Entretanto, os detalhes de implementa√ß√£o para viabilizar a utiliza√ß√£o eficiente da esparsidade em GPU ainda n√£o est√£o claros‚Ä¶ Felizmente, a comunidade de Aprendizado de M√°quina j√° est√° trabalhando nisso!&lt;/p&gt;

&lt;!-- Fran√ßois Lagunas, an ML researcher, wrote this great [blog post](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70) discussing his optimism for adopting sparse tensors in neural network models. The hope is to employ some form of sparsity to reduce the size of current models that at some point become unpractical due to their size and speed. This concept may be worth exploring in ML due to the sheer size of current models like Transformers (often relying on billions of parameters). However, the implementation details to support efficient sparsity in neural networks on GPUs are not so clear from a developer tool perspective and that is something the machine learning community is working on already. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Treinando Seu Pr√≥prio Modelo de Linguagem&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Se voc√™ est√° interessado em aprender como treinar um modelo de linguagem do zero, confira esse excelente &lt;a href=&quot;https://huggingface.co/blog/how-to-train&quot;&gt;tutorial&lt;/a&gt; da Hugging Face que utiliza as suas incr√≠veis bibliotecas Tokenizers e Transformers no treinamento do modelo.
&lt;!--
If you are interested to learn how to train a language model scratch, check out this impressive and comprehensive [tutorial](https://huggingface.co/blog/how-to-train) by Hugging Face. They obviously leverage their own libraries Transformers and Tokenizers to train the model. --&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizers: How machines read&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cathal Horan publicou um &lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/&quot;&gt;blog post&lt;/a&gt; impresssionante e bem detalhado sobre como e quais tipos de &lt;em&gt;tokenizers&lt;/em&gt; v√™m sendo utilizados nos mais recentes modelos de NLP, auxiliando modelos de Intelig√™ncia a aprender por meio de informa√ß√µes textuais. O post tamb√©m discute e motiva porqu√™ a tarefa de tokeniza√ß√£o √© uma importante e desafiadora √°rea de pesquisa ativa. O artigo apresenta ainda como treinar o seu pr√≥prio &lt;em&gt;tokenizer&lt;/em&gt; utilizando m√©todos como o SentencePiece e o WordPiece.&lt;/p&gt;

&lt;!-- Cathal Horan published an impressive and very detailed [blog post](https://blog.floydhub.com/tokenization-nlp/) about how and what type of tokenizers are being used by the most recent NLP models to help machine learning algorithms learn from textual information. He also discusses and motivated why tokenization is an exciting and important active area of research. The article even shows you how to train your own tokenizers using tokenization methods like SentencePiece and WordPiece. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/%27&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;educa√ß√£o-&quot;&gt;Educa√ß√£o üéì&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Machine Learning na VU Amsterdam&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Agora voc√™ pode acompanahar o curso &lt;a href=&quot;https://mlvu.github.io/&quot;&gt;2020 MLVU machine learning&lt;/a&gt; pela internet, onde est√£o inclusas a lista completa de slides, &lt;a href=&quot;https://www.youtube.com/watch?v=excCZSTJEPs&amp;amp;feature=youtu.be&quot;&gt;videos&lt;/a&gt; e o plano de estudos. O curso oferece uma introdu√ß√£o √† ML, al√©m de cobrir t√≥picos mais avan√ßados de Deep Learning, como &lt;em&gt;Variational AutoEncoders&lt;/em&gt; (VAEs) e Redes Neurais Adversariais (GANs).&lt;/p&gt;

&lt;!-- You can now follow the [2020 MLVU machine learning](https://mlvu.github.io/) course online, which includes the full set of slides, [videos](https://www.youtube.com/watch?v=excCZSTJEPs&amp;feature=youtu.be), and syllabus. It is meant to be an introduction to ML but it also has other deep learning related topics such as VAEs and GANs. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zFpU2rQL5Fby7X3boJyQNg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mlvu.github.io/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Materiais de Matem√°tica para ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Suzana Iliƒá e a organiza√ß√£o Machine Learning Tokyo (MLT) v√™m realizando um excelente trabalho em prol da democratiza√ß√£o do conhecimento em ML. Confira esse &lt;a href=&quot;https://github.com/Machine-Learning-Tokyo/Math_resources&quot;&gt;reposit√≥rio&lt;/a&gt; que apresenta uma cole√ß√£o de fontes e materiais sobre os fundamentos matem√°ticos utilizados em Aprendizado de M√°quina.&lt;/p&gt;

&lt;!-- Suzana Iliƒá and the Machine Learning Tokyo (MLT) have been doing amazing work in terms of democratizing ML education. For example, check out this [repository](https://github.com/Machine-Learning-Tokyo/Math_resources) showcasing a collection of free online resources for learning about the foundations of mathematical concepts used in ML. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction to Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Acompanhe o curso ‚Äú&lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;Introduction to Deep Learning&lt;/a&gt;‚Äù do MIT nesse site. Novas aulas ser√£o postadas toda semanas e todos os materiais, como slides, v√≠deos e c√≥digos utilizados, ser√£o publicados.&lt;/p&gt;

&lt;!-- Keep track of the ‚Äú[Introduction to Deep Learning](http://introtodeeplearning.com/)‚Äù course by MIT on this website. New lectures will be posted every week and all the sides and videos, including coding labs, will be published. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning com PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Alfredo Canziani publicou os slides e notebooks utilizados no minicurso de Deep Learning com Pytorch. O reposit√≥rio cont√©m ainda um &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/&quot;&gt;site&lt;/a&gt; que incluem notas sobre os conceitos apresentados no curso.&lt;/p&gt;

&lt;!-- Alfredo Canziani has published the slides and notebooks for the minicourse on Deep Learning with PyTorch. The repository also contains a [companion website](https://atcold.github.io/pytorch-Deep-Learning-Minicourse/) that includes text descriptions of the concepts taught in the course. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Missing Semester of Your CS&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O ‚Äú&lt;a href=&quot;https://missing.csail.mit.edu/&quot;&gt;Missing Semester of Your CS&lt;/a&gt;‚Äù √© um excelente curso online composto por recursos que podem ser potencialmente √∫teis para cientistas de dados que n√£o possuem background na √°rea de desenvolvimenteo. Est√£o inclusos materiais sobre &lt;em&gt;shell scripting&lt;/em&gt; e versionamento. O curso foi disponibilizado por alunos do MIT.
&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://missing.csail.mit.edu/2020/shell-tools/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Advanced Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A CMU disponibilizou recentemente os slides e plano de estudos para o curso ‚Äú&lt;a href=&quot;https://andrejristeski.github.io/10707-S20/syllabus.html&quot;&gt;Advanced Deep Learning&lt;/a&gt;‚Äù, que cobre t√≥picos como modelos autoregressivos, modelos generativos, aprendizado autosupervisionado, entre outros. O p√∫blico-alvo s√£o alunos de mestrado e doutorado com s√≥lidos conhecimentos de ML.&lt;/p&gt;

&lt;!-- A CMU released the slides and syllabus for the ‚Äú[Advanced Deep Learning](https://andrejristeski.github.io/10707-S20/syllabus.html)‚Äù course which includes topics such as autoregressive models, generative models, and self-supervised/predictive learning, among others. The course is meant for MS or Ph.D. students with an advanced background in ML. --&gt;

&lt;h1 id=&quot;men√ß√µes-honrosas-Ô∏è&quot;&gt;Men√ß√µes honrosas ‚≠êÔ∏è&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
Voc√™ pode encontrar a √∫ltima Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060&quot;&gt;aqui&lt;/a&gt;. Essa edi√ß√£o cobriu t√≥picos como melhorias em agentes conversacionais, divulga√ß√£o de modelos BERT para idiomas espec√≠ficos (entre eles o &lt;strong&gt;Portugu√™s&lt;/strong&gt;!!!), bases de dados publicamente dispon√≠vies, introdu√ß√£o de novas bibliotecas para Deep Learning, e muito mais.&lt;/p&gt;

&lt;!-- You can catch the previous NLP Newsletter here. The [issue](https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060) covers topics such as improving conversational agents, releases of language-specific BERT models, free datasets, releases of deep learning libraries, and much more. --&gt;

&lt;p&gt;Em Xu et al. (2020), foi proposto um &lt;a href=&quot;https://arxiv.org/abs/2002.02925]&quot;&gt;m√©todo&lt;/a&gt; para progressivamente substituir e comprimir modelos BERT atrav√©s da separa√ß√£o de seus componentes originais. Atrav√©s dessa substitui√ß√£o progressiva, aliado ao processo de treinamento, √© poss√≠vel combinar os componentes originais e suas vers√µes compactadas. A metodologia apresentada supera outras abordagens de &lt;em&gt;knowledge distillation&lt;/em&gt; no benchmark GLUE.&lt;/p&gt;

&lt;!-- Xu et al. (2020) proposed a [method](https://arxiv.org/abs/2002.02925]) for progressively replacing and compressing a BERT model by dividing it into its original components. Through progressive replacement and training, there is also the advantage of combining the original components and compacted versions of the model. The proposed model outperforms other knowledge distillation approaches on the GLUE benchmark. --&gt;

&lt;p&gt;Um outro curso interessante √© o ‚Äú&lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;Introduction to Machine Learning&lt;/a&gt;‚Äù, que cobre o b√°sico de ML, regress√£o supervisionada, &lt;em&gt;random forests&lt;/em&gt;, ajuste de par√¢metros, dentre outros conceitos fundamentais.&lt;/p&gt;

&lt;!-- Here is another interesting course called ‚Äú[Introduction to Machine Learning](https://compstat-lmu.github.io/lecture_i2ml/index.html)‚Äù which covers the ML basics, supervised regression, random forests, parameter tuning, and many more fundamental ML topics. --&gt;

&lt;p&gt;A vers√£o para üá¨üá∑ grego do BERT (&lt;a href=&quot;https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1&quot;&gt;GreekBERT&lt;/a&gt;) est√° dispon√≠vel para uso atrav√©s da biblioteca Transformers, da Hugging Face.&lt;/p&gt;

&lt;p&gt;Jeremy Howard publicou um &lt;a href=&quot;https://arxiv.org/abs/2002.04688&quot;&gt;artigo&lt;/a&gt; descrevendo a biblioteca de Deep Learning fast.ai, que √© amplamente utilizada para pesquisa e ensino em seus cursos de Deep Learning. Uma leitura bastante recomendada para desenvolvedores de software que trabalham construindo e melhorando pacotes de Aprendizado de M√°quina e Deep Learning.
&lt;!-- 
Jeremy Howard publishes a [paper](https://arxiv.org/abs/2002.04688) describing the fastai deep learning library which is widely used for research and to teach their open courses on deep learning. A recommended read for software developers working on building and improving deep learning and ML libraries. --&gt;&lt;/p&gt;

&lt;p&gt;A Deeplearning.ai completou o lan√ßamento dos seus 4 cursos da s√©rie &lt;a href=&quot;https://www.coursera.org/specializations/tensorflow-data-and-deployment&quot;&gt;TensorFlow: Data and Deployment Specialization&lt;/a&gt;. A especializa√ß√£o visa ensinar desenvolverdores a como realizar o &lt;em&gt;deploy&lt;/em&gt; de modelos de maneira efetiva e eficiente nos mais diferentes cen√°rios, al√©m de utilizar dados de maneiras eficazes durante o treinamento de modelos.&lt;/p&gt;

&lt;!-- Deeplearning.ai completes the release of all four courses of the [TensorFlow: Data and Deployment Specialization](https://www.coursera.org/specializations/tensorflow-data-and-deployment). The specialization mainly aims to educate developers on how to efficiently and effectively deploy models in different scenarios and make use of data in interesting and effective ways while training models. --&gt;

&lt;p&gt;Sebastian Raschka publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/2002.04803&quot;&gt;artigo&lt;/a&gt; entitulado ‚ÄúMachine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence‚Äù. O trabalho apresenta uma revis√£o bastante acess√≠vel √†s mais variadas ferramentas de ML. √â um artigo excelente para compreender as vantagens de algumas bibliotecas e conceitos utilizados em Aprendizado de M√°quina. Al√©m disso, levanta-se a discuss√£o sobre o futuro de bibliotecas de ML baseadas em Python.&lt;/p&gt;

&lt;!-- Sebastian Raschka recently published a [paper](https://arxiv.org/abs/2002.04803) titled ‚ÄúMachine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence‚Äù. The paper serves as a comprehensive review of the machine learning tools landscape. It is an excellent report for understanding the various advantages of some libraries and concepts used in ML engineering. In addition, a word on the future of Python-based machine learning libraries is provided. --&gt;</content><author><name>VictorGarritano</name></author><summary type="html">Essa edi√ß√£o cobre t√≥picos como modelos de linguagem maiores, avan√ßos na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_4.png%22%7D" /><media:content medium="image" url="https://victorgarritano.github.io/personal_blog/%7B%22thumb%22=%3E%22nlp_newsletter_4.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>