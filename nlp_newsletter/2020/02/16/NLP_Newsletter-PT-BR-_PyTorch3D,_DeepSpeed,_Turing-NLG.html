<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,… | Garritano’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…" />
<meta name="author" content="VictorGarritano" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais" />
<meta property="og:description" content="Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais" />
<link rel="canonical" href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" />
<meta property="og:url" content="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" />
<meta property="og:site_name" content="Garritano’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-16T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"VictorGarritano"},"description":"Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html"},"url":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html","headline":"NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…","dateModified":"2020-02-16T00:00:00-06:00","datePublished":"2020-02-16T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/personal_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://victorgarritano.github.io/personal_blog/feed.xml" title="Garritano's Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-162360650-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/personal_blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,… | Garritano’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…" />
<meta name="author" content="VictorGarritano" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais" />
<meta property="og:description" content="Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais" />
<link rel="canonical" href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" />
<meta property="og:url" content="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" />
<meta property="og:site_name" content="Garritano’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-16T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"VictorGarritano"},"description":"Essa edição cobre tópicos como modelos de linguagem maiores, avanços na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html"},"url":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html","headline":"NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…","dateModified":"2020-02-16T00:00:00-06:00","datePublished":"2020-02-16T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://victorgarritano.github.io/personal_blog/feed.xml" title="Garritano's Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-162360650-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/personal_blog/">Garritano&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/personal_blog/about/">About Me</a><a class="page-link" href="/personal_blog/search/">Search</a><a class="page-link" href="/personal_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">NLP Newsletter [PT-BR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-02-16T00:00:00-06:00" itemprop="datePublished">
        Feb 16, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">VictorGarritano</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      17 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/personal_blog/categories/#nlp_newsletter">nlp_newsletter</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png" alt="" /></p>

<h1 id="publicações-">Publicações 📙</h1>

<p><strong><em>Turing-NLG: A 17-billion-parameter language model by Microsoft</em></strong></p>

<p><br />
O Turing Natural Language Generation (T-NLG) é um modelo de linguagem com 17 bilhões de parâmetros <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">proposto</a> pelo grupo de pesquisa em Inteligência Artificial da Microsoft. O T-NLG é composto por 78 camadas, baseado na arquitetura dos Transformers, que superou o estado da arte anterior (atribuido ao <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> da Nvidia) considerando a Perplexidade na base de dados WikiText-103. O modelo foi testado em diversas tarefas, como <em>question answering</em> e sumarização abstrata, onde foram observados comportamentos interessantes e desejáveis para modelos dessa categoria, como <em>“zero shot” question answering</em> (onde o modelo responde a um pergunta sem estar ciente do contexto explicitamente), e baixa necessidade de bases previamente anotadas, para as tarefas citadas anteriormente. O modelo pode ser treinado graças à biblioteca DeepSpeed, utilizando o esquema de otimização ZeRO (que também aparece nessa edição da <em>Newsletter</em>).</p>

<!-- Turing Natural Language Generation (T-NLG) is a 17-billion-parameter language model [proposed](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) by Microsoft AI researchers. Besides being the largest known language model to date (depicted in the figure below), T-NLG is a 78-layers Transformer-based language model that outperforms the previous state-of-the-art results (held by NVIDIA’s [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)) on WikiText-103 perplexity. It was tested on a variety of tasks such as question answering and abstractive summarization while demonstrating desirable benefits such as zero short question capabilities and minimizing supervision, respectively. The model is made possible by a training optimization library called DeepSpeed with ZeRO, which is also featured later in this newsletter. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png" alt="" /></p>

<p><em>Modelos de Linguagem e suas quantidades de parâmetros —</em> <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/"><em>fonte</em></a></p>

<p><br />
<strong><em>Neural based Dependency Parsing</em></strong></p>

<p><br />
A pesquisadora Miryam de Lhoneux liberou recentemente sua tese de Doutorado, entitulada “<a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;dswid=7905">Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages</a>”. O trabalho desenvolvido propõe a utilização de abordagens baseadas em redes neurais para a tarefa de análise de dependências (<a href="http://nlpprogress.com/english/dependency_parsing.html">dependency parsing</a>) em idiomas com diversas tipologias (línguas que apresentam padrões funcionais e estruturais diferentes). O trabalho apresentado pela autora indica que a incorporação de RNNs e camadas recursivas nos analisadores pode ser benéfica para a tarefa, uma vez que essas arquiteturas podem indicar o conhecimento linguístico necessário à análise. Outras extensões incluem a utilização de analisadores poliglotas e estratégias de compartilhamento de parâmetros para a análise de linguagens correlacionadas ou descorrelacionadas.</p>

<!-- Miryam de Lhoneux released her Ph.D. thesis titled “[Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages](http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&dswid=7905)”. This work is about using neural approaches for [dependency parsing](http://nlpprogress.com/english/dependency_parsing.html) in typologically diverse languages (i.e. languages that construct and express meaning in structurally different ways). This paper reports that RNNs and recursive layers could be beneficial for incorporating into parsers as they help to inform models with important linguistic knowledge needed for parsing. Other ideas include the use of polyglot parsing and parameter sharing strategies for parsing in related and unrelated languages. -->

<p><br />
<strong><em>End-to-end Cloud-based Information Extraction with BERT</em></strong></p>

<p><br />
Um time de pesquisadores publicou um <a href="https://arxiv.org/abs/2002.01861">artigo</a> descrevendo como modelos de Transformers, como o BERT, podem auxiliar sistemas de extração de informação de ponta a ponta em documentos de domínios específicos, como documentação regulatória e de concessão de propriedades. Esse tipo de trabalho, além de otimizar operações de negócios, demonstra a eficiência e aplicabilidade de modelos baseados no BERT em cenários onde bases de dados anotadas são extremamente limitadas. Uma plataforma na nuvem é apresentada e discutida, assim como os detalhes de sua implementação (ver figura abaixo).</p>

<!-- A team of researchers published a [paper](https://arxiv.org/abs/2002.01861) describing how Transformer models like BERT can help for end-to-end information extraction in domain-specific business documents such as regulatory filings and property lease agreements. Not only can this type of work help to optimize business operations but it also shows the applicability and effectiveness of BERT-based models on regimes with very low annotated data. An application, and its implementation details, that operates on the cloud is also proposed and discussed (see figure below). -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.01861"><em>fonte</em></a></p>

<p><br />
<strong><em>Question Answering Benchmark</em></strong></p>

<p><br />
Em <a href="https://arxiv.org/abs/2001.11770v1">Wolfson et al. (2020)</a>, apresenta-se um <em>benchmark</em> para a tarefa de <em>question understanding</em> e um método para decomposição de questões, uma etapa necessária para a determinação de uma resposta apropriada. Os autores recorreram à um serviço de <em>crowdsourcing</em> para a criação da base anotada de decomposição de questões. Com objetivo de demonstar a viabilidade e aplicabilidade do método proposto, os autores mostraram que é possível melhorar o desempenho de modelos utilizando essa técnica sobre a base de dados HotPotQA.</p>

<!-- [Wolfson et al. (2020)](https://arxiv.org/abs/2001.11770v1) published a question understanding benchmark and a method for breaking down a question that is necessary for computing an appropriate answer. They leverage crowdsourcing to annotate the required steps needed to break down questions. To show the feasibility and applicability of the approach, they improve on open-domain question answering using the HotPotQA dataset. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png" alt="" /></p>

<p><em>“Questões de diferentes fontes de informações exibem uma estrutura composicional semelhante. Questões em linguagem natural (parte de cima) são decompostas seguindo a metodologia QDMR (meio) e deterministicamente mapeadas para uma linguagem pseudo-formal (parte de baixo).” —</em> <a href="https://arxiv.org/pdf/2001.11770v1.pdf"><em>fonte</em></a>
<!-- Questions over different sources share a similar compositional structure. Natural language questions from multiple sources (top) are annotated with the QDMR formalism (middle) and deterministically mapped into a pseudo-formal language (parte inferior). --></p>

<p><br />
<strong><em>Radioactive data: tracing through training</em></strong></p>

<p><br />
Membros da equipe de pesquisa em IA do Facebook publicaram recentemente um <a href="https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/">trabalho interessante</a> que propõe a marcação de imagens (referenciadas como <em>radioactivate data</em>) de tal maneira que seja possível verificar se uma determinada base de dados foi utilizada no treinamento de um modelo de Aprendizado de Máquina. Os autores concluíram que é possível utilizar uma marcação mais robusta, que move as <em>features</em> para uma determinada direção, e que pode ser empregada para auxiliar a detecção de dados “radioativos”, mesmo quando apenas <strong>1%</strong> destes estão presentes na base de treinamento.</p>

<p>Essa é uma tarefa bem desafiadora, uma vez que qualquer modificação nos dados pode potencialmente prejudicar o desempenho do modelo. De acordo com os autores, o trabalho proposto pode “<em>ajudar pesquisadores e engenheiros a monitorar quais bases de dados foram utilizadas no treinamento de um modelo, com o objetivo de compreender melhor como bases de dados de diferentes naturezas influenciam o desempenho de diversas redes neurais</em>”. Parece uma tarefa crucial para aplicações <em>mission-critical</em>. Confira o artigo completo <a href="https://arxiv.org/pdf/2002.00937.pdf">aqui</a>.</p>

<!-- Facebook AI researchers recently published [an interesting work](https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/) that aims to mark images (referred to as radioactive data) so as to verify if that particular data set was used for training the ML model. They found that it is possible to use a clever marker that moves features towards a direction, which the model uses to help detect the usage of radioactive data even when only 1 percent of the training data is radioactive. This is challenging since any change in the data can potentially degrade the model accuracy. According to the authors, this work can “*help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks*”. It seems like an important approach in mission-critical ML applications. Check out the full paper [here](https://arxiv.org/pdf/2002.00937.pdf). -->

<p><br />
<strong><em>REALM: Retrieval-Augmented Language Model Pre-Training</em></strong></p>

<p><br />
O <a href="https://kentonl.com/pub/gltpc.2020.pdf">REALM</a> é um método de recuperação em larga escala baseado em redes neurais, que faz uso de bases de conhecimento textual para pré-treinar um modelo de linguagem de maneira não-supervisionada. Essencialmente, o objetivo da abordagem é capturar o conhecimento,  de uma maneira mais interpretável, expondo o modelo à conhecimentos gerais utilizados durante o processo de treinamento e inferência através do <em>backpropagation</em>. As bases onde o método foi testado e avaliado incluem <em>benchmarks</em> de <em>open-domain question answering</em>. Além do aumento observado na acurácia do modelo, outros benefícios incluem modularidade e interpretabilidade dos componentes.</p>

<!-- [REALM](https://kentonl.com/pub/gltpc.2020.pdf) is a large-scale neural-based retrieval approach that makes use of a corpus of textual knowledge to pre-train a language model in an unsupervised manner. This approach essentially aims to capture knowledge in a more interpretable way by exposing the model to world knowledge that is used for training and predictions via backpropagation. Tasks approached and evaluated using REALM include open-domain question answering benchmarks. Besides the improvements in the accuracy of the model, other benefits include the modularity and interpretability components. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png" alt="" /></p>

<p><a href="https://kentonl.com/pub/gltpc.2020.pdf"><em>fonte</em></a></p>

<h1 id="criatividade-e-sociedade-">Criatividade e Sociedade 🎨</h1>

<p><strong><em>Apresentações remotas de artigos e pôsteres em conferências científicas</em></strong></p>

<p><br />
Durante a semana passada, uma <a href="https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences">petição</a> circulou na internet, reivindicando a permissão para apresentações remotas de artigos e pôsteres em conferências científicas, como as relacionadas à Aprendizado de Máquina. Para saber mais, acesse <a href="https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences">change.org</a>. Parece que Yoshua Bengio, um dos pioneiros do <em>Deep Learning</em>, está convocando as pessoas à assinar a petição. Ele deixou isso bem claro em seu novo <a href="https://yoshuabengio.org/2020/02/10/fusce-risus/">blog</a>.</p>

<!-- The past week there was the circulation of a [petition](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences) to allow for remote paper and poster presentations at scientific conferences like ML related ones. Go read more about it on [change.org](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences). It seems Yoshua Bengio, a pioneer in deep learning, is advocating for people to go and sign the petition. He made this clear in his new [blog](https://yoshuabengio.org/2020/02/10/fusce-risus/). -->

<p><br />
<strong><em>Abstração e Desafios de Raciocínio</em></strong></p>

<p><br />
François Chollet postou recentemente uma <a href="https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview">competição no Kaggle</a> onde ele disponibilizou o <em>Abstraction and Reasoning Corpus (ARC)</em>, uma base de dados que tem como objetivo encorajar os usuários a desenvolver sistemas de IA para resolver tarefas às quais nunca foram expostos. A esperança é que essa competição seja o pontapé inicial para  a construção de modelos mais robustos de IA, capazes de resolver novos problemas por conta própria de maneira mais eficiente e rápida, ajudando na resolução de aplicações mais desafiadoras do mundo real como a melhoria de carros autônomos que operam em ambientes diversos e extremos.</p>

<!-- François Chollet has recently posted a [Kaggle competition](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview) where he released the Abstraction and Reasoning Corpus (ARC) that aims to encourage users to create AI systems that can solve reasoning tasks it has never been exposed to. The hope is to begin to build more robust AI systems that are able to better and quickly solve new problems on its own which could help to address the more challenging real-world applications such as improving self-driving cars that operate in extreme and diverse environments. -->

<p><br />
<strong><em>Publicações de Aprendizado de Máquina e Processamento de Linguagem Natural em 2019</em></strong></p>

<p><br />
Marek Rei liberou sua <a href="https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/">análise anual</a> com estatísticas das publicações sobre ML e NLP em 2019. As conferências consideradas nas análises foram ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, e AAAI.</p>

<p><br />
<strong><em>Growing Neural Cellular Automata</em></strong></p>

<p><br />
Morfogênese é um processo de auto-organização pelo qual alguns animais, como as salamandras, podem regenerar partes de seus corpos que sofreram danos. O processo é robusto a perturbações e adaptativo na natureza. Inspirado nesse esse fenômeno biológico e com a necessidade de uma melhor compreensão desse mecanismo, pesquisadores publicaram um <a href="https://distill.pub/2020/growing-ca/">trabalho</a> entitulado “<em>Growing Neural Cellular Automata</em>”, que adota um modelo diferenciável para o processo de morfogênese buscando replicar os comportamentos e propriedades de sistemas de auto-reparação.</p>

<p>Espera-se que o processo seja capaz de criar máquinas “auto-reparáveis” que possuam a mesma robustez e maleabilidade dos organismos biológicos. Além disso, o método pode possibilitar um melhor entendimento do processo de regeneração em si. Áreas que podem se beneficiar com essa pesquisa incluem a medicina regenerativa e a modelagem de sistemas sociais e biológicos.</p>

<!-- Morphogenesis is a self-organization process by which some creatures such as salamanders can regenerate or repair body damage. The process is robust to perturbations and adaptive in nature. Inspired by this biological phenomenon and a need to understand the process better, researchers published a [paper](https://distill.pub/2020/growing-ca/) titled “Growing Neural Cellular Automata”, which adopts a differentiable model for morphogenesis that aims to replicate behaviors and properties of self-repairing systems. The hope is to be able to build self-repairing machines that possess the same robustness and plasticity as biological life. In addition, it would help to better understand the process of regeneration itself. Applications that can benefit include regenerative medicine and modeling of social and biological systems. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png" alt="" /></p>

<p><a href="https://distill.pub/2020/growing-ca/"><em>fonte</em></a></p>

<p><br />
<strong><em>Visualização do mecanismo de Atenção do Transformer</em></strong></p>

<p><br />
Hendrik Strobelt compartilhou esse <a href="https://github.com/SIDN-IAP/attnvis">repositório bem interessante</a> que mostra como construir rapidamente uma  visualização simples e interativa da Atenção do Transformer através de uma aplicação web utilizando as bibliotecas Hugging Face e d3.js.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*lMaZGDRJUI1Qcv7T5AdhlQ.gif" alt="" /></p>

<p><a href="https://github.com/SIDN-IAP/attnvis"><em>fonte</em></a></p>

<p><br />
<strong><em>SketchTransfer: A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks</em></strong></p>

<!-- SketchTransfer proposes a new task to test the ability of deep neural networks to support invariance in the presence/absence of details. It has long been debated that deep networks cannot generalize to variations that have not yet been seen during training, something humans can do with relative ease such as dealing with the missing visual details when watching cartoons. The paper discusses and releases a dataset to help researchers carefully study the “detail-invariance” problem by providing unlabelled sketch images and labeled examples of real images. -->

<p><br />
O SketchTransfer propõe uma nova tarefa que tem por objetivo testar a habilidade de redes neurais profundas em manter a capacidade invariância frente a presença/ausência de detalhes. Um debate de longa data existe acerca da inabilidade de redes profundas em generalizar variações que não foram vistas durante o treinamento, algo que os humanos conseguem fazer com relativa facilidade em situações como, por exemplo, a falta de alguns detalhes visuais quando assistimos desenhos. O trabalho discute e disponibiliza uma base de dados esboços não-anotados e imagens reais anotadas, permitindo que os pesquisadores possam estudar o problema de “<em>detail-invariance</em>” de maneira bastante cuidadosa.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png" alt="" /></p>

<p><a href="https://arxiv.org/pdf/1912.11570.pdf"><em>fonte</em></a></p>

<h1 id="bibliotecas-e-bases-de-dados-️">Bibliotecas e Bases de Dados ⚙️</h1>

<p><strong><em>DeepSpeed + ZeRO</em></strong></p>

<p><br />
A Microsoft liberou um pacote para otimização chamado DeepSpeed, compatível com o PyTorch, que possibilita o treinamento de modelos com 100 bilhões de parâmetros. A biblioteca dá destaque a 4 importantes aspectos do processo de treinamento: <em>operação em escala</em>, <em>velocidade</em>, <em>custo</em>, e <em>usabilidade</em>. A DeepSeed foi <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">liberada</a> junto com o ZeRO, uma tecnologia que otimiza a utilização da memória, e que possibilita o emprego do Deep Learning em larga escala de maneira distribuída com as atuais tecnologias de GPU, além de melhorar o <em>throughput</em> em 3-5 vezes em relação à melhor solução atual. A tecnologia possibilita o treinamento de modelos de tamanho arbitrário que podem ocupar a memória total disponível, distribuída pelos diversos dispositivos na infra-estrutura.</p>

<!-- Microsoft open sources a training optimization library called DeepSpeed, which is compatible with PyTorch and can enable the ability to train a 100-billion-parameter model. The library focuses on four important aspects of training a model: *scale*, *speed*, *cost*, and *usability*. DeepSpeed was [released](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) together with ZeRO which is a memory optimization technology for enabling large-scale distributed deep learning in current GPU technology while improving throughput three to five times more than the best current system. ZeRO allows the training of models with any arbitrary size that can fit in the aggregated available memory in terms of shared model states. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif" alt="" /></p>

<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/"><em>fonte</em></a></p>

<!-- ***A library for conducting fast and efficient 3D deep learning research*** -->

<p><br />
<strong><em>Deep Learning em Superfícies 3D de Maneira Rápida e Eficiente</em></strong></p>

<!-- [PyTorch3D](https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/) is an open-source toolkit for 3D based deep learning research. This PyTorch library aims to help with the support and understanding of 3D data in deep learning systems. The library consists of fast and optimized implementations of frequently used 3D operators and loss functions. It also comes with a modular differentiable renderer which helps to conduct research on complex 3D inputs and supports high-quality 3D predictions. -->
<p><br />
A <a href="https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/">PyTorch3D</a> é uma biblioteca em código aberto para a pesquisa de Deep Learning aplicado à superfícies 3D. Esse pacote, baseado no PyTorch, busca auxiliar no suporte e entendimento de dados em 3D aplicados à redes neurais. A biblioteca consiste de implementações eficientes e otimizadas de operadores e funções de custo 3D comumente utilizadas. Um renderizador diferenciável modular também está disponível, que pode ser útil durante a pesquisa e exploração de entradas 3D com padrões complexos e na geração de predições de alta qualidade.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png" alt="" /></p>

<p><a href="https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/"><em>fonte</em></a></p>

<!-- ***Managing Configuration of ML projects*** -->

<p><br />
<strong><em>Gerenciamento de Configurações para projetos de ML</em></strong></p>

<p><br />
Hydra é uma ferramenta de configuração escrita em Python, que auxilia no gerenciamento de projetos complexos de ML de maneira mais eficiente. O propósito é dar suporte a pesquisadores que utilizam o PyTorch, oferencedo a possibilidade de reutilização de configurações de projetos de maneira funcional. O benefício principal oferecido é a possibilidade do programador <em>compor configurações como compõe-se código</em>, o que permite a rápida alteração de arquivos de configuração. A Hydra pode ainda gerenciar automaticamente o diretório de trabalho que armazena as saídas do seu projeto de ML, o que é bem útil quando precisamos salvar e acessar diversos resultados provenientes de múltiplos <em>jobs</em>. Para saber, mais visite o <a href="https://hydra.cc/">site</a>.</p>

<!-- Hydra is a Python-based configuration tool for more efficiently managing complex ML projects. It is meant to help PyTorch researchers by offering functional reuse of configurations for ML projects. The main benefit it offers is that it allows the programmer to *compose the configuration like composing code*, which means the configuration file can be easily overridden. Hydra can also help with automatically managing the working directory of your ML project outputs which is useful when needing to save and accessing the results of several experiments for multiple jobs. Learn more about it [here](https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710%27). -->

<!-- ***A Toolkit for Causal Inferencing with Bayesian Networks*** -->

<p><br />
<strong><em>Uma biblioteca para Inferência Causal com Redes Bayesianas</em></strong></p>

<!-- [CausalNex](https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html) is a toolkit for “causal inference with Bayesian Networks”. The tool aims to combine machine learning and causal reasoning for uncovering structural relationships in data. The authors also prepared an introductory guide on why and how to infer causation with Bayesian networks using the proposed Python library. -->

<p><br />
A <a href="https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html">CausalNex</a> é uma ferramenta que busca combinar o aprendizado de máquina e o raciocínio causal, possibilitando a descoberta de relacionamentos estruturais na base de dados. Os autores prepararam um tutorial introdutório que mostra porquê e como inferir causalidade com as Redes Bayesianas utilizando a biblioteca proposta.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png" alt="" /></p>

<p><a href="https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Google Colab Pro agora está disponível</em></strong></p>

<p><br />
O Google Colab comecou a oferecer uma versão Pro, que disponibiliza vantagens como o acesso exclusivo à GPUs e TPUs, tempos de execução mais longos e mais memória.</p>

<!-- Google Colab is now offering a Pro edition, which offers advantages such as exclusive access to faster GPUs and TPUs, longer runtimes, and more memory. -->

<p><br />
<strong><em>TyDi QA: A Multilingual Question Answering Benchmark</em></strong></p>

<p><br />
O grupo de IA da Google introduziu recentemente a <a href="https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html">TyDi QA</a>, uma base de dados multi-idiomas que busca encorajar pesquisadores a abordar a tarefa de <em>question answering</em> em línguas mais tipologicamente diversas, ou seja, que apresentam padrões estrturais não-convencionais. A liberação da base visa motivar a construção de modelos mais robustos a idiomas tipologicamente distantes, como o Árabe, Bengali, Coreano, Russo, Telugo e Tailândes, podendo generalizar para outros dialetos.</p>

<!-- Google AI releases [TyDi QA](https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html) which is a multilingual dataset that can encourage researchers to perform question answering on more typologically diverse languages that construct and express meaning in different ways. The idea is to motivate researchers to build more robust models on typologically distant languages, such as Arabic, Bengali, Korean, Russian, Telugu, and Thai, so as to generalize to even more languages. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png" alt="" /></p>

<p><a href="https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Question Answering para Node.js</em></strong></p>

<p><br />
A empresa Hugging Face liberou uma <a href="https://github.com/huggingface/node-question-answering">biblioteca para <em>question answering</em></a> baseada no DistilBERT, dando continuidade a sua missão de tornar a área de Processamento de Linguagem Natural mais acessível. O modelo apresentado pode rodar num ambiente de produção utilizando Node.js com apenas 3 linhas de código, se benficiando da implementação eficiente oferecida pela Tokenizers, também desenvolvida pelo Hugging Face, e a versão em Javascript do TensorFlow (TensorFlow.js).</p>

<!-- Hugging Face releases a [question answering library](https://github.com/huggingface/node-question-answering) based on DistilBERT and continues to make NLP more accessible. This model can run in production using Node.js with just 3 lines of code. The model leverages the fast implementation of Tokenizers, also built by Hugging Face, and TensorFlow.js (a popular library for using machine learning models with Javascript). -->

<h1 id="ética-em-ia-">Ética em IA 🚨</h1>

<p><strong><em>Identificando viés subjetivo em texto</em></strong></p>

<p><br />
Num <a href="https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922">podcast</a> que contou com a participação de Diyi Yang, um pesquisador em ciência social computacional, foi discutido como sistemas de IA podem auxiliar na identificação de viés subjetivo em informações textuais. Essa é uma área de pesquisa importante envolvendo Inteligência Artificial e NLP, especialmente quando discutimos sobre o consumo de textos, como títulos e chamadas de notícias, e a facilidade que esses meios possuem para influenciar leitores com opiniões subjetivas.</p>

<p>Do ponto de vista da aplicação, se torna de vital importância a tarefa de identificação desse viés de maneira automática, assim como a conscientização dos leitores, para que esses se tornem mais atentos e criteriosos em relação ao conteúdo que estão consumindo.</p>

<!-- This [podcast episode](https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922) features Diyi Yang, a researcher in computational social science, who talks about how AI systems can help to identify subjective bias in textual information. This is an important area of research involving AI systems and NLP especially when we discuss the consumption of text media such as news headlines that can be easily framed to bias consumers when in reality they should aim to be more objective. From an application perspective, it becomes critical to automatically identify the subjective bias present in text media so as to help consumers become more aware of the content they are consuming. The episode also discusses how AI can also perpetuate bias. -->

<p><br />
<strong><em>Artificial Intelligence, Values and Alignment</em></strong></p>

<p><br />
A disseminação de sistemas de IA e a forma com que esses sistemas se alinham com os valores humanos é uma área de pesquisa envolvendo ética na Inteligência Artificial em crescente atividade. A DeepMind publicou recentemente um <a href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment">artigo</a> que busca investigar de maneira mais profunda as questões filosóficas envolvidas no alinhamento da IA com os valores humanos. O trabalho discute duas frentes: a técnica (<em>como codificar valores que permitem que agentes de IA produzam resultados confiáveis</em>), e a normativa (<em>quais princípios deveriam ser codificados pelos modelos</em>), e como eles se relacionam e podem ser garantidos. O artigo encoraja uma abordagem baseada em princípios para o alinhamento de valores pelos sistemas de Inteligência, de modo a preservar um tratamento igualitário e justo frente às diferenças de convicções e opiniões.</p>

<!-- The rise of AI systems and how they align human values is an active area of research that involves ethics in AI systems. DeepMind recently released a [paper](https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment) that takes a deeper look at the philosophical questions surrounding AI alignment. The report focuses on discussing two parts, technical (i.e., *how to encode values that render reliable results from AI agents*) and normative (*what principles would be right to encode in AI*), and how they relate and can be ensured. The paper pushes for a principle-based approach for AI alignment and to preserve fair treatment despite the difference in beliefs and opinions. -->

<p><br />
<strong><em>Auditoria em Sistemas de IA</em></strong></p>

<p><br />
A VentureBeat divulgou que pesquisadores da Google, numa colaboração com outros grupos, criaram um <em>framework</em> chamado SMACTR, que permite a auditoria de sistemas de IA. A motivação para esse trabalho envolve a ausência de “prestação de contas” dos sistemas atuais, que são colocados à disposição do público geral. A reportagem completa pode ser acessada <a href="https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/">aqui</a>, assim como o <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372873">trabalho completo</a>.</p>

<!-- A VentureBeat reports that Google Researchers, in collaboration with other groups, created a framework called SMACTR that allows engineers to audit AI systems. The reason for this work is to address the accountability gap that exists with current AI systems that are put in the wild to be used by consumers. Read the full report [here](https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/) and the full paper [here](https://dl.acm.org/doi/abs/10.1145/3351095.3372873). -->

<h1 id="artigos-e-postagens-️">Artigos e Postagens ✍️</h1>

<p><strong><em>Destilação de modelos em sistemas de NLP</em></strong></p>

<p><br />
Num <a href="https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf">novo episódio</a> do podcast <em>NLP Highlights</em>, Thomas Wolf and Victor Sanh discutiram sobre a destilação de modelos e como a técnica pode ser utilizada como uma alternativa factível para a compressão de grandes arquiteturas, como o BERT, para aplicações escaláveis de NLP em cenários reais. A metodologia é discutida no trabalho publicado pelos convidados, entitulado <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, onde são construídos modelos menores (baseados na mesma arquitetura do modelo original) que tentam simluar o comportamento do modelo com maior número de parâmetros, de acordo com suas saídas. Essencialmente, o menor modelo (<em>student</em>) tenta modelar a distribuição de probabilidade do modelo maior (<em>teacher</em>) baseado na distribuição empírica gerado por suas saídas.</p>

<p><br />
<strong><em>BERT, ELMo, &amp; GPT-2: How contextual are contextualized word representations?</em></strong></p>

<p><br />
O sucesso de métodos contextualizados como o BERT para resolução de uma ampla gama de tarefas complexas de NLP é um assunto que está em voga no momento. Nesse <a href="https://kawine.github.io/blog/nlp/2020/02/03/contextual.html">post</a>, Kawin Ethayarajh tenta responder a questão que diz respeito à quão contextuais os modelos como BERT, o ELMo e o GPT-2 e seus respectivos <em>word embedddings</em> contextualizados são. As características exploradas incluem métricas de contextualidade, especificidade de contexto, além de comparações entre representações vetoriais de palavras “estáticas” e suas versões contextualizadas.</p>

<!-- Recently there has been a lot of talk on the success of contextualized methods like BERT for approaching a wide variety of complex NLP tasks. In this [post](https://kawine.github.io/blog/nlp/2020/02/03/contextual.html), Kawin Ethayarajh attempts to answer the question of how contextual models like BERT, ELMo and GPT-2 and their contextualized word representation are? Topics include measures of contextuality, context-specificity, and comparisons between static embeddings and contextualized representations. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png" alt="" /></p>

<p><a href="https://kawine.github.io/blog/nlp/2020/02/03/contextual.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Esparsidade em Redes Neurais</em></strong></p>

<p><br />
François Lagunas, pesquisador na área de ML, escreveu esse excelente <a href="https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70">post</a> compartilhando seu otimismo em relação à utilização de tensores esparsos em modelos de redes neurais. A expectativa é empregar alguma forma de esparsidade visando a redução do tamanho dos modelos atuais, que de certa forma estão se tornando impraticáveis, dadas suas colossais quantidades de parâmetros. Os Transformers, por exemplo, com seus bilhões de parâmetros, poderiam se beneficar com o emprego dessa técnica.</p>

<p>Entretanto, os detalhes de implementação para viabilizar a utilização eficiente da esparsidade em GPU ainda não estão claros… Felizmente, a comunidade de Aprendizado de Máquina já está trabalhando nisso!</p>

<!-- François Lagunas, an ML researcher, wrote this great [blog post](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70) discussing his optimism for adopting sparse tensors in neural network models. The hope is to employ some form of sparsity to reduce the size of current models that at some point become unpractical due to their size and speed. This concept may be worth exploring in ML due to the sheer size of current models like Transformers (often relying on billions of parameters). However, the implementation details to support efficient sparsity in neural networks on GPUs are not so clear from a developer tool perspective and that is something the machine learning community is working on already. -->

<p><br />
<strong><em>Treinando Seu Próprio Modelo de Linguagem</em></strong></p>

<p><br />
Se você está interessado em aprender como treinar um modelo de linguagem do zero, confira esse excelente <a href="https://huggingface.co/blog/how-to-train">tutorial</a> da Hugging Face que utiliza as suas incríveis bibliotecas Tokenizers e Transformers no treinamento do modelo.
<!--
If you are interested to learn how to train a language model scratch, check out this impressive and comprehensive [tutorial](https://huggingface.co/blog/how-to-train) by Hugging Face. They obviously leverage their own libraries Transformers and Tokenizers to train the model. --></p>

<p><br />
<strong><em>Tokenizers: How machines read</em></strong></p>

<p><br />
Cathal Horan publicou um <a href="https://blog.floydhub.com/tokenization-nlp/">blog post</a> impresssionante e bem detalhado sobre como e quais tipos de <em>tokenizers</em> vêm sendo utilizados nos mais recentes modelos de NLP, auxiliando modelos de Inteligência a aprender por meio de informações textuais. O post também discute e motiva porquê a tarefa de tokenização é uma importante e desafiadora área de pesquisa ativa. O artigo apresenta ainda como treinar o seu próprio <em>tokenizer</em> utilizando métodos como o SentencePiece e o WordPiece.</p>

<!-- Cathal Horan published an impressive and very detailed [blog post](https://blog.floydhub.com/tokenization-nlp/) about how and what type of tokenizers are being used by the most recent NLP models to help machine learning algorithms learn from textual information. He also discusses and motivated why tokenization is an exciting and important active area of research. The article even shows you how to train your own tokenizers using tokenization methods like SentencePiece and WordPiece. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png" alt="" /></p>

<p><a href="https://blog.floydhub.com/tokenization-nlp/%27"><em>fonte</em></a></p>

<h1 id="educação-">Educação 🎓</h1>

<p><strong><em>Machine Learning na VU Amsterdam</em></strong></p>

<p><br />
Agora você pode acompanahar o curso <a href="https://mlvu.github.io/">2020 MLVU machine learning</a> pela internet, onde estão inclusas a lista completa de slides, <a href="https://www.youtube.com/watch?v=excCZSTJEPs&amp;feature=youtu.be">videos</a> e o plano de estudos. O curso oferece uma introdução à ML, além de cobrir tópicos mais avançados de Deep Learning, como <em>Variational AutoEncoders</em> (VAEs) e Redes Neurais Adversariais (GANs).</p>

<!-- You can now follow the [2020 MLVU machine learning](https://mlvu.github.io/) course online, which includes the full set of slides, [videos](https://www.youtube.com/watch?v=excCZSTJEPs&feature=youtu.be), and syllabus. It is meant to be an introduction to ML but it also has other deep learning related topics such as VAEs and GANs. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*zFpU2rQL5Fby7X3boJyQNg.png" alt="" /></p>

<p><a href="https://mlvu.github.io/"><em>fonte</em></a></p>

<p><br />
<strong><em>Materiais de Matemática para ML</em></strong></p>

<p><br />
Suzana Ilić e a organização Machine Learning Tokyo (MLT) vêm realizando um excelente trabalho em prol da democratização do conhecimento em ML. Confira esse <a href="https://github.com/Machine-Learning-Tokyo/Math_resources">repositório</a> que apresenta uma coleção de fontes e materiais sobre os fundamentos matemáticos utilizados em Aprendizado de Máquina.</p>

<!-- Suzana Ilić and the Machine Learning Tokyo (MLT) have been doing amazing work in terms of democratizing ML education. For example, check out this [repository](https://github.com/Machine-Learning-Tokyo/Math_resources) showcasing a collection of free online resources for learning about the foundations of mathematical concepts used in ML. -->

<p><br />
<strong><em>Introduction to Deep Learning</em></strong></p>

<p><br />
Acompanhe o curso “<a href="http://introtodeeplearning.com/">Introduction to Deep Learning</a>” do MIT nesse site. Novas aulas serão postadas toda semanas e todos os materiais, como slides, vídeos e códigos utilizados, serão publicados.</p>

<!-- Keep track of the “[Introduction to Deep Learning](http://introtodeeplearning.com/)” course by MIT on this website. New lectures will be posted every week and all the sides and videos, including coding labs, will be published. -->

<p><br />
<strong><em>Deep Learning com PyTorch</em></strong></p>

<p><br />
Alfredo Canziani publicou os slides e notebooks utilizados no minicurso de Deep Learning com Pytorch. O repositório contém ainda um <a href="https://atcold.github.io/pytorch-Deep-Learning/">site</a> que incluem notas sobre os conceitos apresentados no curso.</p>

<!-- Alfredo Canziani has published the slides and notebooks for the minicourse on Deep Learning with PyTorch. The repository also contains a [companion website](https://atcold.github.io/pytorch-Deep-Learning-Minicourse/) that includes text descriptions of the concepts taught in the course. -->

<p><br />
<strong><em>Missing Semester of Your CS</em></strong></p>

<p><br />
O “<a href="https://missing.csail.mit.edu/">Missing Semester of Your CS</a>” é um excelente curso online composto por recursos que podem ser potencialmente úteis para cientistas de dados que não possuem background na área de desenvolvimenteo. Estão inclusos materiais sobre <em>shell scripting</em> e versionamento. O curso foi disponibilizado por alunos do MIT.
<br />
<img src="https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png" alt="" /></p>

<p><a href="https://missing.csail.mit.edu/2020/shell-tools/"><em>fonte</em></a></p>

<p><br />
<strong><em>Advanced Deep Learning</em></strong></p>

<p><br />
A CMU disponibilizou recentemente os slides e plano de estudos para o curso “<a href="https://andrejristeski.github.io/10707-S20/syllabus.html">Advanced Deep Learning</a>”, que cobre tópicos como modelos autoregressivos, modelos generativos, aprendizado autosupervisionado, entre outros. O público-alvo são alunos de mestrado e doutorado com sólidos conhecimentos de ML.</p>

<!-- A CMU released the slides and syllabus for the “[Advanced Deep Learning](https://andrejristeski.github.io/10707-S20/syllabus.html)” course which includes topics such as autoregressive models, generative models, and self-supervised/predictive learning, among others. The course is meant for MS or Ph.D. students with an advanced background in ML. -->

<h1 id="menções-honrosas-️">Menções honrosas ⭐️</h1>

<p><br />
Você pode encontrar a última Newsletter <a href="https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060">aqui</a>. Essa edição cobriu tópicos como melhorias em agentes conversacionais, divulgação de modelos BERT para idiomas específicos (entre eles o <strong>Português</strong>!!!), bases de dados publicamente disponívies, introdução de novas bibliotecas para Deep Learning, e muito mais.</p>

<!-- You can catch the previous NLP Newsletter here. The [issue](https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060) covers topics such as improving conversational agents, releases of language-specific BERT models, free datasets, releases of deep learning libraries, and much more. -->

<p>Em Xu et al. (2020), foi proposto um <a href="https://arxiv.org/abs/2002.02925]">método</a> para progressivamente substituir e comprimir modelos BERT através da separação de seus componentes originais. Através dessa substituição progressiva, aliado ao processo de treinamento, é possível combinar os componentes originais e suas versões compactadas. A metodologia apresentada supera outras abordagens de <em>knowledge distillation</em> no benchmark GLUE.</p>

<!-- Xu et al. (2020) proposed a [method](https://arxiv.org/abs/2002.02925]) for progressively replacing and compressing a BERT model by dividing it into its original components. Through progressive replacement and training, there is also the advantage of combining the original components and compacted versions of the model. The proposed model outperforms other knowledge distillation approaches on the GLUE benchmark. -->

<p>Um outro curso interessante é o “<a href="https://compstat-lmu.github.io/lecture_i2ml/index.html">Introduction to Machine Learning</a>”, que cobre o básico de ML, regressão supervisionada, <em>random forests</em>, ajuste de parâmetros, dentre outros conceitos fundamentais.</p>

<!-- Here is another interesting course called “[Introduction to Machine Learning](https://compstat-lmu.github.io/lecture_i2ml/index.html)” which covers the ML basics, supervised regression, random forests, parameter tuning, and many more fundamental ML topics. -->

<p>A versão para 🇬🇷 grego do BERT (<a href="https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1">GreekBERT</a>) está disponível para uso através da biblioteca Transformers, da Hugging Face.</p>

<p>Jeremy Howard publicou um <a href="https://arxiv.org/abs/2002.04688">artigo</a> descrevendo a biblioteca de Deep Learning fast.ai, que é amplamente utilizada para pesquisa e ensino em seus cursos de Deep Learning. Uma leitura bastante recomendada para desenvolvedores de software que trabalham construindo e melhorando pacotes de Aprendizado de Máquina e Deep Learning.
<!-- 
Jeremy Howard publishes a [paper](https://arxiv.org/abs/2002.04688) describing the fastai deep learning library which is widely used for research and to teach their open courses on deep learning. A recommended read for software developers working on building and improving deep learning and ML libraries. --></p>

<p>A Deeplearning.ai completou o lançamento dos seus 4 cursos da série <a href="https://www.coursera.org/specializations/tensorflow-data-and-deployment">TensorFlow: Data and Deployment Specialization</a>. A especialização visa ensinar desenvolverdores a como realizar o <em>deploy</em> de modelos de maneira efetiva e eficiente nos mais diferentes cenários, além de utilizar dados de maneiras eficazes durante o treinamento de modelos.</p>

<!-- Deeplearning.ai completes the release of all four courses of the [TensorFlow: Data and Deployment Specialization](https://www.coursera.org/specializations/tensorflow-data-and-deployment). The specialization mainly aims to educate developers on how to efficiently and effectively deploy models in different scenarios and make use of data in interesting and effective ways while training models. -->

<p>Sebastian Raschka publicou recentemente um <a href="https://arxiv.org/abs/2002.04803">artigo</a> entitulado “Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence”. O trabalho apresenta uma revisão bastante acessível às mais variadas ferramentas de ML. É um artigo excelente para compreender as vantagens de algumas bibliotecas e conceitos utilizados em Aprendizado de Máquina. Além disso, levanta-se a discussão sobre o futuro de bibliotecas de ML baseadas em Python.</p>

<!-- Sebastian Raschka recently published a [paper](https://arxiv.org/abs/2002.04803) titled “Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence”. The paper serves as a comprehensive review of the machine learning tools landscape. It is an excellent report for understanding the various advantages of some libraries and concepts used in ML engineering. In addition, a word on the future of Python-based machine learning libraries is provided. -->

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="VictorGarritano/personal_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/personal_blog/nlp_newsletter/2020/02/16/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/personal_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/personal_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/personal_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/VictorGarritano" title="VictorGarritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/victor-garritano" title="victor-garritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vic_garritano" title="vic_garritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://t.me/victorgarritano" title="victorgarritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#telegram"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
