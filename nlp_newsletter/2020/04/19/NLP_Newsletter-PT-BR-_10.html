<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶ | Garritano‚Äôs Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶" />
<meta name="author" content="VictorGarritano" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)" />
<meta property="og:description" content="Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)" />
<link rel="canonical" href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html" />
<meta property="og:url" content="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html" />
<meta property="og:site_name" content="Garritano‚Äôs Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"VictorGarritano"},"description":"Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html"},"url":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html","headline":"NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/personal_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://victorgarritano.github.io/personal_blog/feed.xml" title="Garritano's Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-162360650-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/personal_blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶ | Garritano‚Äôs Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶" />
<meta name="author" content="VictorGarritano" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)" />
<meta property="og:description" content="Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)" />
<link rel="canonical" href="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html" />
<meta property="og:url" content="https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html" />
<meta property="og:site_name" content="Garritano‚Äôs Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"VictorGarritano"},"description":"Nessa edi√ß√£o, s√£o cobertos t√≥picos como melhores pr√°ticas envolvendo modelos de linguagem, reprodutibilidade em ML e privacidade e seguran√ßa no Processamento de Linguagem Natural (NLP)","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html"},"url":"https://victorgarritano.github.io/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html","headline":"NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://victorgarritano.github.io/personal_blog/feed.xml" title="Garritano's Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-162360650-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/personal_blog/">Garritano&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/personal_blog/about/">About Me</a><a class="page-link" href="/personal_blog/search/">Search</a><a class="page-link" href="/personal_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">NLP Newsletter [PT-BR] #10: Aprimorando a reprodutibilidade em ML, Privacidade e Seguran√ßa em NLP, XTREME, Longformer, VilBERT, exBERT,‚Ä¶</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-19T00:00:00-05:00" itemprop="datePublished">
        Apr 19, 2020
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">VictorGarritano</span></span>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/personal_blog/categories/#nlp_newsletter">nlp_newsletter</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="https://cdn-images-1.medium.com/max/1200/1*WxbP3uKvd2GB6B-NaxtiIw.png" alt="" /></p>

<p><br />
Seja muito bem-vindo √† 10¬™ edi√ß√£o da NLP Newsletter. N√≥s esperamos que todos estejam bem e se mantendo seguros. Essa edi√ß√£o cobre t√≥picos como melhores pr√°ticas envolvendo Modelos de Linguagem, reprodutibilidade em ML e privacidade e seguran√ßa em NLP.
<!-- Welcome to the 10th issue of the NLP Newsletter. We hope you are well and staying safe. In this issue, we cover topics that range from best practices regarding language models to reproducibility in machine learning to privacy and security in natural language processing (NLP). --></p>

<h1 id="atualiza√ß√µes-da-darai-Ô∏è">Atualiza√ß√µes da dar.ai üî¨üéì‚öôÔ∏è</h1>

<ul>
  <li>Com o intuito de ajudar na an√°lise explorat√≥ria do <a href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"><em>COVID-19 Open Research Dataset</em></a> e na obten√ß√£o de <em>insights</em> a partir dessa literatura, n√≥s publicamos um <a href="https://github.com/dair-ai/covid_19_search_application"><em>notebook</em></a> com os passos para a implementa√ß√£o de uma aplica√ß√£o simples de busca por similaridade textual utilizando ferramentas de c√≥digo-aberto e modelos de linguagem pr√©-treinados publicamente dispon√≠veis.</li>
</ul>

<!-- - In order to help in the exploration of the [COVID-19 Open Research Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) and obtain insights from scientific literature, we published a [notebook](https://github.com/dair-ai/covid_19_search_application) that walks through the steps of building a simple text similarity search application using open source tools and publicly available pretrained language models. -->

<ul>
  <li>N√≥s realizamos um treinamento virtual na <a href="https://odsc.com/boston/"><em>Open Data Science Conference</em></a> na semana passada, com o tema <em>Deep Learning for Modern NLP</em>. Voc√™ pode acessar os materiais <a href="https://github.com/dair-ai/odsc_2020_nlp">aqui</a>.</li>
</ul>

<!-- - We delivered a virtual training at the [Open Data Science Conference](https://odsc.com/boston/) this past week on ["Deep Learning for Modern NLP"](https://github.com/dair-ai/odsc_2020_nlp). Find materials here. -->

<ul>
  <li>Tamb√©m na semana passada, n√≥s publicamos dois artigos bem interessantes, numa colabora√ß√£o com membros da nossa comunidade. Um dos trabalhos aborda <a href="https://medium.com/dair-ai/unsupervised-progressive-learning-upl-a-new-problem-for-ai-9a1c68c70a28"><em>unsupervised progressive learning</em></a>, um problema que envolve um agente que analisa uma sequ√™ncia de vetores de dados n√£o anotados (fluxo de dados) e aprende representa√ß√µes a partir da mesma. O segundo <a href="https://medium.com/dair-ai/structural-scaffolds-for-citation-intent-classification-in-scientific-publications-e5acd2f0ebf9">trabalho</a> resume uma abordagem para <em>Citation Intent Classification</em> (que consiste em identificar porqu√™ um autor citou outro trabalho) utilizando o modelo ELMo.</li>
</ul>

<!-- - This past week we published two articles together with members of our community. One is about [unsupervised progressive learning](https://medium.com/dair-ai/unsupervised-progressive-learning-upl-a-new-problem-for-ai-9a1c68c70a28) which is a problem that involves an agent that analyzes a sequence of unlabelled data vectors (data stream) and learns representations from these. The second [article](https://medium.com/dair-ai/structural-scaffolds-for-citation-intent-classification-in-scientific-publications-e5acd2f0ebf9) summarizes an approach for citation intent classification using ELMo. -->

<!-- - We recently published a [notebook](https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X) that helps to provide ideas on how to fine-tune pretrained language models for the task of emotion classification. -->

<ul>
  <li>N√≥s publicamos recentemente um <a href="https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X"><em>notebook</em></a> que fornece ideias para o ajuste fino de modelos de linguagem pr√©-treinados para a tarefa de classifica√ß√£o de emo√ß√µes.</li>
</ul>

<h1 id="pesquisas-e-publica√ß√µes-">Pesquisas e Publica√ß√µes üìô</h1>

<p><strong><em>XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</em></strong></p>

<p><br />
No in√≠cio dessa semana, pesquisadores da Google AI e da DeepMind publicaram um interessante <em>benchmark</em> multi-tarefa denominado <a href="https://arxiv.org/abs/2003.11080">XTREME</a>, que busca encorajar a avalia√ß√£o das capacidades de generaliza√ß√£o em diferentes idiomas de modelos de linguagem que aprendem representa√ß√µes multil√≠ngues. O <em>benchmark</em> conta com 40 idiomas e 9 tarefas, que requerem entendimento sobre diferentes n√≠veis de significado, tanto do ponto de vista sint√°tico quanto sem√¢ntico. O trabalho fornece bases para compara√ß√µes utilizando modelos estado-da-arte para representa√ß√µes multil√≠ngues, como o mBERT, XML e o MMTE.</p>

<!-- Earlier this week, researchers at Google AI and DeepMind published an interesting multi-task benchmark called [XTREME](https://arxiv.org/abs/2003.11080) that aims to encourage evaluation of the cross-lingual generalization capabilities of language models that learn multilingual representations. The benchmark tests on 40 languages and 9 different tasks that collectively require reasoning about different levels of meaning either syntactically or semantically. The paper also provides baseline results using state-of-the-art models for multilingual representation such as mBERT, XLM, and MMTE. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*kk7J1fCht_VZR_su.png" alt="" /></p>

<p><em>Fonte:</em> <a href="https://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html"><em>Google AI Blog</em></a></p>

<p><br />
<strong><em>Evaluating Machines by their Real-World Language Use</em></strong></p>

<p><br />
Foi demonstrado que modelos de linguagem apresentam um desempenho relativamente bom em diversas tarefas, como <em>question answering</em> e <em>sequence labeling</em>. Entretanto, um novo <a href="https://arxiv.org/abs/2004.03607">artigo</a> prop√µe um <em>framework</em> e <em>benchmark</em> para melhor avaliar se modelos de linguagem (LMs) conseguem desempenhar bem seu papel com o uso de linguagem do mundo real em situa√ß√µes mais complexas (por exemplo, gerar conselhos proveitosos para o cen√°rio atual do mundo). Resultados emp√≠ricos mostraram que modelos do estado-da-arte atual, como o T5, geram conselhos √∫teis como os escritos por humanos em apenas 9% dos casos. Essas observa√ß√µes apontam as defici√™ncias dos LMs no que diz respeito a entender e modelar conhecimentos de mundo e do senso comum.</p>

<!-- It has been shown that language models perform relatively well on a variety of tasks such as question answering and sequence labeling. However, a new [paper](https://arxiv.org/abs/2004.03607) proposes a framework and benchmark to better evaluate whether language models can perform at real-world language use with more complex settings (e.g., generating helpful advice for current situations). Empirical results demonstrate that current state-of-the-art models such as T5 generate advice that is as helpful as human-written advice in only 9% of the cases. These results point out the shortcomings of LMs in the ability to understand and model world knowledge and common-sense reasoning. -->

<p><br />
<strong><em>Give your Text Representation Models some Love: the Case for Basque</em></strong></p>

<p><br />
√â poss√≠vel que modelos monol√≠ngues (como os <em>word embeddings</em> do FastText e o BERT) treinados em grandes bases de dados de idiomas espec√≠ficos produzam melhores resultados que alternativas multil√≠ngues? Num <a href="https://arxiv.org/abs/2004.00033">artigo recente</a>, pesquisadores estudaram o desempenho de diversos modelos desse tipo utilizando uma grande base de dados para a l√≠ngua basca. Os resultados indicaram que modelos monol√≠ngues podem de fato produzir melhores resultados em tarefas como classifica√ß√£o de t√≥picos, de sentimentos e <em>PoS tagging</em> para esse idioma. Seria muito interessante verificar se o comportamento se repete para outros idiomas e quais resultados interessantes e novos desafio podem surgir.</p>

<!-- Can training monolingual models (FastText word embeddings and BERT) on large language-specific datasets produce better results than pretrained multilingual versions? In a recent [paper](https://arxiv.org/abs/2004.00033), researchers study the effect and performance of pertaining models using larger Basque corpora. Results indicate that indeed the model does produce better results on downstream tasks such as topic classification, sentiment classification, and PoS tagging for Basque. It could be interesting to test if this holds for other languages and whether there could some interesting results or new challenges that arise. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*rN7mNPz0os7kd8rBboliIg.png" alt="" /></p>

<p><em>Figura extra√≠da de</em> <a href="https://arxiv.org/abs/2004.00033"><em>Agerri et al. (2020)</em></a></p>

<p><br />
<strong><em>Advancing Self-Supervised and Semi-Supervised Learning with SimCLR</em></strong></p>

<p><br />
Numa <a href="https://dair.ai/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/">edi√ß√£o anterior</a> da Newsletter, n√≥s apresentamos o SimCLR, m√©todo desenvolvido pela Google AI que prop√µe um <em>framework</em> para <em>contrastive self-supervised learning</em> de representa√ß√µes visuais, com o objetivo de melhorar os resultados da tarefa de classifica√ß√£o de imagens em diferentes cen√°rios, como <em>transfer-learning</em> ou aprendizado semi-supervisionado, utilizando bases n√£o-anotadas. Os <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">resultados</a> obtidos demonstraram que a abordagem alcan√ßa resultados estado-da-arte no ImageNet utilizando apenas 1% de dados anotados, o que tamb√©m √© um indicativo das poss√≠veis vantagens do m√©todo em cen√°rios com escassez de dados.</p>

<!-- In a previous [issue](https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82) of the newsletter, we featured SimCLR, a method by Google AI that proposes a framework for *contrastive self-supervised learning* of visual representations for improving image classification results in different settings such as transfer learning and semi-supervised learning. It is a new approach to self-and semi-supervised learning to learn visual representations from unlabeled data. [Results](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html) demonstrate that it achieves state-of-the-art results on ImageNet while only relying on 1% labeled data which indicates that the method could also be beneficial in low-resourced settings. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*kGiv7LFJW1g_R6m2XblSSA.png" alt="" /></p>

<p><em>Fonte:</em> <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"><em>Google AI Blog</em></a></p>

<p><br />
Vale mencionar que o aprendizado auto-supervisionado (<em>self-supervised learning</em>) √© um dos t√≥picos mais quentes na √°rea atualmente. Se voc√™ tem interesse em saber mais, confira:</p>

<!-- It is worth mentioning that self-supervised learning is one of the hot topics in the field. If you are interested to know more you can check out the following: -->

<ul>
  <li><a href="https://www.nytimes.com/2020/04/08/technology/ai-computers-learning-supervised-unsupervised.html">Computers Already Learn From Us. But Can They Teach Themselves?</a></li>
  <li><a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/">The Illustrated Self-Supervised Learning</a></li>
  <li><a href="https://www.fast.ai/2020/01/13/self_supervised/">Self-supervised learning and computer vision</a></li>
</ul>

<p><br />
<strong><em>Byte Pair Encoding is Suboptimal for Language Model Pretraining</em></strong></p>

<p><br />
Kaj Bostrom e Greg Durrett publicaram um <a href="https://arxiv.org/pdf/2004.03720.pdf">trabalho</a> onde foi investigado se o <em>Byte Pair Encoding (BPE)</em>, um algoritmo para tokeniza√ß√£o habitualmente utilizado, √© a estrat√©gia √≥tima para o treinamento de modelos de linguagem. Os autores propuseram uma avalia√ß√£o direta do impacto da tokeniza√ß√£o no desempenho desses modelos, o que, segundo eles, √© raramente examinado, como observado na literatura. Para verificar isso, LMs foram treinados do zero em experimentos controlados, empregando diferentes t√©cnicas de tokeniza√ß√£o, a saber, <em>Unigram</em> e <em>BPE</em>. Ap√≥s isso, os modelos pr√©-treinados foram testados em diversas tarefas. Os resultados mostraram que o desempenho utilizando a estrat√©gia <em>Unigram</em> se equiparou e at√© mesmo foi superior ao BPE.</p>

<!-- Kaj Bostrom and Greg Durrett published a [paper](https://arxiv.org/pdf/2004.03720.pdf) where they aimed to investigate whether the commonly used tokenization algorithm called Byte Pair Encoding (BPE) is the most optimal for pretraining language models (LMs). In other words, they proposed a direct evaluation of the tokenization impact on the performance of LMs. According to the authors, this is rarely ever examined as observed in the literature. To achieve this, they pretrain LMs from scratch using controlled experiments and apply different tokenization, namely unigram and BPE. Thereafter they would test the resulting pretrained LMs on several downstream tasks. Results demonstrate that the unigram tokenization matches or outperforms the more common BPE. -->

<p><br />
<strong><em>Longformer: The Long-Document Transformer</em></strong></p>

<p><br />
Pesquisadores do Allen AI publicaram um novo modelo baseado no Transformer, denominado <a href="https://arxiv.org/abs/2004.05150">Longformer</a>, desenvolvido visando um desempenho mais eficiente em textos longos. Como j√° √© conhecido, uma das limita√ß√µes de modelos baseados no Transformer √© que eles s√£o computacionalmente custosos, devido √† maneira como a opera√ß√£o de <em>self-attention</em> escala (quadraticamente com o tamanho da sequ√™ncia), limitando assim a utiliza√ß√£o de contextos mais longos. Recentemente, v√°rias alternativas como o <a href="https://arxiv.org/abs/2001.04451">Reformer</a> e o <a href="https://arxiv.org/abs/1904.10509">Sparse Transformers</a> foram propostas, visando possibilitar a aplica√ß√£o dessa classe de modelos para documentos maiores. O Longformer combina modelagem a n√≠vel de caractere e <em>self-attention</em> (uma mistura do mecanismo de aten√ß√£o local e global) para requerer menos mem√≥ria e demonstra sua efici√™ncia na modelagens de textos longos. Os autores tamb√©m mostraram que o seu modelo pr√©-treinado supera outros m√©todos quando aplicados √† tarefas a n√≠vel de documento, como <em>question answering</em> e classifica√ß√£o de texto.</p>

<!-- Researchers at Allen AI published a new Transformer-based model called [Longformer](https://arxiv.org/abs/2004.05150) that is targeted at performing more efficiently with longer text. It is well known that one of the limitations of Transformer-based models is that they are computationally expensive due to how the self-attention operation scales (quadratically with sequence length) thus limiting the ability to process longer sequences. Recently, there have been many efforts such as the [Reformer](https://arxiv.org/abs/2001.04451) and [Sparse Transformers](https://arxiv.org/abs/1904.10509) to enable the applicability of Transformers for long documents. The Longformer combines character-level modeling and self-attention (mix of local and global attention) to consume less memory and demonstrate effectiveness in long document modeling. Authors also show that their pretrained model outperforms other methods when applied to document-level downstream tasks including QA and text classification. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*uTxVqLtO_nQaDw4OedUUtQ.png" alt="" /></p>

<p><em>Figura extra√≠da de</em> <a href="https://arxiv.org/abs/2004.05150"><em>Beltagy et al. (2020)</em></a></p>

<h1 id="criatividade-√©tica-e-sociedade-">Criatividade, √âtica e Sociedade üåé</h1>

<p><strong><em>Reprodutibilidade em ML</em></strong></p>

<ul>
  <li>A quest√£o da reprodutibilidade vem sendo discutida ativamente pelas comunidades de Aprendizado de M√°quina. Com o intuito de encorajar uma ci√™ncia mais aberta, transparente e acess√≠vel, diversos esfor√ßos v√™m sendo realizados a favor dela. Se voc√™ quiser entender como est√° essa quest√£o no campo de ML, confira essa <a href="https://arxiv.org/abs/2003.12206">publica√ß√£o</a> feita por Joelle Pineau, dentre outros.</li>
</ul>

<!-- - Reproducibility has been an ongoing topic of discussion amongst the machine learning communities. In order to encourage more open, transparent and accessible science, there have been many efforts around reproducibility. If you want to understand where the field of machine learning stands in terms of reproducibility, check out this [publication](https://arxiv.org/abs/2003.12206) by Joelle Pineau and others. -->

<ul>
  <li>Recentemente, e inspirado por esses esfor√ßos, o time do Papers With Code (que agora fazem parte do grupo de IA do Facebook) realizaram uma <a href="https://medium.com/paperswithcode/ml-code-completeness-checklist-e9127b168501">postagem</a> explicando uma <a href="https://github.com/paperswithcode/releasing-research-code"><em>checklist</em> de reprodutibilidade</a> bem √∫til, com o objetivo de <em>‚Äúfacilitar pesquisas reprodut√≠veis apresentadas nas principais confer√™ncias de ML‚Äù</em> (em tradu√ß√£o livre). A <em>checklist</em> avalia os c√≥digos disponibilizados nos seguintes aspectos:</li>
</ul>

<!-- - More recently, and inspired by these efforts, the Papers With Code team (now part of Facebook AI) published a [blog post](https://medium.com/paperswithcode/ml-code-completeness-checklist-e9127b168501) explaining a useful [reproducibility checklist](https://github.com/paperswithcode/releasing-research-code) to ‚Äú*facilitate reproducible research presented at major ML conferences*‚Äù. The checklist assesses code submission on the following: -->

<ol>
  <li>
    <p><strong>Depend√™ncias</strong>: O reposit√≥rio apresenta informa√ß√µes sobre as depend√™ncias ou instru√ß√µes sobre como preparar o ambiente de desenvolvimento?</p>
  </li>
  <li>
    <p><strong>C√≥digos de treinamento</strong>: O reposit√≥rio fornece uma maneira de treinar o(s) modelo(s) descritos no artigo?</p>
  </li>
  <li>
    <p><strong>C√≥digos de Avalia√ß√£o</strong>: O reposit√≥rio fornece um c√≥digo para calcular o desempenho do(s) modelo(s) treinado(s) ou rodar os experimentos neles?</p>
  </li>
  <li>
    <p><strong>Modelos pr√©-treinados</strong>: O reposit√≥rio fornece acesso gratuito aos par√¢metros do modelo pr√©-treinado?</p>
  </li>
  <li>
    <p><strong>Resultados</strong>: O reposit√≥rio fornece uma tabela/gr√°fico com os principais resultados e o c√≥digo para reproduzir esses resultados?</p>
  </li>
</ol>

<p><img src="https://cdn-images-1.medium.com/max/800/1*BQH6F1J3TE1T_GREv5xSew.png" alt="" /></p>

<p><em>Fonte:</em> <a href="https://medium.com/paperswithcode/ml-code-completeness-checklist-e9127b168501"><em>Papers with Code</em></a></p>

<ul>
  <li>Ainda nessa quest√£o de ci√™ncia aberta e reprodutibilidade, aqui est√° uma postagem interessante, feita por um pesquisador de NLP, <a href="https://twitter.com/srush_nlp/status/1245825437240102913?s=20">oferecendo uma recompensa</a> pela reprodu√ß√£o de resultados de um artigo que outro pesquisador n√£o conseguiu reproduzir.</li>
</ul>

<!-- - On the topic of open science and reproducibility, here is an interesting post by an NLP researcher [offering](https://twitter.com/srush_nlp/status/1245825437240102913?s=20) a bounty for replicating results from a paper that another researcher couldn‚Äôt replicate. -->

<p><br />
<strong><em>Privacidade e Seguran√ßa em NLP</em></strong></p>

<p><br />
Ser√° que um modelo de linguagem pr√©-treinado pode ser roubado, ou sua exposi√ß√£o para uso via <em>API</em> pode trazer implica√ß√µes de seguran√ßa? Em um novo artigo, pesquisadores testaram <em>APIs</em> de modelos baseados no BERT para implica√ß√µes de seguran√ßa, no que diz respeito √† utiliza√ß√£o de consultas para roubo do modelo. Resumidamente, eles observaram que um advers√°rio pode roubar um modelo refinado apenas utilizando sequ√™ncias de palavras sem sentido e refinando o seu pr√≥prio modelo com as predi√ß√µes do modelo-alvo. Leia mais sobre ataques de extra√ß√£o de modelos <a href="http://www.cleverhans.io/2020/04/06/stealing-bert.html">aqui</a>.</p>

<!-- Can a pretrained language model be stolen or does it impose any security implications when exposed for usage via APIs? In a new paper, researchers aim to test BERT-based APIs for security implications particularly regarding the use of queries to steal the model. In summary, they did found that an adversary can steal a fine-tuned model by just feeding gibberish sequences and fine-tuning their own model on the predicted labels of the victim model. Read more about model extraction attacks [here](http://www.cleverhans.io/2020/04/06/stealing-bert.html). -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K9ZD4USdovdyHXomB7csfA.png" alt="" /></p>

<p>Sistema de extra√ß√£o de modelos aplicado a um modelo-alvo treinado no SQuAD (<a href="http://www.cleverhans.io/2020/04/06/stealing-bert.html">Fonte</a>).</p>

<p><br />
Outro <a href="https://arxiv.org/abs/2004.06660">artigo interessante</a>, aceito na ACL 2020, investigou se modelos de linguagem pr√©-treinados s√£o suscet√≠veis a ataques. Os autores desenvolveram um m√©todo de ‚Äúenvenenamento‚Äù que √© capaz de injetar vulnerabilidades nos par√¢metros pr√©-treinados, tornando os modelos vulner√°veis √† amea√ßas. Devido a essas vulnerabilidades, √© poss√≠vel mostrar que esses modelos exp√µem <em>backdoors</em> que podem ser aproveitadas por invasores com o intuito de manipular as predi√ß√µes do modelo, simplesmente injetando qualquer palavra-chave arbitr√°ria. Para testar esse comportamento, modelos pr√©-treinados foram utilizados em tarefas que envolviam bases de dados ‚Äúcorrompidas‚Äù com palavras-chave espec√≠ficas, feitas para for√ßar o modelos a classificar exemplos de maneira incorreta.</p>

<!-- Another interesting [paper](https://arxiv.org/abs/2004.06660), accepted at ACL 2020, investigates whether pretrained language models are susceptible to attacks. The authors develop a *poisoning* method that is able to inject vulnerabilities into pretrained weights rendering these pretrained models vulnerable to serious threats. Due to this vulnerability, it is possible to show that these models expose backdoors that can be leveraged by an attacker to manipulate the model‚Äôs predictions by simply injecting any arbitrary keyword. To test this, pretrained models were used to perform downstream tasks that involved datasets injected with specific keywords meant to force the model to misclassify instances. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*s4QscGOeDiN6tHOfM99pww.png" alt="" /></p>

<p><em>Figura extra√≠da de</em> <a href="https://arxiv.org/abs/2004.06660"><em>Kurita et al. (2020)</em></a></p>

<p><br />
<strong><em>Uma s√©rie de pesquisas e aplica√ß√µes baseadas em IA para COVID-19</em></strong>
<!-- ***A COVID-19 series of AI-based applications and research*** --></p>

<ul>
  <li>
    <p>A COVID-19 provou-se um dos maiores desafios dos tempos modernos. Pesquisadores de todas as partes do mundo tentam encontrar maneiras de contribuir e ajudar a entender a doen√ßa, fornecendo desde ferramentas de busca at√© bases de dados. Sebastian Ruder publicou uma <a href="http://newsletter.ruder.io/issues/covid-19-edition-236509">edi√ß√£o dedicada</a> da sua <em>Newsletter</em> destacando alguns projetos interessantes que pesquisadores de IA v√™m desenvolvendo.
<!-- - COVID-19 has proven one of the biggest challenges in modern times. Researchers from all over the world are trying to find ways to contribute and help in understanding COVID-19, from search engines to data set releases. Sebastian Ruder recently published a dedicated [issue](http://newsletter.ruder.io/issues/covid-19-edition-236509) of his newsletter highlighting a few interesting projects that AI researchers have been work on. --></p>
  </li>
  <li>
    <p>Ainda nesse assunto, pesquisadores do Allen AI ir√£o discutir a agora popular base de dados <em>COVID-19 Open Research Dataset (CORD-19)</em> num <a href="https://www.meetup.com/NY-NLP/events/269849442"><em>meetup</em> virtual</a> que acontecer√° no final desse m√™s (27/04/2020).
<!-- - On the topic of COVID-19, researchers at Allen AI will discuss the now popular COVID-19 Open Research Dataset (CORD-19) in a [virtual meetup](https://www.meetup.com/NY-NLP/events/269849442) happening towards the end of this month. --></p>
  </li>
  <li>
    <p>A CORD-19 vem sendo utilizada por muitos pesquisadores para a constru√ß√£o de aplica√ß√µes impulsionadas por NLP, como ferramentas de busca. Confira esse <a href="https://openreview.net/forum?id=PlUA_mgGaPq">artigo recente</a> para um exemplo de implementa√ß√£o dessas ferramentas que auxiliam pesquisadores a obter <em>insights</em> r√°pidos relacionados √† CORD-19 a partir de resultados reportados em artigos de especialistas. De acordo com os autores, tais ferramentas ajudam em tomadas de decis√£o baseadas em evid√™ncias.
<!-- - The CORD-19 dataset is being used by many researchers to build NLP-powered applications such as search engines. Take a look at this recent [paper](https://openreview.net/forum?id=PlUA_mgGaPq) for an example of a search engine implementation that can help researchers obtain quick insights related to CORD-19 from results reported in scholarly articles. Such tools can help inform evidence-based decision making according to the authors. --></p>
  </li>
  <li>
    <p>ArCOV-19 √© uma base de dados de <em>tweets</em> em √°rabe sobre COVID-19, que cobre um per√≠odo de 27 de janeiro at√© 31 de mar√ßo de 2020 (e a coleta continua!). √â a primeira base dados publicamente dispon√≠vel do Twitter √Årabe cobrindo a pandemia do COVID-19, onde est√£o inclusos cerca de 748K <em>tweets</em> populares (de acordo com o crit√©rio de busca do pr√≥prio Twitter) junto com as redes de propaga√ß√£o do sub-conjunto mais popular de postagens. As redes incluem tanto <em>retweets</em> quando <em>threads</em> de respostas.  <a href="https://gitlab.com/bigirqu/ArCOV-19">ArCOV-19</a> √© projetado para permitir a pesquisa em diversas √°reas, como NLP, Ci√™ncia de Dados, Computadores e Sociedade, entre outras.
<!-- - ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from the 27th of January till the 31st of March 2020 (and still ongoing). It is the first publicly-available Arabic Twitter dataset covering the COVID-19 pandemic that includes around 748k popular tweets (according to Twitter search criterion) alongside the propagation networks of the most-popular subset of them. The propagation networks include both retweets and conversational threads (i.e., threads of replies). [ArCOV-19](https://gitlab.com/bigirqu/ArCOV-19) is designed to enable research under several domains including natural language processing, data science, and social computing, among others. --></p>
  </li>
</ul>

<h1 id="ferramentas-e-bases-de-dados-Ô∏è">Ferramentas e Bases de Dados ‚öôÔ∏è</h1>

<p><strong><em>Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence</em></strong></p>

<p><br />
Mesmo n√£o sendo uma ferramenta ou base de dados por si s√≥, esse excelente <a href="https://www.mdpi.com/2078-2489/11/4/193">artigo</a>, com autoria de Sebastian Raschka, Joshua Patterson, e Corey Nolet, fornece uma vis√£o geral compreensiva de alguns dos principais desenvolvimentos em termos de tend√™ncias tecnol√≥gicas em ML, com foco na linguagem de programa√ß√£o Python.</p>

<!-- Not a tool or dataset per se, but this excellent [paper](https://www.mdpi.com/2078-2489/11/4/193) by Sebastian Raschka, Joshua Patterson, and Corey Nolet provides a comprehensive overview of some of the main developments in terms of technology trends in machine learning, particularly focused on the Python programming language. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*OUpM4KS2uvT7zWlMYqy8RQ.png" alt="" /></p>

<p><em>Figura extra√≠da de</em> <a href="https://www.mdpi.com/2078-2489/11/4/193"><em>Raschka et al. (2020)</em></a></p>

<p><br />
<strong><em>Interpretabilidade e Explicabilidade em ML</em></strong></p>

<p><br />
A Hugging Face disponibilizou uma ferramenta de visualiza√ß√£o denominada exBERT, que nos permite visualizar as representa√ß√µes aprendidas por modelos de linguagem como o BERT e RoBERTa. Essa funcionalidade foi integrada √† <a href="https://huggingface.co/models?filter=exbert">p√°gina de modelos</a>, com o objetivo de prover um melhor entendimento sobre como os modelos de linguagem est√£o aprendendo, assim como quais propriedades s√£o potencialmente codificadas por eles nessas representa√ß√µes.</p>

<!-- HuggingFace released a visualization tool called exBERT that allows you to visualize learned representations from language models such as BERT and RoBERTa. This feature was integrated into their [model pages](https://huggingface.co/models?filter=exbert) and aims at better understanding how language models are learning and what properties they are potentially encoding in these learned representations. -->

<p><br />
A OpenAI disponibilizou recentemente uma aplica√ß√£o web chamada <a href="https://microscope.openai.com/models">Microscope</a> que cont√©m uma cole√ß√£o de visualiza√ß√µes obtidas de camadas e neur√¥nios de diversos modelos de vis√£o computacional que s√£o comumente estudados no contexto de interpretabilidade. O objetivo principal √© facilitar a an√°lise e compartilhamento de <em>insights</em> interessantes, obtidos a partir das caracter√≠sticas aprendidas pelas redes neurais, assim como viabilizar um melhor entendimento dos mesmos.</p>

<!-- OpenAI recently released a web application called [Microscope](https://microscope.openai.com/models) that contains a collection of visualizations obtained from significant layers and neurons of various vision models that are often studied in the context of interpretability. The main objective is to allow ease of analysis and sharing of interesting insights that emerge from these features learned in the neural networks so as to better understand them. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*4VdcqSSyzWDMvVDPEuKzIQ.png" alt="" /></p>

<p><br />
<strong><em>CloudCV: ViLBERT Multi-Task Demo</em></strong></p>

<p><br />
No edi√ß√£o anterior da <a href="https://dair.ai/NLP_Research_Highlights_-_Issue_-1/">NLP Research Highlights</a>, n√≥s apresentamos o ViLBERT multi-tarefa, que √© um m√©todo para o aprimoramento de <em>vision-and-language models</em> que pode ser utilizado em recupera√ß√£o de imagens baseada em descri√ß√µes e <em>visual question answering (VQA)</em>. Agora, os autores disponibilizaram uma <a href="https://vilbert.cloudcv.org/">aplica√ß√£o web</a> para teste dos modelos em 8 tarefas diferentes de linguagem e vis√£o computacional, como VQA e <em>pointing question answering</em>.</p>

<!-- In the previous [NLP Research Highlights](https://dair.ai/NLP_Research_Highlights_-_Issue_-1/), we featured multitask ViLBERT which is a method for improving vision-and-language models that can be used for caption-based image retrieval and visual question answering (VQA). The authors now provide a [web application](https://vilbert.cloudcv.org/) to test the models on eight different vision and language tasks such as VQA and pointing question answering. -->

<p><br />
<strong><em>A Twitter Dataset of 150+ million tweets related to COVID-19 for open research</em></strong></p>

<p><br />
Devido √† relev√¢ncia da pandemia global de COVID-19, pesquisadores est√£o liberando uma <a href="https://zenodo.org/record/3738018">base de dados</a> com <em>tweets</em> relacionados a doen√ßa. Desde a primeira vers√£o disponibilizada, dados adicionais de novos colaborados foram adicionados, permitindo o crescimento da base at√© seu volume atual. A aquisi√ß√£o dedicada de dados come√ßou em 11 de mar√ßo, com mais de 4 milh√µes de <em>tweets</em> por dia.</p>

<!-- Due to the relevance of the COVID-19 global pandemic, researchers are releasing a [dataset](https://zenodo.org/record/3738018) of tweets acquired from Twitter related to COVID-19 chatter. Since the first release, additional data from new collaborators has been added, allowing this resource to grow to its current size. Dedicated data gathering started from March 11th yielding over 4 million tweets a day. -->

<p><br />
<strong><em>A tiny autograd engine</em></strong></p>

<p><br />
Andrej Karpathy disponibilizou recentemente uma biblioteca conhecida como <a href="https://github.com/karpathy/micrograd">micrograd</a>, que permite a constru√ß√£o e treinamento de redes neurais utilizando uma interface simples e intuitiva. Na verdade, ele escreveu a biblioteca completa com aproximadamente 150 linhas de c√≥digo, o que, segundo ele, √© a mais compacta ferramenta de diferencia√ß√£o autom√°tica que existe. Idealmente, bibliotecas como essa podem ser utilizadas para fins educacionais.</p>

<!-- Andrej Karpathy recently released a library called [micrograd](https://github.com/karpathy/micrograd) which provides the ability to build and train a neural network using a simple and intuitive interface. In fact, he wrote the whole library in roughly 150 lines of code which he claims is the tiniest autograd engine there is. Ideally, such types of libraries can be used for educational purposes. -->

<h1 id="artigos-e-postagens-Ô∏è">Artigos e Postagens ‚úçÔ∏è</h1>

<p><strong><em>The Transformer Family and Recent Developments</em></strong></p>

<p><br />
Numa nova e oportuna postagem, Lilian Weng resumiu alguns dos recentes avan√ßos no modelo Transformer. O <a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">artigo</a> utiliza uma nota√ß√£o amig√°vel, apresenta uma revis√£o da literatura bem como as √∫ltimas melhorias propostas, como aten√ß√£o com contextos mais longos (Transformer XL) e redu√ß√£o nos requisitos computacionais e de mem√≥ria.</p>

<!-- In a new and timely blog post, Lilian Weng summarizes some of the recent developments of the Transformer model. The [article](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) provides nice notation, historical review, and the latest improvements such as longer attention span (Transformer XL), reduced computation and memory consumption. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*i-4V-EIirg2cvGMVLd8BWA.png" alt="" /></p>

<p><br />
A compress√£o de modelos √© uma importante √°rea de pesquisa em NLP, devido √† natureza e ao tamanho de modelos de linguagem pr√©-treinados. Idealmente, conforme os modelos produzem novos resultados estado-da-arte para as mais diferentes tarefas de NLP, torna-se importante reduzir seus requisitos computacionais, tornando sua utiliza√ß√£o vi√°vel em produ√ß√£o. Madison May publicou recentemente outro excelente <a href="https://www.pragmatic.ml/a-survey-of-methods-for-model-compression-in-nlp/">artigo</a> trazendo um vis√£o geral de alguns m√©todos utilizados para compress√£o de modelos para NLP. Alguns dos t√≥picos principais incluem poda, otimiza√ß√£o dos grafos de computa√ß√£o, destila√ß√£o de conhecimento, substitui√ß√£o progressiva de m√≥dulos, entre outros.</p>

<!-- Model compression is an important area of research in NLP due to the nature and large size of pretrained language models. Ideally, as these models continue to produce state-of-the-art results across a wide variety of NLP tasks it becomes important to reduce their computational needs so as to make them feasible in production. Madison May recently published another excellent [article](https://www.pragmatic.ml/a-survey-of-methods-for-model-compression-in-nlp/) summarizing a few methods used for model compression, particularly in NLP. Some of the main topics include pruning, graph optimizations, knowledge distillation, progressive module replacement, among others. -->

<h1 id="educa√ß√£o-">Educa√ß√£o üéì</h1>

<p><strong><em>Guest Lecture on Language Models by Alec Radford</em></strong></p>

<p><br />
Se voc√™ tem interesse em conhecer os aspectos te√≥ricos dos m√©todos utilizados para o aprendizado de modelos de linguagem como o CBOW, Word2Vec, ELMo, GPT, BERT, ELECTRA, T5 e GPT, ent√£o voc√™ deveria conferir essa excelente <a href="https://www.youtube.com/watch?v=BnpB3GrpsfM">aula</a> do Alec Radford (pesquisador na OpenAI). Ela faz parte de um <a href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">curso em andamento</a>, lecionado pelo Pieter Abbeel, sobre t√©cnicas de aprendizado n√£o-supervisionado com redes neurais profundas.</p>

<!-- If you are curious to know the theoretical aspect of methods used for learning language models such as CBOW, Word2Vec, ELMo, GPT, BERT, ELECTRA, T5, and GPT, then you might be interested in this great [guest lecture](https://www.youtube.com/watch?v=BnpB3GrpsfM) by Alec Radford (researcher at OpenAI). This was delivered as part of the ongoing [course](https://sites.google.com/view/berkeley-cs294-158-sp20/home) taught by Pieter Abbeel on deep unsupervised learning techniques. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*GUxoCXqhozkp_aaRxpT3Sg.png" alt="" /></p>

<p><br />
<strong><em>Python Numpy Tutorial (with Jupyter and Colab)</em></strong></p>

<p><br />
O popular curso online de Stanford, <em>Convolutional Neural Network for Visual Recognition</em>, agora inclui um <em>link</em> para um <em>notebook</em> do Colab com o seu <a href="https://cs231n.github.io/python-numpy-tutorial/">guia introdut√≥rio</a> ao NumPy, que apresenta um passo a passo extenso mas muito interessante para iniciantes.</p>

<!-- Stanford‚Äôs popular online course on Convolutional Neural Network for Visual Recognition now includes a link to a Google Colab notebook for its [introductory guide](https://cs231n.github.io/python-numpy-tutorial/) to Numpy. It‚Äôs a very extensive walkthrough but it‚Äôs very nice for beginners. -->

<p><br />
<strong><em>New mobile neural network architectures</em></strong></p>

<p><br />
Interessado em construir modelos de redes neurais para dispositivos m√≥veis ou de borda? Ent√£o essa <a href="https://machinethink.net/blog/mobile-architectures/">postagem</a> bem acess√≠vel pode te ajudar! O artigo cobre diversas configura√ß√µes de redes e inclui testes de velocidade.</p>

<!-- Interested in building neural network architectures for mobile and edge devices, then this comprehensive [blog post](https://machinethink.net/blog/mobile-architectures/) may be for you. The article covers a range of neural network designs and includes speed performance tests. -->

<p><br />
<strong><em>Data-Driven Sentence Simplification: Survey and Benchmark</em></strong></p>

<p><br />
A tarefa de <em>Sentence simplification</em> (simplifica√ß√£o de frases, numa tradu√ß√£o livre), possui a finalidade de modificar uma frase de modo a torn√°-la mais f√°cil de ler e entender. Essa <a href="https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00370">colet√¢nea</a> foca em abordagens que tentam aprender a simplificar utilizando uma base de pares de senten√ßas em ingl√™s, contendo as vers√µes originais e simplificadas, que √© um paradigma dominante nos dias atuais. Tamb√©m est√° incluso um <em>benchmark</em> dos diferentes m√©todos em diversas bases de dados, que os compara e destaca os pontos fortes e fracos de cada um deles.</p>

<!-- Sentence simplification aims to modify a sentence in order to make it easier to read and understand. This [survey paper](https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00370) focuses on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. It also includes a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. -->

<p><br />
<strong><em>T√≥picos Avan√ßados em Aprendizado de M√°quina</em></strong></p>

<p><br />
Yisong Yue publicou todas as v√≠deo-aulas do curso <a href="https://sites.google.com/view/cs-159-spring-2020/lectures?authuser=0">Data-Driven Algorithm Design</a>. S√£o cobertos t√≥picos de ML como otimiza√ß√£o Bayesiana, computa√ß√£o diferenci√°vel e <em>imitation learning</em>.</p>

<!-- Yisong Yue published all lecture videos for the [Data-Driven Algorithm Design](https://sites.google.com/view/cs-159-spring-2020/lectures?authuser=0) course. It contains advanced topics in machine learning that range from Bayesian optimization to differentiable computation to imitation learning. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8YFTbEPUw3Bqio70xP0WXQ.png" alt="" /></p>

<h1 id="men√ß√µes-honrosas-Ô∏è">Men√ß√µes Honrosas ‚≠êÔ∏è</h1>

<p>Acesse as edi√ß√µes anteriores da NLP Newsletter <a href="https://github.com/dair-ai/nlp_newsletter">aqui</a> (<a href="(https://github.com/dair-ai/nlp_newsletter)">√∫ltima edi√ß√£o em PT-BR</a>).</p>

<!-- Get access to the previous issues of the NLP Newsletter [here](https://github.com/dair-ai/nlp_newsletter). -->

<p><br />
Harvard est√° <a href="https://online-learning.harvard.edu/catalog?keywords=&amp;paid%5B1%5D=1&amp;max_price=&amp;start_date_range%5Bmin%5D%5Bdate%5D=&amp;start_date_range%5Bmax%5D%5Bdate%5D=">oferecendo</a> uma √≥tima sele√ß√£o de cursos <em>self-paced</em> de maneira gratuita!
<!-- Harvard is currently [offering](https://online-learning.harvard.edu/catalog?keywords=&paid%5B1%5D=1&max_price=&start_date_range%5Bmin%5D%5Bdate%5D=&start_date_range%5Bmax%5D%5Bdate%5D=) a great selection of self-paced courses for free. --></p>

<p><br />
<a href="https://github.com/zaidalyafeai/ARBML">ARBML</a> fornece implementa√ß√µes de diversos projetos de NLP e ML para a l√≠ngua √°rabe, incluindo experi√™ncias em tempo real utilizando diversas interfaces, como a <em>web</em>, linha de comando e <em>notebooks</em>.</p>

<!-- [ARBML](https://github.com/zaidalyafeai/ARBML) provides implementations of many Arabic NLP and ML projects providing real-time experience using many interfaces like web, command line and notebooks. -->

<p><br />
<a href="https://nlpdashboard.com">NLP Dashboard</a> √© uma aplica√ß√£o <em>web</em> de NLP interessante, que oferece Reconhecimento de Entidades Nomeadas e an√°lises estat√≠sticas de textos e not√≠cias. Constru√≠da utilizando spaCy, Flask e Python.</p>

<!-- [NLP Dashboard](https://nlpdashboard.com) is a fun NLP web app to perform named entity recognition and statistical analysis of text and news stories. Built using spaCy, Flask, and Python. -->

<p><br />
Caso voc√™ ainda n√£o conhe√ßa, Connor Shorten mant√©m um <a href="https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw?sub_confirmation=1">canal no YouTube</a> bastante informativo, onde ele resume artigos de ML bem recentes e interessantes. S√£o apresentados os detalhes importantes de cada trabalho atrav√©s de um excelente e conciso resumo. Connor tamb√©m come√ßou um <a href="https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ"><em>podcast</em></a> com outros grandes pesquisadores e educadores da √°rea.</p>

<!-- If you haven‚Äôt checked it out, Connor Shorten maintains this really informative [YouTube channel](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw?sub_confirmation=1) where he summarizes interesting and recent ML papers. He covers the important details of each work while providing excellent short and concise summaries. He also started a [podcast](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ) with other great researchers and explainers in the field. -->

<p><br />
<a href="https://github.com/microsoft/nlp-recipes">Aqui</a> est√° um reposit√≥rio impressionante e bem completo que apresenta as melhoras pr√°ticas e recomenda√ß√µes (via <em>notebooks</em> e explica√ß√µes) para diversos cen√°rios de NLP, como classifica√ß√£o de texto, <em>entailment</em> (estabelecer rela√ß√µes l√≥gicas, como implica√ß√£o e contradi√ß√£o, entre textos), sumariza√ß√£o de texto, <em>question answering</em>, etc.</p>

<!-- [Here](https://github.com/microsoft/nlp-recipes) is a rich and impressive repository that provides best practices and recommendations (via notebooks and explanations) for many NLP scenarios such as text classification, entailment, text summarization, question answering, etc. -->

<hr />

<p>Se voc√™ conhece bases de dados, projetos, postagens, tutoriais ou artigos que gostaria de ver na pr√≥xima edi√ß√£o da <em>Newsletter</em>, por favor nos envie utilizando esse <a href="https://forms.gle/3b7Q2w2bzsXE6uYo9">formul√°rio</a>.</p>

<p><br />
<a href="https://dair.ai/newsletter/"><em>Inscreva-se</em></a> <em>üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada!</em></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="VictorGarritano/personal_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/personal_blog/nlp_newsletter/2020/04/19/NLP_Newsletter-PT-BR-_10.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/personal_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/personal_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/personal_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/VictorGarritano" title="VictorGarritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/victor-garritano" title="victor-garritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vic_garritano" title="vic_garritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://t.me/victorgarritano" title="victorgarritano"><svg class="svg-icon grey"><use xlink:href="/personal_blog/assets/minima-social-icons.svg#telegram"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
